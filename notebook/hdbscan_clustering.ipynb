{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a81481e-eb58-45be-a2d0-c86fbe81c3d3",
   "metadata": {},
   "source": [
    "# Clustering with localization-derived features with hdbscan\n",
    "\n",
    "HDBSCAN is a clustering algorithm developed by Campello, Moulavi, and Sander. It extends DBSCAN by converting it into a hierarchical clustering algorithm, and then using a technique to extract a flat clustering based in the stability of clusters. \n",
    "\n",
    "Steps\n",
    "> 1. Transform the space according to the density/sparsity. \n",
    "  2. Build the minimum spanning tree of the distance weighted graph. \n",
    "  3. Construct a cluster hierarchy of connected components. \n",
    "  4. Condense the cluster hierarchy based on minimum cluster size.\n",
    "  5. Extract the stable clusters from the condensed tree.\n",
    "\n",
    "Important parameters\n",
    "\n",
    "> min_cluster_size, int, optional (default=5): The minimum size of clusters; single linkage splits that contain fewer points than this will be considered points “falling out” of a cluster rather than a cluster splitting into two new clusters.\n",
    "\n",
    "> min_samples, int, optional (default=None): The number of samples in a neighbourhood for a point to be considered a core point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ae6d49-91ba-442a-9bef-2d5fa009b8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'run_weighted_triage' from 'spike_psvae.cluster_utils' (/media/cat/julien/spike_psvae/spike_psvae/cluster_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_viz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_self_agreement, plot_single_unit_summary, plot_agreement_venn, plot_isi_distribution, plot_unit_similarities\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_viz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_unit_similarity_heatmaps\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_sorting_from_labels_frames, compute_cluster_centers, relabel_by_depth, run_weighted_triage, remove_duplicate_units\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_agreement_indices, compute_spiketrain_agreement, get_unit_similarities, compute_shifted_similarity, read_waveforms\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_closest_clusters_hdbscan, get_closest_clusters_kilosort, get_closest_clusters_hdbscan_kilosort, get_closest_clusters_kilosort_hdbscan\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'run_weighted_triage' from 'spike_psvae.cluster_utils' (/media/cat/julien/spike_psvae/spike_psvae/cluster_utils.py)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import argparse\n",
    "import hdbscan\n",
    "from spike_psvae.cluster_viz import cluster_scatter, plot_waveforms_geom, plot_raw_waveforms_unit_geom, plot_venn_agreement\n",
    "from spike_psvae.cluster_viz import plot_array_scatter, plot_self_agreement, plot_single_unit_summary, plot_agreement_venn, plot_isi_distribution, plot_waveforms_unit_geom, plot_unit_similarities\n",
    "from spike_psvae.cluster_viz import plot_unit_similarity_heatmaps\n",
    "from spike_psvae.cluster_utils import make_sorting_from_labels_frames, compute_cluster_centers, relabel_by_depth, run_weighted_triage, remove_duplicate_units\n",
    "from spike_psvae.cluster_utils import get_agreement_indices, compute_spiketrain_agreement, get_unit_similarities, compute_shifted_similarity, read_waveforms\n",
    "from spike_psvae.cluster_utils import get_closest_clusters_hdbscan, get_closest_clusters_kilosort, get_closest_clusters_hdbscan_kilosort, get_closest_clusters_kilosort_hdbscan\n",
    "import spikeinterface \n",
    "from spikeinterface.toolkit import compute_correlograms\n",
    "from spikeinterface.comparison import compare_two_sorters\n",
    "from spikeinterface.widgets import plot_agreement_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib_venn import venn3, venn3_circles, venn2\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "import h5py\n",
    "from scipy.spatial import cKDTree\n",
    "import pickle\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy.spatial.distance import cdist\n",
    "from isosplit import isocut\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af75856-a2b6-4081-bbbb-a75e7e2fb1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_psvae.denoise import SingleChanDenoiser\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "denoiser = SingleChanDenoiser()\n",
    "denoiser.load()\n",
    "denoiser.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9d6f3-e14d-4197-a589-16534461c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def denoise_wf_nn_tmp_single_channel(wf, denoiser, device):\n",
    "    denoiser = denoiser.to(device)\n",
    "    n_data, n_times, n_chans = wf.shape\n",
    "    if wf.shape[0]>0:\n",
    "        wf_reshaped = wf.transpose(0, 2, 1).reshape(-1, n_times)\n",
    "        wf_torch = torch.FloatTensor(wf_reshaped).to(device)\n",
    "        denoised_wf = denoiser(wf_torch).data\n",
    "        denoised_wf = denoised_wf.reshape(\n",
    "            n_data, n_chans, n_times)\n",
    "        denoised_wf = denoised_wf.cpu().data.numpy().transpose(0, 2, 1)\n",
    "\n",
    "        del wf_torch\n",
    "    else:\n",
    "        denoised_wf = np.zeros((wf.shape[0], wf.shape[1]*wf.shape[2]),'float32')\n",
    "\n",
    "    return denoised_wf\n",
    "\n",
    "def run_weighted_triage(x, y, z, alpha, maxptps, pcs=None, \n",
    "                        scales=(1,10,1,15,30,10),\n",
    "                        threshold=100, ptp_threshold=3, c=1, ptp_weighting=True):\n",
    "    \n",
    "    ptp_filter = np.where(maxptps>ptp_threshold)\n",
    "    x = x[ptp_filter]\n",
    "    y = y[ptp_filter]\n",
    "    z = z[ptp_filter]\n",
    "    alpha = alpha[ptp_filter]\n",
    "    maxptps = maxptps[ptp_filter]\n",
    "    if pcs is not None:\n",
    "        pcs = pcs[ptp_filter]\n",
    "        feats = np.c_[scales[0]*x,\n",
    "                      scales[1]*np.log(y),\n",
    "                      scales[2]*z,\n",
    "                      scales[3]*np.log(alpha),\n",
    "                      scales[4]*np.log(maxptps),\n",
    "                      scales[5]*pcs[:,:3]]\n",
    "    else:\n",
    "        feats = np.c_[scales[0]*x,\n",
    "                      # scales[1]*np.log(y),\n",
    "                      scales[2]*z,\n",
    "                      # scales[3]*np.log(alpha),\n",
    "                      scales[4]*np.log(maxptps)]\n",
    "    \n",
    "    tree = cKDTree(feats)\n",
    "    dist, ind = tree.query(feats, k=6)\n",
    "    dist = dist[:,1:]\n",
    "    # dist = np.sum((c*np.log(dist)),1)\n",
    "    # print(dist)\n",
    "    if ptp_weighting:\n",
    "        dist = np.sum(c*np.log(dist) + np.log(1/(scales[4]*np.log(maxptps)))[:,None], 1)\n",
    "    else:\n",
    "        dist = np.sum((c*np.log(dist)),1)\n",
    "        \n",
    "    idx_keep = dist <= np.percentile(dist, threshold)\n",
    "    \n",
    "    triaged_x = x[idx_keep]\n",
    "    triaged_y = y[idx_keep]\n",
    "    triaged_z = z[idx_keep]\n",
    "    triaged_alpha = alpha[idx_keep]\n",
    "    triaged_maxptps = maxptps[idx_keep]\n",
    "    triaged_pcs = None\n",
    "    if pcs is not None:\n",
    "        triaged_pcs = pcs[idx_keep]\n",
    "        \n",
    "    \n",
    "    return triaged_x, triaged_y, triaged_z, triaged_alpha, triaged_maxptps, triaged_pcs, ptp_filter, idx_keep\n",
    "\n",
    "def read_waveforms(spike_times, bin_file, geom_array, n_times=121, offset_denoiser = 42, channels=None, dtype=np.dtype('float32')):\n",
    "    '''\n",
    "    read waveforms from recording\n",
    "    n_times : waveform temporal length \n",
    "    channels : channels to read from \n",
    "    '''\n",
    "    # n_times needs to be odd\n",
    "    if n_times % 2 == 0:\n",
    "        n_times += 1\n",
    "\n",
    "    # read all channels\n",
    "    if channels is None:\n",
    "        channels = np.arange(geom_array.shape[0])\n",
    "        \n",
    "    # ***** LOAD RAW RECORDING *****\n",
    "    wfs = np.zeros((len(spike_times), n_times, len(channels)),\n",
    "                   'float32')\n",
    "\n",
    "    skipped_idx = []\n",
    "    n_channels = geom_array.shape[0] #len(channels)\n",
    "    total_size = n_times*n_channels\n",
    "    # spike_times are the centers of waveforms\n",
    "    spike_times_shifted = spike_times - (offset_denoiser) #n_times//2\n",
    "    offsets = spike_times_shifted.astype('int64')*dtype.itemsize*n_channels\n",
    "    with open(bin_file, \"rb\") as fin:\n",
    "        for ctr, spike in enumerate(spike_times_shifted):\n",
    "            try:\n",
    "                fin.seek(offsets[ctr], os.SEEK_SET)\n",
    "                wf = np.fromfile(fin,\n",
    "                                 dtype=dtype,\n",
    "                                 count=total_size)\n",
    "                wfs[ctr] = wf.reshape(\n",
    "                    n_times, n_channels)[:,channels]\n",
    "            except:\n",
    "                print(f\"skipped {ctr, spike}\")\n",
    "                skipped_idx.append(ctr)\n",
    "    wfs=np.delete(wfs, skipped_idx, axis=0)\n",
    "    fin.close()\n",
    "\n",
    "    return wfs, skipped_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc9bcba-cd5e-4a90-806a-7b62eb63daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = 'np1_channel_map.npy'\n",
    "triage_quantile = 85\n",
    "do_infer_ptp = False\n",
    "num_spikes_cluster = None\n",
    "min_cluster_size = 25\n",
    "min_samples = 25\n",
    "num_spikes_plot = 250\n",
    "num_rows_plot = 3\n",
    "no_verbose = True\n",
    "\n",
    "data_path = '/media/cat/cole/'\n",
    "data_name = 'CSH_ZAD_026_1800_1860'\n",
    "data_dir = data_path + data_name + '/'\n",
    "raw_data_bin = data_dir + '1min_standardized.bin'\n",
    "residual_data_bin = data_dir + 'residual_1min_standardized_t_0_None.bin'\n",
    "\n",
    "#load features\n",
    "spike_index = np.load(data_dir+'spike_index.npy')\n",
    "num_spikes = spike_index.shape[0]\n",
    "spike_index[:,0] = spike_index[:,0] #only for Hyun's data\n",
    "results_localization = np.load(data_dir+'localization_results.npy')\n",
    "ptps_localized = np.load(data_dir+'ptps.npy')\n",
    "geom_array = np.load(data_dir+geom)\n",
    "#AE features not used at this point.\n",
    "# ae_features = np.load(data_dir+'ae_features.npy') \n",
    "# register displacement (here starts at sec 50)\n",
    "# displacement = np.load(data_dir+'displacement_array.npy' )(if you have displacement)\n",
    "# z_abs = results_localization[:, 1] - displacement[spike_index[:, 0]//30000] (if you have displacement)\n",
    "z_abs =  np.load(data_dir+'z_reg.npy') #if you already have registered zs\n",
    "x = results_localization[:, 0]\n",
    "y = results_localization[:, 2]\n",
    "z = z_abs\n",
    "alpha = results_localization[:, 3]\n",
    "maxptps = results_localization[:, 4]\n",
    "first_chan = results_localization[:, 5]\n",
    "subtracted_wfs = np.load(data_dir+'subtracted_wfs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement_estimate = np.load(data_dir+'displacement.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement_estimate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2bba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform triaging \n",
    "triaged_x, triaged_y, triaged_z, triaged_alpha, triaged_maxptps, _, ptp_filter, idx_keep = run_weighted_triage(x, y, z, alpha, maxptps, threshold=75, ptp_threshold=3, ptp_weighting=True) #pcs is None here\n",
    "# triaged_x, triaged_y, triaged_z, triaged_alpha, triaged_maxptps, _, ptp_filter, idx_keep = run_weighted_triage(x, y, z, alpha, maxptps, threshold=100, ptp_threshold=0, ptp_weighting=False) #pcs is None here\n",
    "triaged_spike_index = spike_index[ptp_filter][idx_keep]\n",
    "triaged_mcs_abs = spike_index[:,1][ptp_filter][idx_keep]\n",
    "triaged_sub_wfs = subtracted_wfs[ptp_filter][idx_keep]\n",
    "triaged_first_chan = first_chan[ptp_filter][idx_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3d5a9-727a-4c56-8a5e-c8fcbffb3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(spike_index[:,1].size, dtype=bool)\n",
    "mask[ptp_filter[0][idx_keep]] = False\n",
    "triaged_indices = np.where(mask)[0]\n",
    "# np.save('triaged_indices', triaged_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776db0a-cfba-4140-8325-b67f9bf1e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can infer ptp\n",
    "if do_infer_ptp:\n",
    "    if no_verbose:\n",
    "        print(f\"inferring ptps using registered zs..\")\n",
    "    def infer_ptp(x, y, z, alpha):\n",
    "        return (alpha / np.sqrt((geom_array[:, 0] - x)**2 + (geom_array[:, 1] - z)**2 + y**2)).max()\n",
    "    vinfer_ptp = np.vectorize(infer_ptp)\n",
    "    triaged_maxptps = vinfer_ptp(triaged_x, triaged_y, triaged_z, triaged_alpha)\n",
    "\n",
    "#load firstchans\n",
    "#triaged_firstchans = results_localization[:,5][ptp_filter][idx_keep] #if you saved firstchans\n",
    "filename = data_dir + \"subtraction_1min_standardized_t_0_None.h5\"\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[2]\n",
    "    firstchans = np.asarray(list(f[\"first_channels\"]))\n",
    "    print(f[\"end_sample\"])\n",
    "    end_sample = f[\"end_sample\"][()]\n",
    "    start_sample = f[\"start_sample\"][()]\n",
    "triaged_firstchans = firstchans[ptp_filter][idx_keep]\n",
    "end_time = end_sample / 30000\n",
    "start_time = start_sample / 30000\n",
    "recording_duration = end_time - start_time\n",
    "print(f\"duration of recording: {recording_duration} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b3bd2-5071-481b-a26a-25c266988595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kilosort results\n",
    "kilo_spike_samples = np.load(data_dir + 'kilosort_spk_samples.npy')\n",
    "kilo_spike_frames = (kilo_spike_samples - 30*recording_duration*30000) #+18 to match our detection alignment\n",
    "kilo_spike_clusters = np.load(data_dir + 'kilsort_spk_clusters.npy')\n",
    "kilo_spike_depths = np.load(data_dir + 'kilsort_spk_depths.npy')\n",
    "kilo_cluster_depth_means = {}\n",
    "for cluster_id in np.unique(kilo_spike_clusters):\n",
    "    kilo_cluster_depth_means[cluster_id] = np.mean(kilo_spike_depths[kilo_spike_clusters==cluster_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e36cc-bd93-4f8a-b92c-09133dfe65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create feature set for clustering\n",
    "if num_spikes_cluster is None:\n",
    "    num_spikes = triaged_x.shape[0]\n",
    "else:\n",
    "    num_spikes = num_spikes_cluster\n",
    "triaged_firstchans = triaged_firstchans[:num_spikes]\n",
    "triaged_alpha = triaged_alpha[:num_spikes]\n",
    "triaged_spike_index = triaged_spike_index[:num_spikes]\n",
    "triaged_x = triaged_x[:num_spikes]\n",
    "triaged_y = triaged_y[:num_spikes]\n",
    "triaged_z = triaged_z[:num_spikes]\n",
    "triaged_maxptps = triaged_maxptps[:num_spikes]\n",
    "triaged_mcs_abs = triaged_mcs_abs[:num_spikes]\n",
    "triaged_first_chan = triaged_first_chan[:num_spikes]\n",
    "\n",
    "scales = (1,10,1,15,30) #predefined scales for each feature\n",
    "features = np.concatenate((np.expand_dims(triaged_x,1), np.expand_dims(triaged_z,1), np.expand_dims(np.log(triaged_maxptps)*scales[4],1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa888c6-175f-41f3-a458-449514149b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform hdbscan clustering\n",
    "min_cluster_size =  min_cluster_size\n",
    "min_samples = min_samples\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "clusterer.fit(features)\n",
    "if no_verbose:\n",
    "    print(clusterer)\n",
    "cluster_centers = []\n",
    "for label in np.unique(clusterer.labels_):\n",
    "    if label != -1:\n",
    "        cluster_centers.append(clusterer.weighted_cluster_centroid(label))\n",
    "cluster_centers = np.asarray(cluster_centers)\n",
    "\n",
    "#re-label each cluster by z-depth\n",
    "labels_depth = np.argsort(-cluster_centers[:,1])\n",
    "label_to_id = {}\n",
    "for i, label in enumerate(labels_depth):\n",
    "    label_to_id[label] = i\n",
    "label_to_id[-1] = -1\n",
    "new_labels = np.vectorize(label_to_id.get)(clusterer.labels_) \n",
    "clusterer.labels_ = new_labels\n",
    "\n",
    "#re-compute cluster centers\n",
    "cluster_centers = []\n",
    "for label in np.unique(clusterer.labels_):\n",
    "    if label != -1:\n",
    "        cluster_centers.append(clusterer.weighted_cluster_centroid(label))\n",
    "cluster_centers = np.asarray(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615c03b-77e2-4555-b794-0f13723a0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir = cm.get_cmap('viridis')\n",
    "triaged_log_ptp = triaged_maxptps.copy()\n",
    "triaged_log_ptp[triaged_log_ptp >= 27.5] = 27.5\n",
    "triaged_log_ptp = np.log(triaged_log_ptp+1)\n",
    "triaged_log_ptp[triaged_log_ptp<=1.25] = 1.25\n",
    "triaged_ptp_rescaled = (triaged_log_ptp - triaged_log_ptp.min())/(triaged_log_ptp.max() - triaged_log_ptp.min())\n",
    "color_arr = vir(triaged_ptp_rescaled)\n",
    "color_arr[:, 3] = triaged_ptp_rescaled\n",
    "\n",
    "# ## Define colors\n",
    "unique_colors = ['#e6194b', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#008080', '#e6beff', '#9a6324', '#800000', '#aaffc3', '#808000', '#000075', '#000000']\n",
    "\n",
    "cluster_color_dict = {}\n",
    "for cluster_id in np.unique(clusterer.labels_):\n",
    "    cluster_color_dict[cluster_id] = unique_colors[cluster_id % len(unique_colors)]\n",
    "cluster_color_dict[-1] = '#808080' #set outlier color to grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a91e40-3639-42f4-948b-240617eeddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### plot array scatter #####\n",
    "fig = plot_array_scatter(clusterer.labels_, geom_array, triaged_x, triaged_z, triaged_maxptps, cluster_color_dict, color_arr, min_cluster_size=clusterer.min_cluster_size, min_samples=clusterer.min_samples, \n",
    "                         z_cutoff=(0, 3900), figsize=(18, 24))\n",
    "# fig.suptitle(f'x,z,scaled_logptp features,\" {num_spikes} datapoints');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55680099",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597c3c3-b7b3-4fcf-855b-dc5e613f4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_LDA_split(wfs_unit_denoised, n_channels = 10, n_times=121):\n",
    "    lda_model = LDA(n_components = 2)\n",
    "    arr = wfs_unit_denoised.ptp(1).argmax(1)\n",
    "    if np.unique(arr).shape[0]<=2:\n",
    "        arr[-1] = np.unique(arr)[0]-1\n",
    "        arr[0] = np.unique(arr)[-1]+1\n",
    "    lda_comps = lda_model.fit_transform(wfs_unit_denoised.reshape((-1, n_times*n_channels)), arr)\n",
    "    lda_clusterer = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=25)\n",
    "    lda_clusterer.fit(lda_comps)\n",
    "    return lda_clusterer.labels_\n",
    "\n",
    "def split_clusters_better(residual_path, waveforms, first_chans, spike_index, labels, x, z, ptps, geom_array, denoiser, device, n_channels=10):\n",
    "    labels_new = labels.copy()\n",
    "    labels_original = labels.copy()\n",
    "\n",
    "    n_clusters = labels.max()\n",
    "    for unit in tqdm(np.unique(labels)[1:]):\n",
    "        spike_index_unit = spike_index[labels == unit]\n",
    "        waveforms_unit = waveforms[labels == unit]\n",
    "        first_chans_unit = first_chans[labels == unit]\n",
    "        x_unit, z_unit, ptps_unit = x[labels == unit], z[labels == unit], ptps[labels == unit]\n",
    "        is_split, unit_new_labels = split_individual_cluster_better(residual_path, waveforms_unit, first_chans_unit, spike_index_unit, x_unit, z_unit, ptps_unit, geom_array, denoiser, device, n_channels)\n",
    "        if is_split:\n",
    "            for new_label in np.unique(unit_new_labels):\n",
    "                if new_label == -1:\n",
    "                    idx = np.flatnonzero(labels_original == unit)[unit_new_labels == new_label]\n",
    "                    labels_new[idx] = new_label\n",
    "                elif new_label > 0:\n",
    "                    n_clusters += 1\n",
    "                    idx = np.flatnonzero(labels_original == unit)[unit_new_labels == new_label]\n",
    "                    labels_new[idx] = n_clusters\n",
    "    return labels_new\n",
    "\n",
    "def split_individual_cluster_better(residual_path, waveforms_unit, first_chans_unit, spike_index_unit, x_unit, z_unit, ptps_unit, geom_array, denoiser, device, n_channels = 10):\n",
    "    total_channels = geom_array.shape[0]\n",
    "    n_channels_half = n_channels//2\n",
    "    labels_unit = -1*np.ones(spike_index_unit.shape[0])\n",
    "    is_split = False\n",
    "    true_mc = int(np.median(spike_index_unit[:, 1]))\n",
    "    mc = max(n_channels_half, true_mc)\n",
    "    mc = min(total_channels - n_channels_half, mc)\n",
    "    pca_model = PCA(2)\n",
    "    wfs_unit = np.zeros((waveforms_unit.shape[0], waveforms_unit.shape[1], n_channels))\n",
    "    for i in range(wfs_unit.shape[0]):\n",
    "        if mc == n_channels_half:\n",
    "            wfs_unit[i] = waveforms_unit[i, :, :n_channels]\n",
    "        elif mc == total_channels - n_channels_half:\n",
    "            wfs_unit[i] = waveforms_unit[i, :, waveforms_unit.shape[2]-n_channels:]\n",
    "        else:\n",
    "            mc_new = int(mc - first_chans_unit[i])\n",
    "            wfs_unit[i] = waveforms_unit[i, :, mc_new-n_channels_half:mc_new+n_channels_half]\n",
    "    \n",
    "    wfs_unit += read_waveforms(spike_index_unit[:, 0], residual_path, geom_array, n_times=121, channels = np.arange(mc-n_channels_half, mc+n_channels_half))[0]\n",
    "    wfs_unit_denoised = denoise_wf_nn_tmp_single_channel(wfs_unit, denoiser, device)\n",
    "    \n",
    "    if true_mc<n_channels_half:\n",
    "        pcs = pca_model.fit_transform(wfs_unit_denoised[:, :, true_mc])\n",
    "    if true_mc>total_channels-n_channels_half:\n",
    "        true_mc = true_mc - (total_channels-n_channels)\n",
    "        pcs = pca_model.fit_transform(wfs_unit_denoised[:, :, true_mc])\n",
    "    else:\n",
    "        pcs = pca_model.fit_transform(wfs_unit_denoised[:, :, n_channels_half])\n",
    "\n",
    "    alpha1 = (x_unit.max() - x_unit.min())/(pcs[:, 0].max()-pcs[:, 0].min())\n",
    "    alpha2 = (x_unit.max() - x_unit.min())/(pcs[:, 1].max()-pcs[:, 1].min())\n",
    "    features = np.concatenate((np.expand_dims(x_unit,1), np.expand_dims(z_unit,1), np.expand_dims(pcs[:, 0], 1)*alpha1, np.expand_dims(pcs[:, 1], 1)*alpha2, np.expand_dims(np.log(ptps_unit)*30,1)), axis=1) #Use scales parameter\n",
    "    clusterer_herding = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=25)\n",
    "    clusterer_herding.fit(features)\n",
    "    \n",
    "    labels_rec_hdbscan = clusterer_herding.labels_\n",
    "    if np.unique(labels_rec_hdbscan).shape[0]>1:\n",
    "        is_split = True\n",
    "# Don't do diptest Here\n",
    "#         diptest_comps = lda_diptest.fit_transform(wfs_unit_denoised.reshape((-1, 121*10)), clusterer_herding.labels_)\n",
    "#         value_dpt, cut_calue = isocut(diptest_comps[:, 0])\n",
    "#         if value_dpt > 0.5:\n",
    "#             is_split = True\n",
    "    \n",
    "    if is_split:\n",
    "        labels_unit[labels_rec_hdbscan==-1] = -1\n",
    "        label_max_temp = labels_rec_hdbscan.max()\n",
    "        cmp = 0\n",
    "        for new_unit_id in np.unique(labels_rec_hdbscan)[1:]:\n",
    "            wfs_new_unit = wfs_unit_denoised[labels_rec_hdbscan == new_unit_id]\n",
    "            lda_labels = run_LDA_split(wfs_new_unit)\n",
    "            if np.unique(lda_labels).shape[0]==1:\n",
    "                labels_unit[labels_rec_hdbscan==new_unit_id] = cmp\n",
    "                cmp += 1\n",
    "            else:\n",
    "                for lda_unit in np.unique(lda_labels):\n",
    "                    if lda_unit >= 0:\n",
    "                        labels_unit[np.flatnonzero(labels_rec_hdbscan==new_unit_id)[lda_labels == lda_unit]] = cmp\n",
    "                        cmp += 1\n",
    "                    else:\n",
    "                        labels_unit[np.flatnonzero(labels_rec_hdbscan==new_unit_id)[lda_labels == lda_unit]] = -1\n",
    "    else:\n",
    "        lda_labels = run_LDA_split(wfs_unit_denoised)\n",
    "        if np.unique(lda_labels).shape[0]>1:\n",
    "            is_split = True\n",
    "            labels_unit = lda_labels\n",
    "\n",
    "    return is_split, labels_unit\n",
    "\n",
    "def split_individual_cluster(standardized_path, spike_index_unit, x_unit, z_unit, ptps_unit, geom_array, denoiser, device, n_channels = 10):\n",
    "    total_channels = geom_array.shape[0]\n",
    "    n_channels_half = n_channels//2\n",
    "    labels_unit = -1*np.ones(spike_index_unit.shape[0])\n",
    "    is_split = False\n",
    "    true_mc = int(np.median(spike_index_unit[:, 1]))\n",
    "    mc = max(n_channels_half, true_mc)\n",
    "    mc = min(total_channels - n_channels_half, mc)\n",
    "    pca_model = PCA(2)\n",
    "    wfs_unit = read_waveforms(spike_index_unit[:, 0], standardized_path, geom_array, n_times=121, channels = np.arange(mc-n_channels_half, mc+n_channels_half))[0]\n",
    "    wfs_unit_denoised = denoise_wf_nn_tmp_single_channel(wfs_unit, denoiser, device)\n",
    "    if true_mc<n_channels_half:\n",
    "        pcs = pca_model.fit_transform(wfs_unit_denoised[:, :, true_mc])\n",
    "    if true_mc>total_channels-n_channels_half:\n",
    "        true_mc = true_mc - (total_channels-n_channels)\n",
    "        pcs = pca_model.fit_transform(wfs_unit_denoised[:, :, true_mc])\n",
    "    else:\n",
    "        pcs = pca_model.fit_transform(wfs_unit_denoised[:, :, n_channels_half])\n",
    "\n",
    "    alpha1 = (x_unit.max() - x_unit.min())/(pcs[:, 0].max()-pcs[:, 0].min())\n",
    "    alpha2 = (x_unit.max() - x_unit.min())/(pcs[:, 1].max()-pcs[:, 1].min())\n",
    "    features = np.concatenate((np.expand_dims(x_unit,1), np.expand_dims(z_unit,1), np.expand_dims(pcs[:, 0], 1)*alpha1, np.expand_dims(pcs[:, 1], 1)*alpha2, np.expand_dims(np.log(ptps_unit)*30,1)), axis=1) #Use scales parameter\n",
    "    clusterer_herding = hdbscan.HDBSCAN(min_cluster_size=25, min_samples=25)\n",
    "    clusterer_herding.fit(features)\n",
    "    \n",
    "    labels_rec_hdbscan = clusterer_herding.labels_\n",
    "    if np.unique(labels_rec_hdbscan).shape[0]>1:\n",
    "        is_split = True\n",
    "# Don't do diptest Here\n",
    "#         diptest_comps = lda_diptest.fit_transform(wfs_unit_denoised.reshape((-1, 121*10)), clusterer_herding.labels_)\n",
    "#         value_dpt, cut_calue = isocut(diptest_comps[:, 0])\n",
    "#         if value_dpt > 0.5:\n",
    "#             is_split = True\n",
    "    \n",
    "    if is_split:\n",
    "        labels_unit[labels_rec_hdbscan==-1] = -1\n",
    "        label_max_temp = labels_rec_hdbscan.max()\n",
    "        cmp = 0\n",
    "        for new_unit_id in np.unique(labels_rec_hdbscan)[1:]:\n",
    "            wfs_new_unit = wfs_unit_denoised[labels_rec_hdbscan == new_unit_id]\n",
    "            lda_labels = run_LDA_split(wfs_new_unit)\n",
    "            if np.unique(lda_labels).shape[0]==1:\n",
    "                labels_unit[labels_rec_hdbscan==new_unit_id] = cmp\n",
    "                cmp += 1\n",
    "            else:\n",
    "                for lda_unit in np.unique(lda_labels):\n",
    "                    if lda_unit >= 0:\n",
    "                        labels_unit[np.flatnonzero(labels_rec_hdbscan==new_unit_id)[lda_labels == lda_unit]] = cmp\n",
    "                        cmp += 1\n",
    "                    else:\n",
    "                        labels_unit[np.flatnonzero(labels_rec_hdbscan==new_unit_id)[lda_labels == lda_unit]] = -1\n",
    "    else:\n",
    "        lda_labels = run_LDA_split(wfs_unit_denoised)\n",
    "        if np.unique(lda_labels).shape[0]>1:\n",
    "            is_split = True\n",
    "            labels_unit = lda_labels\n",
    "\n",
    "    return is_split, labels_unit\n",
    "\n",
    "def split_clusters(standardized_path, spike_index, labels, x, z, ptps, geom_array, denoiser, device, n_channels=10):\n",
    "    labels_new = labels.copy()\n",
    "    labels_original = labels.copy()\n",
    "\n",
    "    n_clusters = labels.max()\n",
    "    for unit in (np.unique(labels)[1:]):\n",
    "        spike_index_unit = spike_index[labels == unit]\n",
    "        x_unit, z_unit, ptps_unit = x[labels == unit], z[labels == unit], ptps[labels == unit]\n",
    "        is_split, unit_new_labels = split_individual_cluster(standardized_path, spike_index_unit, x_unit, z_unit, ptps_unit, geom_array, denoiser, device, n_channels)\n",
    "        if is_split:\n",
    "            for new_label in np.unique(unit_new_labels):\n",
    "                if new_label == -1:\n",
    "                    idx = np.flatnonzero(labels_original == unit)[unit_new_labels == new_label]\n",
    "                    labels_new[idx] = new_label\n",
    "                elif new_label > 0:\n",
    "                    n_clusters += 1\n",
    "                    idx = np.flatnonzero(labels_original == unit)[unit_new_labels == new_label]\n",
    "                    labels_new[idx] = n_clusters\n",
    "    return labels_new\n",
    "\n",
    "def get_x_z_templates(n_templates, labels, x, z):\n",
    "    x_z_templates = np.zeros((n_templates, 2))\n",
    "    for i in range(n_templates):\n",
    "        x_z_templates[i, 1] = np.median(z[labels==i])\n",
    "        x_z_templates[i, 0] = np.median(x[labels==i])\n",
    "    return x_z_templates\n",
    "\n",
    "def get_n_spikes_templates(n_templates, labels):\n",
    "    n_spikes_templates = np.zeros(n_templates)\n",
    "    for i in range(n_templates):\n",
    "        n_spikes_templates[i] = (labels==i).sum()\n",
    "    return n_spikes_templates\n",
    "\n",
    "def get_templates(standardized_path, geom_array, n_templates, spike_index, labels, max_spikes=250, n_times=121):\n",
    "    templates = np.zeros((n_templates, n_times, geom_array.shape[0]))\n",
    "    for unit in tqdm(range(n_templates)):\n",
    "        spike_times_unit = spike_index[labels==unit, 0]\n",
    "        if spike_times_unit.shape[0]>max_spikes:\n",
    "            idx = np.random.choice(np.arange(spike_times_unit.shape[0]), max_spikes, replace = False)\n",
    "        else:\n",
    "            idx = np.arange(spike_times_unit.shape[0])\n",
    "\n",
    "        wfs_unit = read_waveforms(spike_times_unit[idx], standardized_path, geom_array, n_times)[0]\n",
    "        templates[unit] = wfs_unit.mean(0)\n",
    "    return templates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17c0b1-9b39-4649-91fd-a165190b0a0e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "labels = clusterer.labels_\n",
    "z_labels = np.zeros(clusterer.labels_.max()+1)\n",
    "for i in range(clusterer.labels_.max()+1):\n",
    "    z_labels[i] = triaged_z[clusterer.labels_ == i].mean()\n",
    "ordered_labels = labels.copy()\n",
    "z_argsort = z_labels.argsort()[::-1]\n",
    "for i in range(clusterer.labels_.max()+1):\n",
    "    ordered_labels[labels == z_argsort[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05595f91-38a3-4fdf-9f3d-ccbc99909409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2120720-a446-4f05-87cd-819c518d079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_clusterer = get_templates(raw_data_bin, geom_array, ordered_labels.max()+1, triaged_spike_index, ordered_labels)\n",
    "\n",
    "list_argmin = np.zeros(temp_clusterer.shape[0])\n",
    "for i in range(temp_clusterer.shape[0]):\n",
    "    list_argmin[i] = temp_clusterer[i, :, temp_clusterer[i].ptp(0).argmax()].argmin()\n",
    "\n",
    "idx_not_aligned = np.where(list_argmin!=42)[0]\n",
    "\n",
    "for unit in idx_not_aligned:\n",
    "    mc = temp_clusterer[unit].ptp(0).argmax()\n",
    "    offset = temp_clusterer[unit, :, mc].argmin()\n",
    "    triaged_spike_index[ordered_labels == unit, 0] += offset-42\n",
    "\n",
    "idx_sorted = triaged_spike_index[:, 0].argsort()\n",
    "spike_index_triaged = triaged_spike_index[idx_sorted]\n",
    "ordered_labels = ordered_labels[idx_sorted]\n",
    "triaged_x = triaged_x[idx_sorted]\n",
    "triaged_z = triaged_z[idx_sorted]\n",
    "triaged_maxptps = triaged_maxptps[idx_sorted]\n",
    "triaged_sub_wfs = triaged_sub_wfs[idx_sorted]  \n",
    "triaged_first_chan = triaged_first_chan[idx_sorted]  \n",
    "\n",
    "temp_clusterer[94, :, temp_clusterer[94].ptp(0).argmax()].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b8708-d7bb-4303-8335-aff80047bacf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "labels_split = split_clusters_better(residual_data_bin, triaged_sub_wfs, triaged_first_chan, spike_index_triaged, ordered_labels, triaged_x, triaged_z, triaged_maxptps, geom_array, denoiser, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0b359-e7c1-498f-9e63-53d1452bb398",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# labels_split = split_clusters(raw_data_bin, spike_index_triaged, ordered_labels, triaged_x, triaged_z, triaged_maxptps, geom_array, denoiser, device)\n",
    "temp_clusterer = get_templates(raw_data_bin, geom_array, labels_split.max()+1, spike_index_triaged, labels_split)\n",
    "\n",
    "list_argmin = np.zeros(temp_clusterer.shape[0])\n",
    "for i in range(temp_clusterer.shape[0]):\n",
    "    list_argmin[i] = temp_clusterer[i, :, temp_clusterer[i].ptp(0).argmax()].argmin()\n",
    "\n",
    "idx_not_aligned = np.where(list_argmin!=42)[0]\n",
    "\n",
    "for unit in idx_not_aligned:\n",
    "    mc = temp_clusterer[unit].ptp(0).argmax()\n",
    "    offset = temp_clusterer[unit, :, mc].argmin()\n",
    "    spike_index_triaged[labels_split == unit, 0] += offset-42\n",
    "\n",
    "idx_sorted = spike_index_triaged[:, 0].argsort()\n",
    "spike_index_triaged = spike_index_triaged[idx_sorted]\n",
    "labels_split = labels_split[idx_sorted]\n",
    "triaged_x = triaged_x[idx_sorted]\n",
    "triaged_z = triaged_z[idx_sorted]\n",
    "triaged_maxptps = triaged_maxptps[idx_sorted]\n",
    "triaged_first_chan = triaged_first_chan[idx_sorted]  \n",
    "triaged_sub_wfs = triaged_sub_wfs[idx_sorted]  \n",
    "\n",
    "z_labels = np.zeros(labels_split.max()+1)\n",
    "for i in range(labels_split.max()+1):\n",
    "    z_labels[i] = triaged_z[labels_split == i].mean()\n",
    "    \n",
    "ordered_split_labels = labels_split.copy()\n",
    "z_argsort = z_labels.argsort()[::-1]\n",
    "for i in range(labels_split.max()+1):\n",
    "    ordered_split_labels[labels_split == z_argsort[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir = cm.get_cmap('viridis')\n",
    "triaged_log_ptp = triaged_maxptps.copy()\n",
    "triaged_log_ptp[triaged_log_ptp >= 27.5] = 27.5\n",
    "triaged_log_ptp = np.log(triaged_log_ptp+1)\n",
    "triaged_log_ptp[triaged_log_ptp<=1.25] = 1.25\n",
    "triaged_ptp_rescaled = (triaged_log_ptp - triaged_log_ptp.min())/(triaged_log_ptp.max() - triaged_log_ptp.min())\n",
    "color_arr = vir(triaged_ptp_rescaled)\n",
    "color_arr[:, 3] = triaged_ptp_rescaled\n",
    "\n",
    "# ## Define colors\n",
    "unique_colors = ['#e6194b', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#008080', '#e6beff', '#9a6324', '#800000', '#aaffc3', '#808000', '#000075', '#000000']\n",
    "\n",
    "cluster_color_dict = {}\n",
    "for cluster_id in np.unique(ordered_split_labels):\n",
    "    cluster_color_dict[cluster_id] = unique_colors[cluster_id % len(unique_colors)]\n",
    "cluster_color_dict[-1] = '#808080' #set outlier color to grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d420453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_split_labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35637e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_array_scatter(ordered_split_labels, geom_array, triaged_x, triaged_z, triaged_maxptps, cluster_color_dict, color_arr, min_cluster_size=clusterer.min_cluster_size, min_samples=clusterer.min_samples, \n",
    "                         z_cutoff=(0, 3900), figsize=(18, 24))\n",
    "# fig.suptitle(f'x,z,scaled_logptp features,\" {num_spikes} datapoints');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d736fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_shifted_similarity(template1, template2, shifts=np.arange(-8,9)):\n",
    "    curr_similarities = []\n",
    "    for shift in shifts:\n",
    "        if shift == 0:\n",
    "            similarity = np.max(np.abs(template1 - template2))\n",
    "        elif shift < 0:\n",
    "            template2_shifted_flattened = np.pad(template2.T.flatten(),((-shift,0)), mode='constant')[:shift]\n",
    "            similarity = np.max(np.abs(template1.T.flatten() - template2_shifted_flattened))\n",
    "        else:    \n",
    "            template2_shifted_flattened = np.pad(template2.T.flatten(),((0,shift)), mode='constant')[shift:]\n",
    "            similarity = np.max(np.abs(template1.T.flatten() - template2_shifted_flattened))\n",
    "        curr_similarities.append(similarity)\n",
    "    return np.min(curr_similarities), shifts[np.argmin(curr_similarities)]\n",
    "\n",
    "def get_proposed_pairs(n_templates, templates, x_z_templates, n_temp = 20, n_channels = 10):\n",
    "    n_channels_half = n_channels//2\n",
    "    dist = cdist(x_z_templates, x_z_templates)\n",
    "    dist_argsort = dist.argsort(axis = 1)[:, 1:n_temp+1]\n",
    "    dist_template = np.zeros((dist_argsort.shape[0], n_temp))\n",
    "    for i in range(n_templates):\n",
    "        mc = min(templates[i].ptp(0).argmax(), 384-n_channels_half)\n",
    "        mc = max(mc, n_channels_half)\n",
    "        temp_a = templates[i, :, mc-n_channels_half:mc+n_channels_half]\n",
    "        for j in range(n_temp):\n",
    "            temp_b = templates[dist_argsort[i, j], :, mc-n_channels_half:mc+n_channels_half]\n",
    "            dist_template[i, j] = compute_shifted_similarity(temp_a, temp_b)[0]\n",
    "    return dist_argsort, dist_template\n",
    "\n",
    "def get_diptest_value_better(residual_path, waveforms, first_chans, geom_array, spike_index, labels, unit_a, unit_b, n_spikes_templates, mc, two_units_shift, unit_shifted, denoiser, device, n_channels = 10, n_times=121, rank_pca=8, nn_denoise = False):\n",
    "\n",
    "    # ALIGN BASED ON MAX PTP TEMPLATE MC \n",
    "    n_channels_half = n_channels//2\n",
    "\n",
    "    n_wfs_max = int(min(250, min(n_spikes_templates[unit_a], n_spikes_templates[unit_b]))) \n",
    "\n",
    "    \n",
    "    mc = min(384-n_channels_half, mc)\n",
    "    mc = max(n_channels_half, mc)\n",
    "\n",
    "    spike_times_unit_a = spike_index[labels == unit_a, 0]\n",
    "    idx = np.random.choice(np.arange(spike_times_unit_a.shape[0]), n_wfs_max, replace = False)\n",
    "    spike_times_unit_a = spike_times_unit_a[idx]\n",
    "    wfs_a = waveforms[labels == unit_a][idx]\n",
    "    first_chan_a = triaged_first_chan[labels == unit_a][idx]\n",
    "    \n",
    "    spike_times_unit_b = spike_index[labels == unit_b, 0]\n",
    "    idx = np.random.choice(np.arange(spike_times_unit_b.shape[0]), n_wfs_max, replace = False)\n",
    "    spike_times_unit_b = spike_times_unit_b[idx]\n",
    "    wfs_b = waveforms[labels == unit_b][idx]\n",
    "    first_chan_b = triaged_first_chan[labels == unit_b][idx]\n",
    "    \n",
    "    wfs_a_bis = np.zeros((wfs_a.shape[0], n_times, n_channels))\n",
    "    wfs_b_bis = np.zeros((wfs_b.shape[0], n_times, n_channels))\n",
    "    \n",
    "    if two_units_shift>0:\n",
    "        \n",
    "        if unit_shifted == unit_a:\n",
    "            for i in range(wfs_a_bis.shape[0]):\n",
    "                first_chan = int(mc - first_chan_a[i] - n_channels_half)\n",
    "                first_chan = max(0, int(first_chan))\n",
    "                first_chan = min(wfs_a.shape[2]-n_channels, int(first_chan))\n",
    "                wfs_a_bis[i, :-two_units_shift] = wfs_a[i, two_units_shift:, first_chan:first_chan+n_channels]\n",
    "                first_chan = int(mc - first_chan_b[i] - n_channels_half)\n",
    "                first_chan = max(0, int(first_chan))\n",
    "                first_chan = min(wfs_b.shape[2]-n_channels, int(first_chan))\n",
    "                wfs_b_bis[i, :] = wfs_b[i, :, first_chan:first_chan+n_channels]\n",
    "            wfs_a_bis += read_waveforms(spike_times_unit_a+two_units_shift, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "            wfs_b_bis += read_waveforms(spike_times_unit_b, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "        else:\n",
    "            for i in range(wfs_a_bis.shape[0]):\n",
    "                first_chan = int(mc - first_chan_a[i] - n_channels_half)\n",
    "                first_chan = max(0, int(first_chan))\n",
    "                first_chan = min(wfs_a.shape[2]-n_channels, int(first_chan))\n",
    "                wfs_a_bis[i] = wfs_a[i, :, first_chan:first_chan+n_channels]\n",
    "                first_chan = int(mc - first_chan_b[i] - n_channels_half)\n",
    "                first_chan = max(0, int(first_chan))\n",
    "                first_chan = min(wfs_b.shape[2]-n_channels, int(first_chan))\n",
    "                wfs_b_bis[i, :-two_units_shift] = wfs_b[i, two_units_shift:, first_chan:first_chan+n_channels]\n",
    "            wfs_a_bis += read_waveforms(spike_times_unit_a, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "            wfs_b_bis += read_waveforms(spike_times_unit_b+two_units_shift, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "    elif two_units_shift<0:\n",
    "        if unit_shifted == unit_a:\n",
    "            for i in range(wfs_a_bis.shape[0]):\n",
    "                first_chan = int(mc - first_chan_a[i] - 5)\n",
    "                first_chan = max(0, int(first_chan))\n",
    "                first_chan = min(wfs_a.shape[2]-n_channels, int(first_chan))\n",
    "                wfs_a_bis[i, -two_units_shift:] = wfs_a[i, :two_units_shift, first_chan:first_chan+n_channels]\n",
    "                first_chan = int(mc - first_chan_b[i] - 5)\n",
    "                first_chan = max(0, int(first_chan))\n",
    "                first_chan = min(wfs_b.shape[2]-n_channels, int(first_chan))\n",
    "                wfs_b_bis[i, :] = wfs_b[i, :, first_chan:first_chan+n_channels]\n",
    "            wfs_a_bis += read_waveforms(spike_times_unit_a+two_units_shift, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "            wfs_b_bis += read_waveforms(spike_times_unit_b, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "\n",
    "        else:\n",
    "            for i in range(wfs_a_bis.shape[0]):\n",
    "                first_chan = int(mc - first_chan_a[i] - 5)\n",
    "                first_chan = max(0, int(first_chan))\n",
    "                first_chan = min(wfs_a.shape[2]-n_channels, int(first_chan))\n",
    "                wfs_a_bis[i] = wfs_a[i, :, first_chan:first_chan+n_channels]\n",
    "                first_chan = int(mc - first_chan_b[i] - 5)\n",
    "                first_chan = max(0, int(first_chan))\n",
    "                first_chan = min(wfs_b.shape[2]-n_channels, int(first_chan))\n",
    "                wfs_b_bis[i, -two_units_shift:] = wfs_b[i, :two_units_shift, first_chan:first_chan+n_channels]\n",
    "            wfs_a_bis += read_waveforms(spike_times_unit_a, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "            wfs_b_bis += read_waveforms(spike_times_unit_b+two_units_shift, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "    else:\n",
    "        for i in range(wfs_a_bis.shape[0]):\n",
    "            first_chan = int(mc - first_chan_a[i] - 5)\n",
    "            first_chan = max(0, int(first_chan))\n",
    "            first_chan = min(wfs_a.shape[2]-n_channels, int(first_chan))\n",
    "            wfs_a_bis[i] = wfs_a[i, :, first_chan:first_chan+n_channels]\n",
    "            first_chan = int(mc - first_chan_b[i] - 5)\n",
    "            first_chan = max(0, int(first_chan))\n",
    "            first_chan = min(wfs_b.shape[2]-n_channels, int(first_chan))\n",
    "            wfs_b_bis[i, :] = wfs_b[i, :, first_chan:first_chan+n_channels]\n",
    "        wfs_a_bis += read_waveforms(spike_times_unit_a, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "        wfs_b_bis += read_waveforms(spike_times_unit_b, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]      \n",
    "    \n",
    "    \n",
    "    tpca = PCA(rank_pca)\n",
    "    wfs_diptest = np.concatenate((wfs_a, wfs_b))\n",
    "    \n",
    "    if nn_denoise:\n",
    "        wfs_diptest = denoise_wf_nn_tmp_single_channel(wfs_diptest, denoiser, device)\n",
    "\n",
    "    N, T, C = wfs_diptest.shape\n",
    "    wfs_diptest = wfs_diptest.transpose(0, 2, 1).reshape(N*C, T)\n",
    "    wfs_diptest = tpca.inverse_transform(tpca.fit_transform(wfs_diptest))\n",
    "    wfs_diptest = wfs_diptest.reshape(N, C, T).transpose(0, 2, 1).reshape((N, C*T))\n",
    "    \n",
    "#     wfs_diptest = np.concatenate((wfs_a_bis, wfs_b_bis)).reshape((-1, n_channels*n_times))\n",
    "    labels_diptest = np.zeros(wfs_a_bis.shape[0]+wfs_b_bis.shape[0])\n",
    "    labels_diptest[:wfs_a_bis.shape[0]] = 1\n",
    "    \n",
    "    lda_model = LDA(n_components = 1)\n",
    "    lda_comps = lda_model.fit_transform(wfs_diptest, labels_diptest)\n",
    "    value_dpt, cut_calue = isocut(lda_comps[:, 0])\n",
    "    return value_dpt\n",
    "\n",
    "def get_merged_better(residual_path, waveforms, first_chans, geom_array, templates, n_spikes_templates, x_z_templates, n_templates, spike_index, labels, x, z, denoiser, device, n_channels=10, n_temp = 10, distance_threshold = 3., threshold_diptest = .75, rank_pca=8, nn_denoise = False):\n",
    "     \n",
    "    n_spikes_templates = get_n_spikes_templates(n_templates, labels)\n",
    "    x_z_templates = get_x_z_templates(n_templates, labels, x, z)\n",
    "    print(\"GET PROPOSED PAIRS\")\n",
    "    dist_argsort, dist_template = get_proposed_pairs(n_templates, templates, x_z_templates, n_temp = n_temp)\n",
    "    \n",
    "    labels_updated = labels.copy()\n",
    "    reference_units = np.unique(labels)[1:]\n",
    "    \n",
    "    for unit in tqdm(range(n_templates)): #tqdm\n",
    "        unit_reference = reference_units[unit]\n",
    "        to_be_merged = [unit_reference]\n",
    "        merge_shifts = [0]\n",
    "        is_merged = False\n",
    "\n",
    "        for j in range(n_temp):\n",
    "            if dist_template[unit, j] < distance_threshold:\n",
    "                unit_bis = dist_argsort[unit, j]\n",
    "                unit_bis_reference = reference_units[unit_bis]\n",
    "                if unit_reference != unit_bis_reference:\n",
    "#                     ALIGN BASED ON MAX PTP TEMPLATE MC \n",
    "                    if templates[unit_reference].ptp(0).max() < templates[unit_bis_reference].ptp(0).max():\n",
    "                        mc = templates[unit_bis_reference].ptp(0).argmax()\n",
    "                        two_units_shift = templates[unit_reference, :, mc].argmin() - templates[unit_bis_reference, :, mc].argmin()\n",
    "                        unit_shifted = unit_reference\n",
    "                    else:\n",
    "                        mc = templates[unit_reference].ptp(0).argmax()\n",
    "                        two_units_shift = templates[unit_bis_reference, :, mc].argmin() - templates[unit_reference, :, mc].argmin()\n",
    "                        unit_shifted = unit_bis_reference\n",
    "                    dpt_val = get_diptest_value_better(residual_path, waveforms, first_chans, geom_array, spike_index, labels_updated, unit_reference, unit_bis_reference, n_spikes_templates, mc, two_units_shift,unit_shifted, denoiser, device, n_channels, rank_pca=rank_pca, nn_denoise = nn_denoise)\n",
    "                    if dpt_val<threshold_diptest and np.abs(two_units_shift)<2:\n",
    "                        to_be_merged.append(unit_bis_reference)\n",
    "                        if unit_shifted == unit_bis_reference:\n",
    "                            merge_shifts.append(-two_units_shift)\n",
    "                        else:\n",
    "                            merge_shifts.append(two_units_shift)\n",
    "                        is_merged = True\n",
    "        if is_merged:\n",
    "            n_total_spikes = 0\n",
    "            for unit_merged in np.unique(np.asarray(to_be_merged)):\n",
    "                n_total_spikes += n_spikes_templates[unit_merged]\n",
    "\n",
    "            new_reference_unit = np.unique(np.asarray(to_be_merged))[0]\n",
    "\n",
    "            templates[new_reference_unit] = n_spikes_templates[new_reference_unit]*templates[new_reference_unit]/n_total_spikes\n",
    "            cmp = 1\n",
    "            for unit_merged in np.unique(np.asarray(to_be_merged))[1:]:\n",
    "                shift_ = merge_shifts[cmp]\n",
    "                templates[new_reference_unit] += n_spikes_templates[unit_merged]*np.roll(templates[unit_merged], shift_, axis = 0)/n_total_spikes\n",
    "                n_spikes_templates[new_reference_unit] += n_spikes_templates[unit_merged]\n",
    "                n_spikes_templates[unit_merged] = 0\n",
    "                labels_updated[labels_updated == unit_merged] = new_reference_unit\n",
    "                reference_units[unit_merged] = new_reference_unit\n",
    "                cmp += 1\n",
    "    return labels_updated\n",
    "\n",
    "\n",
    "# def get_diptest_value(standardized_path, geom_array, spike_index, labels, unit_a, unit_b, n_spikes_templates, mc, two_units_shift, unit_shifted, denoiser, device, n_channels = 10, n_times=121):\n",
    "\n",
    "#     # ALIGN BASED ON MAX PTP TEMPLATE MC \n",
    "#     n_channels_half = n_channels//2\n",
    "\n",
    "#     n_wfs_max = int(min(500, min(n_spikes_templates[unit_a], n_spikes_templates[unit_b]))) \n",
    "#     if unit_b == unit_shifted:\n",
    "#         spike_index_unit_a = spike_index[labels == unit_a, 0] #denoiser offset ## SHIFT BASED ON TEMPLATES ARGMIN PN MAX PTP TEMPLATE\n",
    "#         spike_index_unit_b = spike_index[labels == unit_b, 0]+two_units_shift #denoiser offset\n",
    "#     else:\n",
    "#         spike_index_unit_a = spike_index[labels == unit_a, 0]+two_units_shift #denoiser offset ## SHIFT BASED ON TEMPLATES ARGMIN PN MAX PTP TEMPLATE\n",
    "#         spike_index_unit_b = spike_index[labels == unit_b, 0] #denoiser offset\n",
    "#     idx = np.random.choice(np.arange(spike_index_unit_a.shape[0]), n_wfs_max, replace = False)\n",
    "#     spike_times_unit_a = spike_index_unit_a[idx]\n",
    "#     idx = np.random.choice(np.arange(spike_index_unit_b.shape[0]), n_wfs_max, replace = False)\n",
    "#     spike_times_unit_b = spike_index_unit_b[idx]\n",
    "#     mc = min(384-n_channels_half, mc)\n",
    "#     mc = max(n_channels_half, mc)\n",
    "#     wfs_a = read_waveforms(spike_times_unit_a, standardized_path, geom_array, n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#     wfs_b = read_waveforms(spike_times_unit_b, standardized_path, geom_array, n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#     wfs_a = denoise_wf_nn_tmp_single_channel(wfs_a, denoiser, device)\n",
    "#     wfs_b = denoise_wf_nn_tmp_single_channel(wfs_b, denoiser, device)\n",
    "#     wfs_diptest = np.concatenate((wfs_a, wfs_b)).reshape((-1, n_channels*n_times))\n",
    "#     labels_diptest = np.zeros(wfs_a.shape[0]+wfs_b.shape[0])\n",
    "#     labels_diptest[:wfs_a.shape[0]] = 1\n",
    "    \n",
    "    \n",
    "#     lda_model = LDA(n_components = 1)\n",
    "#     lda_comps = lda_model.fit_transform(wfs_diptest, labels_diptest)\n",
    "#     value_dpt, cut_calue = isocut(lda_comps[:, 0])\n",
    "#     return value_dpt\n",
    "\n",
    "    \n",
    "# def get_merged(standardized_path, geom_array, n_templates, spike_index, labels, x, z, denoiser, device, n_channels=10, n_temp = 10, distance_threshold = 3., threshold_diptest = 1.):\n",
    "     \n",
    "#     templates = get_templates(standardized_path, geom_array, n_templates, spike_index, labels)\n",
    "#     n_spikes_templates = get_n_spikes_templates(n_templates, labels)\n",
    "#     x_z_templates = get_x_z_templates(n_templates, labels, x, z)\n",
    "#     print(\"GET PROPOSED PAIRS\")\n",
    "#     dist_argsort, dist_template = get_proposed_pairs(n_templates, templates, x_z_templates, n_temp = n_temp)\n",
    "    \n",
    "#     labels_updated = labels.copy()\n",
    "#     reference_units = np.unique(labels)[1:]\n",
    "    \n",
    "#     for unit in tqdm(range(n_templates)): #tqdm\n",
    "#         unit_reference = reference_units[unit]\n",
    "#         to_be_merged = [unit_reference]\n",
    "#         merge_shifts = [0]\n",
    "#         is_merged = False\n",
    "\n",
    "#         for j in range(n_temp):\n",
    "#             if dist_template[unit, j] < distance_threshold:\n",
    "#                 unit_bis = dist_argsort[unit, j]\n",
    "#                 unit_bis_reference = reference_units[unit_bis]\n",
    "#                 if unit_reference != unit_bis_reference:\n",
    "# #                     ALIGN BASED ON MAX PTP TEMPLATE MC \n",
    "#                     if templates[unit_reference].ptp(0).max() < templates[unit_bis_reference].ptp(0).max():\n",
    "#                         mc = templates[unit_bis_reference].ptp(0).argmax()\n",
    "#                         two_units_shift = templates[unit_reference, :, mc].argmin() - templates[unit_bis_reference, :, mc].argmin()\n",
    "#                         unit_shifted = unit_reference\n",
    "#                     else:\n",
    "#                         mc = templates[unit_reference].ptp(0).argmax()\n",
    "#                         two_units_shift = templates[unit_bis_reference, :, mc].argmin() - templates[unit_reference, :, mc].argmin()\n",
    "#                         unit_shifted = unit_bis_reference\n",
    "#                     dpt_val = get_diptest_value(standardized_path, geom_array, spike_index, labels_updated, unit_reference, unit_bis_reference, n_spikes_templates, mc, two_units_shift,unit_shifted, denoiser, device, n_channels)\n",
    "#                     if dpt_val<threshold_diptest and np.abs(two_units_shift)<4:\n",
    "#                         to_be_merged.append(unit_bis_reference)\n",
    "#                         if unit_shifted == unit_bis_reference:\n",
    "#                             merge_shifts.append(-two_units_shift)\n",
    "#                         else:\n",
    "#                             merge_shifts.append(two_units_shift)\n",
    "#                         is_merged = True\n",
    "#         if is_merged:\n",
    "#             n_total_spikes = 0\n",
    "#             for unit_merged in np.unique(np.asarray(to_be_merged)):\n",
    "#                 n_total_spikes += n_spikes_templates[unit_merged]\n",
    "\n",
    "#             new_reference_unit = np.unique(np.asarray(to_be_merged))[0]\n",
    "\n",
    "#             templates[new_reference_unit] = n_spikes_templates[new_reference_unit]*templates[new_reference_unit]/n_total_spikes\n",
    "#             cmp = 1\n",
    "#             for unit_merged in np.unique(np.asarray(to_be_merged))[1:]:\n",
    "#                 shift_ = merge_shifts[cmp]\n",
    "#                 templates[new_reference_unit] += n_spikes_templates[unit_merged]*np.roll(templates[unit_merged], shift_, axis = 0)/n_total_spikes\n",
    "#                 n_spikes_templates[new_reference_unit] += n_spikes_templates[unit_merged]\n",
    "#                 n_spikes_templates[unit_merged] = 0\n",
    "#                 labels_updated[labels_updated == unit_merged] = new_reference_unit\n",
    "#                 reference_units[unit_merged] = new_reference_unit\n",
    "#                 cmp += 1\n",
    "#     return labels_updated\n",
    "\n",
    "# def get_diptest_value_yass(residual_path, waveforms, spatial_cov, temporal_whitener, geom_array, spike_index, labels, unit_a, unit_b, n_spikes_templates, mc, two_units_shift, unit_shifted, denoiser, device, n_channels = 10, n_times=121):\n",
    "\n",
    "#     # ALIGN BASED ON MAX PTP TEMPLATE MC \n",
    "#     n_channels_half = n_channels//2\n",
    "\n",
    "#     n_wfs_max = int(min(250, min(n_spikes_templates[unit_a], n_spikes_templates[unit_b]))) \n",
    "\n",
    "    \n",
    "#     mc = min(384-n_channels_half, mc)\n",
    "#     mc = max(n_channels_half, mc)\n",
    "\n",
    "#     spike_times_unit_a = spike_index[labels == unit_a, 0]\n",
    "#     idx = np.random.choice(np.arange(spike_times_unit_a.shape[0]), n_wfs_max, replace = False)\n",
    "#     spike_times_unit_a = spike_times_unit_a[idx]\n",
    "#     wfs_a = waveforms[labels == unit_a][idx]\n",
    "#     first_chan_a = triaged_first_chan[labels == unit_a][idx]\n",
    "    \n",
    "#     spike_times_unit_b = spike_index[labels == unit_b, 0]\n",
    "#     idx = np.random.choice(np.arange(spike_times_unit_b.shape[0]), n_wfs_max, replace = False)\n",
    "#     spike_times_unit_b = spike_times_unit_b[idx]\n",
    "#     wfs_b = waveforms[labels == unit_b][idx]\n",
    "#     first_chan_b = triaged_first_chan[labels == unit_b][idx]\n",
    "    \n",
    "#     wfs_a_bis = np.zeros((wfs_a.shape[0], n_times, n_channels))\n",
    "#     wfs_b_bis = np.zeros((wfs_b.shape[0], n_times, n_channels))\n",
    "    \n",
    "#     if two_units_shift>0:\n",
    "        \n",
    "#         if unit_shifted == unit_a:\n",
    "#             for i in range(wfs_a_bis.shape[0]):\n",
    "#                 first_chan = int(mc - first_chan_a[i] - 5)\n",
    "#                 first_chan = min(384-n_channels, first_chan)\n",
    "#                 first_chan = max(n_channels, first_chan)\n",
    "#                 wfs_a_bis[i, :-two_units_shift] = wfs_a[i, two_units_shift:, first_chan:first_chan+n_channels]\n",
    "#                 first_chan = int(mc - first_chan_b[i] - 5)\n",
    "#                 first_chan = min(384-n_channels, first_chan)\n",
    "#                 first_chan = max(n_channels, first_chan)\n",
    "#                 wfs_b_bis[i, :] = wfs_b[i, :, first_chan:first_chan+n_channels]\n",
    "#             wfs_a_bis += read_waveforms(spike_times_unit_a+two_units_shift, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#             wfs_b_bis += read_waveforms(spike_times_unit_b, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#         else:\n",
    "#             for i in range(wfs_a_bis.shape[0]):\n",
    "#                 first_chan = int(mc - first_chan_a[i] - 5)\n",
    "#                 first_chan = min(384-n_channels, first_chan)\n",
    "#                 first_chan = max(n_channels, first_chan)\n",
    "#                 wfs_a_bis[i] = wfs_a[i, :, first_chan:first_chan+n_channels]\n",
    "#                 first_chan = int(mc - first_chan_b[i] - 5)\n",
    "#                 first_chan = min(384-n_channels, first_chan)\n",
    "#                 first_chan = max(n_channels, first_chan)\n",
    "#                 wfs_b_bis[i, :-two_units_shift] = wfs_b[i, two_units_shift:, first_chan:first_chan+n_channels]\n",
    "#             wfs_a_bis += read_waveforms(spike_times_unit_a, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#             wfs_b_bis += read_waveforms(spike_times_unit_b+two_units_shift, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#     elif two_units_shift<0:\n",
    "#         if unit_shifted == unit_a:\n",
    "#             for i in range(wfs_a_bis.shape[0]):\n",
    "#                 first_chan = int(mc - first_chan_a[i] - 5)\n",
    "#                 first_chan = min(384-n_channels, first_chan)\n",
    "#                 first_chan = max(n_channels, first_chan)\n",
    "#                 wfs_a_bis[i, -two_units_shift:] = wfs_a[i, :two_units_shift, first_chan:first_chan+n_channels]\n",
    "#                 first_chan = int(mc - first_chan_b[i] - 5)\n",
    "#                 first_chan = min(384-n_channels, first_chan)\n",
    "#                 first_chan = max(n_channels, first_chan)\n",
    "#                 wfs_b_bis[i, :] = wfs_b[i, :, first_chan:first_chan+n_channels]\n",
    "#             wfs_a_bis += read_waveforms(spike_times_unit_a, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#             wfs_b_bis += read_waveforms(spike_times_unit_b-two_units_shift, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "\n",
    "#         else:\n",
    "#             for i in range(wfs_a_bis.shape[0]):\n",
    "#                 first_chan = int(mc - first_chan_a[i] - 5)\n",
    "#                 first_chan = min(384-n_channels, first_chan)\n",
    "#                 first_chan = max(n_channels, first_chan)\n",
    "#                 wfs_a_bis[i] = wfs_a[i, :, first_chan:first_chan+n_channels]\n",
    "#                 first_chan = int(mc - first_chan_b[i] - 5)\n",
    "#                 first_chan = min(384-n_channels, first_chan)\n",
    "#                 first_chan = max(n_channels, first_chan)\n",
    "#                 wfs_b_bis[i, -two_units_shift:] = wfs_b[i, :two_units_shift, first_chan:first_chan+n_channels]\n",
    "#             wfs_a_bis += read_waveforms(spike_times_unit_a-two_units_shift, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#             wfs_b_bis += read_waveforms(spike_times_unit_b, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#     else:\n",
    "#         for i in range(wfs_a_bis.shape[0]):\n",
    "#             first_chan = int(mc - first_chan_a[i] - 5)\n",
    "#             wfs_a_bis[i] = wfs_a[i, :, first_chan:first_chan+n_channels]\n",
    "#             first_chan = int(mc - first_chan_b[i] - 5)\n",
    "#             wfs_b_bis[i, :] = wfs_b[i, :, first_chan:first_chan+n_channels]\n",
    "#         wfs_a_bis += read_waveforms(spike_times_unit_a, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "#         wfs_b_bis += read_waveforms(spike_times_unit_b, residual_path, geom_array, n_times=n_times, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "\n",
    "#     spatial_whitener = get_spatial_whitener(spatial_cov, np.arange(mc-n_channels_half, mc+n_channels_half), geom_array)\n",
    "\n",
    "#     wfs1_w = np.matmul(wfs_a_bis, spatial_whitener)\n",
    "#     wfs2_w = np.matmul(wfs_b_bis, spatial_whitener)\n",
    "#     wfs1_w = np.matmul(wfs1_w.transpose(0,2,1),\n",
    "#                       temporal_whitener).transpose(0,2,1)\n",
    "#     wfs2_w = np.matmul(wfs2_w.transpose(0,2,1),\n",
    "#                       temporal_whitener).transpose(0,2,1)\n",
    "\n",
    "#     temp_diff_w = np.mean(wfs1_w, 0) - np.mean(wfs2_w,0)\n",
    "#     c_w = np.sum(0.5*(np.mean(wfs1_w, 0) + np.mean(wfs2_w,0))*temp_diff_w)\n",
    "#     dat1_w = np.sum(wfs1_w*temp_diff_w, (1,2))\n",
    "#     dat2_w = np.sum(wfs2_w*temp_diff_w, (1,2))\n",
    "#     dat_all = np.hstack((dat1_w, dat2_w))\n",
    "\n",
    "#     labels_diptest = np.zeros(wfs_a.shape[0]+wfs_b.shape[0])\n",
    "#     labels_diptest[:wfs_a.shape[0]] = 1\n",
    "    \n",
    "#     lda_model = LDA(n_components = 1)\n",
    "#     lda_comps = lda_model.fit_transform(dat_all.reshape(-1, 1), labels_diptest)\n",
    "#     value_dpt, cut_calue = isocut(lda_comps[:, 0])\n",
    "#     return value_dpt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_merged_yass(standardized_path, residual_path, waveforms, spatial_cov, temporal_whitener, geom_array, n_templates, spike_index, labels, x, z, denoiser, device, n_channels=10, n_temp = 10, distance_threshold = 3., threshold_diptest = 0.75):\n",
    "     \n",
    "#     templates = get_templates(standardized_path, geom_array, n_templates, spike_index, labels)\n",
    "#     n_spikes_templates = get_n_spikes_templates(n_templates, labels)\n",
    "#     x_z_templates = get_x_z_templates(n_templates, labels, x, z)\n",
    "#     print(\"GET PROPOSED PAIRS\")\n",
    "#     dist_argsort, dist_template = get_proposed_pairs(n_templates, templates, x_z_templates, n_temp = n_temp)\n",
    "    \n",
    "#     labels_updated = labels.copy()\n",
    "#     reference_units = np.unique(labels)[1:]\n",
    "    \n",
    "#     for unit in tqdm(range(n_templates)): #tqdm\n",
    "#         unit_reference = reference_units[unit]\n",
    "#         to_be_merged = [unit_reference]\n",
    "#         merge_shifts = [0]\n",
    "#         is_merged = False\n",
    "\n",
    "#         for j in range(n_temp):\n",
    "#             if dist_template[unit, j] < distance_threshold:\n",
    "#                 unit_bis = dist_argsort[unit, j]\n",
    "#                 unit_bis_reference = reference_units[unit_bis]\n",
    "#                 if unit_reference != unit_bis_reference:\n",
    "# #                     ALIGN BASED ON MAX PTP TEMPLATE MC \n",
    "#                     if templates[unit_reference].ptp(0).max() < templates[unit_bis_reference].ptp(0).max():\n",
    "#                         mc = templates[unit_bis_reference].ptp(0).argmax()\n",
    "#                         two_units_shift = templates[unit_reference, :, mc].argmin() - templates[unit_bis_reference, :, mc].argmin()\n",
    "#                         unit_shifted = unit_reference\n",
    "#                     else:\n",
    "#                         mc = templates[unit_reference].ptp(0).argmax()\n",
    "#                         two_units_shift = templates[unit_bis_reference, :, mc].argmin() - templates[unit_reference, :, mc].argmin()\n",
    "#                         unit_shifted = unit_bis_reference\n",
    "#                     if np.abs(two_units_shift)<4:\n",
    "#                         dpt_val = get_diptest_value_yass(residual_path, waveforms, spatial_cov, temporal_whitener, geom_array, spike_index, labels_updated, unit_reference, unit_bis_reference, n_spikes_templates, mc, two_units_shift,unit_shifted, denoiser, device, n_channels)\n",
    "#                         if dpt_val<threshold_diptest:\n",
    "#                             to_be_merged.append(unit_bis_reference)\n",
    "#                             if unit_shifted == unit_bis_reference:\n",
    "#                                 merge_shifts.append(-two_units_shift)\n",
    "#                             else:\n",
    "#                                 merge_shifts.append(two_units_shift)\n",
    "#                             is_merged = True\n",
    "#         if is_merged:\n",
    "#             n_total_spikes = 0\n",
    "#             for unit_merged in np.unique(np.asarray(to_be_merged)):\n",
    "#                 n_total_spikes += n_spikes_templates[unit_merged]\n",
    "\n",
    "#             new_reference_unit = np.unique(np.asarray(to_be_merged))[0]\n",
    "\n",
    "#             templates[new_reference_unit] = n_spikes_templates[new_reference_unit]*templates[new_reference_unit]/n_total_spikes\n",
    "#             cmp = 1\n",
    "#             for unit_merged in np.unique(np.asarray(to_be_merged))[1:]:\n",
    "#                 shift_ = merge_shifts[cmp]\n",
    "#                 templates[new_reference_unit] += n_spikes_templates[unit_merged]*np.roll(templates[unit_merged], shift_, axis = 0)/n_total_spikes\n",
    "#                 n_spikes_templates[new_reference_unit] += n_spikes_templates[unit_merged]\n",
    "#                 n_spikes_templates[unit_merged] = 0\n",
    "#                 labels_updated[labels_updated == unit_merged] = new_reference_unit\n",
    "#                 reference_units[unit_merged] = new_reference_unit\n",
    "#                 cmp += 1\n",
    "#     return labels_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e6dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# import random\n",
    "\n",
    "\n",
    "# def read_data(bin_file, dtype, s_start, s_end, n_channels):\n",
    "#     \"\"\"Read a chunk of a binary file\"\"\"\n",
    "#     offset = s_start * np.dtype(dtype).itemsize * n_channels\n",
    "#     with open(bin_file, \"rb\") as fin:\n",
    "#         data = np.fromfile(\n",
    "#             fin,\n",
    "#             dtype=dtype,\n",
    "#             count=(s_end - s_start) * n_channels,\n",
    "#             offset=offset,\n",
    "#         )\n",
    "#     data = data.reshape(-1, n_channels)\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def kill_signal(recordings, threshold, window_size):\n",
    "#     \"\"\"\n",
    "#     Thresholds recordings, values above 'threshold' are considered signal\n",
    "#     (set to 0), a window of size 'window_size' is drawn around the signal\n",
    "#     points and those observations are also killed\n",
    "#     Returns\n",
    "#     -------\n",
    "#     recordings: numpy.ndarray\n",
    "#         The modified recordings with values above the threshold set to 0\n",
    "#     is_noise_idx: numpy.ndarray\n",
    "#         A boolean array with the same shap as 'recordings' indicating if the\n",
    "#         observation is noise (1) or was killed (0).\n",
    "#     \"\"\"\n",
    "#     recordings = np.copy(recordings)\n",
    "\n",
    "#     T, C = recordings.shape\n",
    "#     R = int((window_size-1)/2)\n",
    "\n",
    "#     # this will hold a flag 1 (noise), 0 (signal) for every obseration in the\n",
    "#     # recordings\n",
    "#     is_noise_idx = np.zeros((T, C))\n",
    "\n",
    "#     # go through every neighboring channel\n",
    "#     for c in range(C):\n",
    "\n",
    "#         # get obserations where observation is above threshold\n",
    "#         idx_temp = np.where(np.abs(recordings[:, c]) > threshold)[0]\n",
    "\n",
    "#         if len(idx_temp) == 0:\n",
    "#             is_noise_idx[:, c] = 1\n",
    "#             continue\n",
    "\n",
    "#         # shift every index found\n",
    "#         for j in range(-R, R+1):\n",
    "\n",
    "#             # shift\n",
    "#             idx_temp2 = idx_temp + j\n",
    "\n",
    "#             # remove indexes outside range [0, T]\n",
    "#             idx_temp2 = idx_temp2[np.logical_and(idx_temp2 >= 0,\n",
    "#                                                  idx_temp2 < T)]\n",
    "\n",
    "#             # set surviving indexes to nan\n",
    "#             recordings[idx_temp2, c] = np.nan\n",
    "\n",
    "#         # noise indexes are the ones that are not nan\n",
    "#         # FIXME: compare to np.nan instead\n",
    "#         is_noise_idx_temp = (recordings[:, c] == recordings[:, c])\n",
    "\n",
    "#         # standarize data, ignoring nans\n",
    "#         recordings[:, c] = recordings[:, c]/np.nanstd(recordings[:, c])\n",
    "\n",
    "#         # set non noise indexes to 0 in the recordings\n",
    "#         recordings[~is_noise_idx_temp, c] = 0\n",
    "\n",
    "#         # save noise indexes\n",
    "#         is_noise_idx[is_noise_idx_temp, c] = 1\n",
    "\n",
    "#     return recordings, is_noise_idx\n",
    "\n",
    "# def search_noise_snippets(recordings, is_noise_idx, sample_size,\n",
    "#                           temporal_size, channel_choices=None,\n",
    "#                           max_trials_per_sample=100,\n",
    "#                           allow_smaller_sample_size=False):\n",
    "#     \"\"\"\n",
    "#     Randomly search noise snippets of 'temporal_size'\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     channel_choices: list\n",
    "#         List of sets of channels to select at random on each trial\n",
    "#     max_trials_per_sample: int, optional\n",
    "#         Maximum random trials per sample\n",
    "#     allow_smaller_sample_size: bool, optional\n",
    "#         If 'max_trials_per_sample' is reached and this is True, the noise\n",
    "#         snippets found up to that time are returned\n",
    "#     Raises\n",
    "#     ------\n",
    "#     ValueError\n",
    "#         if after 'max_trials_per_sample' trials, no noise snippet has been\n",
    "#         found this exception is raised\n",
    "#     Notes\n",
    "#     -----\n",
    "#     Channels selected at random using the random module from the standard\n",
    "#     library (not using np.random)\n",
    "#     \"\"\"\n",
    "\n",
    "#     T, C = recordings.shape\n",
    "\n",
    "#     if channel_choices is None:\n",
    "#         noise_wf = np.zeros((sample_size, temporal_size))\n",
    "#     else:\n",
    "#         lenghts = set([len(ch) for ch in channel_choices])\n",
    "\n",
    "#         if len(lenghts) > 1:\n",
    "#             raise ValueError('All elements in channel_choices must have '\n",
    "#                              'the same length, got {}'.format(lenghts))\n",
    "\n",
    "#         n_channels = len(channel_choices[0])\n",
    "#         noise_wf = np.zeros((sample_size, temporal_size, n_channels))\n",
    "\n",
    "#     count = 0\n",
    "\n",
    "\n",
    "#     trial = 0\n",
    "\n",
    "#     # repeat until you get sample_size noise snippets\n",
    "#     while count < sample_size:\n",
    "\n",
    "#         # random number for the start of the noise snippet\n",
    "#         t_start = np.random.randint(T-temporal_size)\n",
    "\n",
    "#         if channel_choices is None:\n",
    "#             # random channel\n",
    "#             ch = random.randint(0, C - 1)\n",
    "#         else:\n",
    "#             ch = random.choice(channel_choices)\n",
    "\n",
    "#         t_slice = slice(t_start, t_start+temporal_size)\n",
    "\n",
    "#         # get a snippet from the recordings and the noise flags for the same\n",
    "#         # location\n",
    "#         snippet = recordings[t_slice, ch]\n",
    "#         snipped_idx_noise = is_noise_idx[t_slice, ch]\n",
    "\n",
    "#         # check if all observations in snippet are noise\n",
    "#         if snipped_idx_noise.all():\n",
    "#             # add the snippet and increase count\n",
    "#             noise_wf[count] = snippet\n",
    "#             count += 1\n",
    "#             trial = 0\n",
    "\n",
    "#         trial += 1\n",
    "\n",
    "#         if trial == max_trials_per_sample:\n",
    "#             if allow_smaller_sample_size:\n",
    "#                 return noise_wf[:count]\n",
    "#             else:\n",
    "#                 raise ValueError(\"Couldn't find snippet {} of size {} after \"\n",
    "#                                  \"{} iterations (only {} found)\"\n",
    "#                                  .format(count + 1, temporal_size,\n",
    "#                                          max_trials_per_sample,\n",
    "#                                          count))\n",
    "\n",
    "#     return noise_wf\n",
    "\n",
    "\n",
    "# def get_noise_covariance(raw_data, geom_array, dtype = 'float32', n_channels = 384, rec_len = 60, sampling_rate = 30000, spike_size = 121):\n",
    "#     rec_len = rec_len*sampling_rate\n",
    "#     # get data chunk\n",
    "#     chunk_5sec = 5*sampling_rate\n",
    "#     if rec_len < chunk_5sec:\n",
    "#         chunk_5sec = rec_len\n",
    "#     small_batch = read_data(raw_data, dtype,\n",
    "#                 rec_len//2 - chunk_5sec//2,\n",
    "#                 rec_len//2 + chunk_5sec//2, n_channels = n_channels)\n",
    "    \n",
    "\n",
    "#     # get noise floor of recording\n",
    "#     noised_killed, is_noise_idx = kill_signal(small_batch, 3, spike_size)\n",
    "#     print (\"small_batch: \", small_batch.shape, \", noised_killed: \", noised_killed.shape)\n",
    "#     # spatial covariance\n",
    "#     spatial_cov_all = np.divide(np.matmul(noised_killed.T, noised_killed),\n",
    "#                         np.matmul(is_noise_idx.T, is_noise_idx))\n",
    "#     sig = np.sqrt(np.diag(spatial_cov_all))\n",
    "#     sig[sig == 0] = 1\n",
    "#     spatial_cov_all = spatial_cov_all/(sig[:,None]*sig[None])\n",
    "# #     chan_dist = squareform(pdist(geom_array))\n",
    "# #     chan_dist_unique = np.unique(chan_dist)\n",
    "# #     cov_by_dist = np.zeros(len(chan_dist_unique))\n",
    "# #     for ii, d in enumerate(chan_dist_unique):\n",
    "# #         cov_by_dist[ii] = np.mean(spatial_cov_all[chan_dist == d])\n",
    "# #     dist_in = cov_by_dist > 0.1\n",
    "# #     chan_dist_unique = chan_dist_unique[dist_in]\n",
    "# #     cov_by_dist = cov_by_dist[dist_in]\n",
    "# #     spatial_cov = np.vstack((cov_by_dist, chan_dist_unique)).T\n",
    "\n",
    "#     # get noise snippets\n",
    "#     noise_wf = search_noise_snippets(\n",
    "#                     noised_killed, is_noise_idx, 1000,\n",
    "#                     spike_size,\n",
    "#                     channel_choices=None,\n",
    "#                     max_trials_per_sample=100,\n",
    "#                     allow_smaller_sample_size=True)\n",
    "\n",
    "#     # get temporal covariance\n",
    "#     temp_cov = np.cov(noise_wf.T)\n",
    "#     sig = np.sqrt(np.diag(temp_cov))\n",
    "#     temp_cov = temp_cov/(sig[:,None]*sig[None])\n",
    "\n",
    "\n",
    "#     return spatial_cov_all, temp_cov\n",
    "\n",
    "# def get_spatial_whitener(spatial_cov, vis_chan, geom_array):\n",
    "\n",
    "#     chan_dist = squareform(pdist(geom_array[vis_chan]))\n",
    "#     spat_cov = np.zeros((len(vis_chan), len(vis_chan)))\n",
    "#     for ii, c in enumerate(spatial_cov[:,1]):\n",
    "#         spat_cov[chan_dist == c] = spatial_cov[ii, 0]\n",
    "\n",
    "#     w, v = np.linalg.eig(spat_cov)\n",
    "#     w[w<=0] = 1E-10\n",
    "#     inv_half_spat_cov = np.matmul(np.matmul(v, np.diag(1/np.sqrt(w))), v.T)\n",
    "\n",
    "#     return inv_half_spat_cov\n",
    "\n",
    "# def get_temporal_whitener(temporal_cov):\n",
    "#     w, v = np.linalg.eig(temporal_cov)\n",
    "#     return np.matmul(np.matmul(v, np.diag(1/np.sqrt(w))), v.T)\n",
    "        \n",
    "# spatial_cov, temporal_cov = get_noise_covariance(residual_data_bin, geom_array)\n",
    "# temporal_whitener = get_temporal_whitener(temporal_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfd9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_merged_yass = get_merged_yass(raw_data_bin, residual_data_bin, triaged_sub_wfs, spatial_cov, temporal_whitener, geom_array, \n",
    "#                                      ordered_split_labels.max()+1, spike_index_triaged, ordered_split_labels, triaged_x, triaged_z, denoiser, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709aad24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "templates_split = get_templates(raw_data_bin, geom_array, ordered_split_labels.max()+1, triaged_spike_index, ordered_split_labels)\n",
    "n_spikes_templates = get_n_spikes_templates(templates_split.shape[0], ordered_split_labels)\n",
    "x_z_templates = get_x_z_templates(templates_split.shape[0], ordered_split_labels, triaged_x, triaged_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merged = get_merged_better(residual_data_bin, triaged_sub_wfs, triaged_first_chan, geom_array, \n",
    "                                  templates_split, n_spikes_templates, x_z_templates, templates_split.shape[0], spike_index_triaged, \n",
    "                                  ordered_split_labels, triaged_x, triaged_z, denoiser, device, distance_threshold = 1., \n",
    "                                  threshold_diptest = .5, rank_pca=8, nn_denoise = True)\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673abb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d4f3c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "labels_corrected_merged = np.zeros(labels_merged.shape)\n",
    "cmp = -1\n",
    "for i in np.unique(labels_merged):\n",
    "    labels_corrected_merged[labels_merged == i] = cmp\n",
    "    cmp += 1\n",
    "labels_corrected_merged = labels_corrected_merged.astype('int')\n",
    "\n",
    "z_labels = np.zeros(labels_corrected_merged.max()+1)\n",
    "for i in range(labels_corrected_merged.max()+1):\n",
    "    z_labels[i] = triaged_z[labels_corrected_merged == i].mean()\n",
    "    \n",
    "ordered_merged_labels = labels_corrected_merged.copy()\n",
    "z_argsort = z_labels.argsort()[::-1]\n",
    "for i in range(labels_corrected_merged.max()+1):\n",
    "    ordered_merged_labels[labels_corrected_merged == z_argsort[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63077197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_merged_labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f077ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir = cm.get_cmap('viridis')\n",
    "triaged_log_ptp = triaged_maxptps.copy()\n",
    "triaged_log_ptp[triaged_log_ptp >= 27.5] = 27.5\n",
    "triaged_log_ptp = np.log(triaged_log_ptp+1)\n",
    "triaged_log_ptp[triaged_log_ptp<=1.25] = 1.25\n",
    "triaged_ptp_rescaled = (triaged_log_ptp - triaged_log_ptp.min())/(triaged_log_ptp.max() - triaged_log_ptp.min())\n",
    "color_arr = vir(triaged_ptp_rescaled)\n",
    "color_arr[:, 3] = triaged_ptp_rescaled\n",
    "\n",
    "# ## Define colors\n",
    "unique_colors = ['#e6194b', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#008080', '#e6beff', '#9a6324', '#800000', '#aaffc3', '#808000', '#000075', '#000000']\n",
    "\n",
    "cluster_color_dict = {}\n",
    "for cluster_id in np.unique(ordered_merged_labels):\n",
    "    cluster_color_dict[cluster_id] = unique_colors[cluster_id % len(unique_colors)]\n",
    "cluster_color_dict[-1] = '#808080' #set outlier color to grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_array_scatter(ordered_merged_labels, geom_array, triaged_x, triaged_z, triaged_maxptps, cluster_color_dict, color_arr, min_cluster_size=clusterer.min_cluster_size, min_samples=clusterer.min_samples, \n",
    "                         z_cutoff=(0, 3900), figsize=(18, 24))\n",
    "# fig.suptitle(f'x,z,scaled_logptp features,\" {num_spikes} datapoints');\n",
    "# fig.savefig('yass_merge.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10daf6f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# import random\n",
    "\n",
    "\n",
    "# def read_data(bin_file, dtype, s_start, s_end, n_channels):\n",
    "#     \"\"\"Read a chunk of a binary file\"\"\"\n",
    "#     offset = s_start * np.dtype(dtype).itemsize * n_channels\n",
    "#     with open(bin_file, \"rb\") as fin:\n",
    "#         data = np.fromfile(\n",
    "#             fin,\n",
    "#             dtype=dtype,\n",
    "#             count=(s_end - s_start) * n_channels,\n",
    "#             offset=offset,\n",
    "#         )\n",
    "#     data = data.reshape(-1, n_channels)\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def kill_signal(recordings, threshold, window_size):\n",
    "#     \"\"\"\n",
    "#     Thresholds recordings, values above 'threshold' are considered signal\n",
    "#     (set to 0), a window of size 'window_size' is drawn around the signal\n",
    "#     points and those observations are also killed\n",
    "#     Returns\n",
    "#     -------\n",
    "#     recordings: numpy.ndarray\n",
    "#         The modified recordings with values above the threshold set to 0\n",
    "#     is_noise_idx: numpy.ndarray\n",
    "#         A boolean array with the same shap as 'recordings' indicating if the\n",
    "#         observation is noise (1) or was killed (0).\n",
    "#     \"\"\"\n",
    "#     recordings = np.copy(recordings)\n",
    "\n",
    "#     T, C = recordings.shape\n",
    "#     R = int((window_size-1)/2)\n",
    "\n",
    "#     # this will hold a flag 1 (noise), 0 (signal) for every obseration in the\n",
    "#     # recordings\n",
    "#     is_noise_idx = np.zeros((T, C))\n",
    "\n",
    "#     # go through every neighboring channel\n",
    "#     for c in range(C):\n",
    "\n",
    "#         # get obserations where observation is above threshold\n",
    "#         idx_temp = np.where(np.abs(recordings[:, c]) > threshold)[0]\n",
    "\n",
    "#         if len(idx_temp) == 0:\n",
    "#             is_noise_idx[:, c] = 1\n",
    "#             continue\n",
    "\n",
    "#         # shift every index found\n",
    "#         for j in range(-R, R+1):\n",
    "\n",
    "#             # shift\n",
    "#             idx_temp2 = idx_temp + j\n",
    "\n",
    "#             # remove indexes outside range [0, T]\n",
    "#             idx_temp2 = idx_temp2[np.logical_and(idx_temp2 >= 0,\n",
    "#                                                  idx_temp2 < T)]\n",
    "\n",
    "#             # set surviving indexes to nan\n",
    "#             recordings[idx_temp2, c] = np.nan\n",
    "\n",
    "#         # noise indexes are the ones that are not nan\n",
    "#         # FIXME: compare to np.nan instead\n",
    "#         is_noise_idx_temp = (recordings[:, c] == recordings[:, c])\n",
    "\n",
    "#         # standarize data, ignoring nans\n",
    "#         recordings[:, c] = recordings[:, c]/np.nanstd(recordings[:, c])\n",
    "\n",
    "#         # set non noise indexes to 0 in the recordings\n",
    "#         recordings[~is_noise_idx_temp, c] = 0\n",
    "\n",
    "#         # save noise indexes\n",
    "#         is_noise_idx[is_noise_idx_temp, c] = 1\n",
    "\n",
    "#     return recordings, is_noise_idx\n",
    "\n",
    "# def search_noise_snippets(recordings, is_noise_idx, sample_size,\n",
    "#                           temporal_size, channel_choices=None,\n",
    "#                           max_trials_per_sample=100,\n",
    "#                           allow_smaller_sample_size=False):\n",
    "#     \"\"\"\n",
    "#     Randomly search noise snippets of 'temporal_size'\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     channel_choices: list\n",
    "#         List of sets of channels to select at random on each trial\n",
    "#     max_trials_per_sample: int, optional\n",
    "#         Maximum random trials per sample\n",
    "#     allow_smaller_sample_size: bool, optional\n",
    "#         If 'max_trials_per_sample' is reached and this is True, the noise\n",
    "#         snippets found up to that time are returned\n",
    "#     Raises\n",
    "#     ------\n",
    "#     ValueError\n",
    "#         if after 'max_trials_per_sample' trials, no noise snippet has been\n",
    "#         found this exception is raised\n",
    "#     Notes\n",
    "#     -----\n",
    "#     Channels selected at random using the random module from the standard\n",
    "#     library (not using np.random)\n",
    "#     \"\"\"\n",
    "\n",
    "#     T, C = recordings.shape\n",
    "\n",
    "#     if channel_choices is None:\n",
    "#         noise_wf = np.zeros((sample_size, temporal_size))\n",
    "#     else:\n",
    "#         lenghts = set([len(ch) for ch in channel_choices])\n",
    "\n",
    "#         if len(lenghts) > 1:\n",
    "#             raise ValueError('All elements in channel_choices must have '\n",
    "#                              'the same length, got {}'.format(lenghts))\n",
    "\n",
    "#         n_channels = len(channel_choices[0])\n",
    "#         noise_wf = np.zeros((sample_size, temporal_size, n_channels))\n",
    "\n",
    "#     count = 0\n",
    "\n",
    "\n",
    "#     trial = 0\n",
    "\n",
    "#     # repeat until you get sample_size noise snippets\n",
    "#     while count < sample_size:\n",
    "\n",
    "#         # random number for the start of the noise snippet\n",
    "#         t_start = np.random.randint(T-temporal_size)\n",
    "\n",
    "#         if channel_choices is None:\n",
    "#             # random channel\n",
    "#             ch = random.randint(0, C - 1)\n",
    "#         else:\n",
    "#             ch = random.choice(channel_choices)\n",
    "\n",
    "#         t_slice = slice(t_start, t_start+temporal_size)\n",
    "\n",
    "#         # get a snippet from the recordings and the noise flags for the same\n",
    "#         # location\n",
    "#         snippet = recordings[t_slice, ch]\n",
    "#         snipped_idx_noise = is_noise_idx[t_slice, ch]\n",
    "\n",
    "#         # check if all observations in snippet are noise\n",
    "#         if snipped_idx_noise.all():\n",
    "#             # add the snippet and increase count\n",
    "#             noise_wf[count] = snippet\n",
    "#             count += 1\n",
    "#             trial = 0\n",
    "\n",
    "#         trial += 1\n",
    "\n",
    "#         if trial == max_trials_per_sample:\n",
    "#             if allow_smaller_sample_size:\n",
    "#                 return noise_wf[:count]\n",
    "#             else:\n",
    "#                 raise ValueError(\"Couldn't find snippet {} of size {} after \"\n",
    "#                                  \"{} iterations (only {} found)\"\n",
    "#                                  .format(count + 1, temporal_size,\n",
    "#                                          max_trials_per_sample,\n",
    "#                                          count))\n",
    "\n",
    "#     return noise_wf\n",
    "\n",
    "\n",
    "# def get_noise_covariance(raw_data, geom_array, dtype = 'float32', n_channels = 384, rec_len = 60, sampling_rate = 30000, spike_size = 121):\n",
    "#     rec_len = rec_len*sampling_rate\n",
    "#     # get data chunk\n",
    "#     chunk_5sec = 5*sampling_rate\n",
    "#     if rec_len < chunk_5sec:\n",
    "#         chunk_5sec = rec_len\n",
    "#     small_batch = read_data(raw_data, dtype,\n",
    "#                 rec_len//2 - chunk_5sec//2,\n",
    "#                 rec_len//2 + chunk_5sec//2, n_channels = n_channels)\n",
    "    \n",
    "\n",
    "#     # get noise floor of recording\n",
    "#     noised_killed, is_noise_idx = kill_signal(small_batch, 3, spike_size)\n",
    "#     print (\"small_batch: \", small_batch.shape, \", noised_killed: \", noised_killed.shape)\n",
    "#     # spatial covariance\n",
    "#     spatial_cov_all = np.divide(np.matmul(noised_killed.T, noised_killed),\n",
    "#                         np.matmul(is_noise_idx.T, is_noise_idx))\n",
    "#     sig = np.sqrt(np.diag(spatial_cov_all))\n",
    "#     sig[sig == 0] = 1\n",
    "#     spatial_cov_all = spatial_cov_all/(sig[:,None]*sig[None])\n",
    "# #     chan_dist = squareform(pdist(geom_array))\n",
    "# #     chan_dist_unique = np.unique(chan_dist)\n",
    "# #     cov_by_dist = np.zeros(len(chan_dist_unique))\n",
    "# #     for ii, d in enumerate(chan_dist_unique):\n",
    "# #         cov_by_dist[ii] = np.mean(spatial_cov_all[chan_dist == d])\n",
    "# #     dist_in = cov_by_dist > 0.1\n",
    "# #     chan_dist_unique = chan_dist_unique[dist_in]\n",
    "# #     cov_by_dist = cov_by_dist[dist_in]\n",
    "# #     spatial_cov = np.vstack((cov_by_dist, chan_dist_unique)).T\n",
    "\n",
    "#     # get noise snippets\n",
    "#     noise_wf = search_noise_snippets(\n",
    "#                     noised_killed, is_noise_idx, 1000,\n",
    "#                     spike_size,\n",
    "#                     channel_choices=None,\n",
    "#                     max_trials_per_sample=100,\n",
    "#                     allow_smaller_sample_size=True)\n",
    "\n",
    "#     # get temporal covariance\n",
    "#     temp_cov = np.cov(noise_wf.T)\n",
    "#     sig = np.sqrt(np.diag(temp_cov))\n",
    "#     temp_cov = temp_cov/(sig[:,None]*sig[None])\n",
    "\n",
    "\n",
    "#     return spatial_cov_all, temp_cov\n",
    "\n",
    "# def get_spatial_whitener(spatial_cov, vis_chan, geom_array):\n",
    "\n",
    "#     chan_dist = squareform(pdist(geom_array[vis_chan]))\n",
    "#     spat_cov = np.zeros((len(vis_chan), len(vis_chan)))\n",
    "#     for ii, c in enumerate(spatial_cov[:,1]):\n",
    "#         spat_cov[chan_dist == c] = spatial_cov[ii, 0]\n",
    "\n",
    "#     w, v = np.linalg.eig(spat_cov)\n",
    "#     w[w<=0] = 1E-10\n",
    "#     inv_half_spat_cov = np.matmul(np.matmul(v, np.diag(1/np.sqrt(w))), v.T)\n",
    "\n",
    "#     return inv_half_spat_cov\n",
    "\n",
    "# def get_temporal_whitener(temporal_cov):\n",
    "#     w, v = np.linalg.eig(temporal_cov)\n",
    "#     return np.matmul(np.matmul(v, np.diag(1/np.sqrt(w))), v.T)\n",
    "        \n",
    "# spatial_cov, temporal_cov = get_noise_covariance(residual_data_bin, geom_array)\n",
    "# temporal_whitener = get_temporal_whitener(temporal_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a46173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_cov, temporal_cov = get_noise_covariance(residual_data_bin, geom_array)\n",
    "# temporal_whitener = get_temporal_whitener(temporal_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_merged = get_templates(raw_data_bin, geom_array, ordered_merged_labels.max()+1, triaged_spike_index, ordered_merged_labels)\n",
    "n_spikes_templates = get_n_spikes_templates(templates_merged.shape[0], ordered_merged_labels)\n",
    "x_z_templates = get_x_z_templates(templates_merged.shape[0], ordered_merged_labels, triaged_x, triaged_z)\n",
    "templates_merged[94, :, templates_merged[94].ptp(0).argmax()].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_a = 309\n",
    "unit_b = 310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ca8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = templates_merged[unit_a].ptp(0).argmax()\n",
    "templates_merged[unit_a, :, mc].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99da8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_merged[unit_a, :, mc].argmin()-templates_merged[unit_b, :, mc].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa78533",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 2.5))\n",
    "plt.plot(templates_merged[unit_a, :, mc-5:mc+5].T.flatten())\n",
    "plt.plot(templates_merged[unit_b, :, mc-5:mc+5].T.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bea1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 10\n",
    "n_channels_half = n_channels//2\n",
    "\n",
    "wfs_a = triaged_sub_wfs[ordered_merged_labels == unit_a]\n",
    "wfs_b = triaged_sub_wfs[ordered_merged_labels == unit_b]\n",
    "first_chan_a = triaged_first_chan[ordered_merged_labels == unit_a]\n",
    "first_chan_b = triaged_first_chan[ordered_merged_labels == unit_b]\n",
    "\n",
    "wfs_a_bis = np.zeros((wfs_a.shape[0], 121, n_channels))\n",
    "wfs_b_bis = np.zeros((wfs_b.shape[0], 121, n_channels))\n",
    "for i in range(wfs_a_bis.shape[0]):\n",
    "    first_chan = int(mc - first_chan_a[i] - 5)\n",
    "    wfs_a_bis[i] = wfs_a[i, :, first_chan:first_chan+n_channels]\n",
    "    first_chan = int(mc - first_chan_b[i] - 5)\n",
    "    wfs_b_bis[i] = wfs_b[i, :, first_chan:first_chan+n_channels]\n",
    "\n",
    "\n",
    "spike_index_unit_a = triaged_spike_index[ordered_merged_labels == unit_a, 0]#denoiser offset ## SHIFT BASED ON TEMPLATES ARGMIN PN MAX PTP TEMPLATE\n",
    "spike_index_unit_b = triaged_spike_index[ordered_merged_labels == unit_b, 0]-4\n",
    "\n",
    "n_wfs_max = int(min(250, min(n_spikes_templates[unit_a], n_spikes_templates[unit_b]))) \n",
    "idx = np.random.choice(np.arange(spike_index_unit_a.shape[0]), n_wfs_max, replace = False)\n",
    "spike_times_unit_a = spike_index_unit_a[idx]\n",
    "wfs_a = wfs_a[idx]\n",
    "first_chan_a = first_chan_a[idx]\n",
    "idx = np.random.choice(np.arange(spike_index_unit_b.shape[0]), n_wfs_max, replace = False)\n",
    "spike_times_unit_b = spike_index_unit_b[idx]\n",
    "wfs_b = wfs_b[idx]\n",
    "first_chan_b = first_chan_b[idx]\n",
    "\n",
    "mc = min(384-n_channels_half, mc)\n",
    "mc = max(n_channels_half, mc)\n",
    "\n",
    "wfs_a_bis = np.zeros((wfs_a.shape[0], 121, 10))\n",
    "wfs_b_bis = np.zeros((wfs_b.shape[0], 121, 10))\n",
    "for i in range(wfs_a_bis.shape[0]):\n",
    "    first_chan = int(mc - first_chan_a[i] - 5)\n",
    "    wfs_a_bis[i] = wfs_a[i, :, first_chan:first_chan+10]\n",
    "    first_chan = int(mc - first_chan_b[i] - 5)\n",
    "    wfs_b_bis[i] = wfs_b[i, :, first_chan:first_chan+10]\n",
    "\n",
    "wfs_a = wfs_a_bis + read_waveforms(spike_times_unit_a, residual_data_bin, geom_array, n_times=121, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "wfs_b = wfs_b_bis + read_waveforms(spike_times_unit_b, residual_data_bin, geom_array, n_times=121, channels = np.arange(mc-n_channels_half,mc+n_channels_half))[0]\n",
    "wfs_a_denoised = denoise_wf_nn_tmp_single_channel(wfs_a, denoiser, device)\n",
    "wfs_b_denoised = denoise_wf_nn_tmp_single_channel(wfs_b, denoiser, device)\n",
    "\n",
    "wfs_diptest = np.concatenate((wfs_a_denoised, wfs_b_denoised)).reshape((-1, n_channels*121))\n",
    "labels_diptest = np.zeros(wfs_a.shape[0]+wfs_b.shape[0])\n",
    "labels_diptest[:wfs_a.shape[0]] = 1\n",
    "\n",
    "\n",
    "lda_model = LDA(n_components = 1)\n",
    "lda_comps = lda_model.fit_transform(wfs_diptest, labels_diptest)\n",
    "value_dpt, cut_calue = isocut(lda_comps[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_dpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50065833",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lda_comps, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71dd09a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tpca = PCA(3)\n",
    "cleaned = np.concatenate((wfs_a, wfs_b))\n",
    "N, T, C = cleaned.shape\n",
    "cleaned = cleaned.transpose(0, 2, 1).reshape(N*C, T)\n",
    "cleaned = tpca.inverse_transform(tpca.fit_transform(cleaned))\n",
    "cleaned = cleaned.reshape(N, C, T) #.transpose(0, 2, 1)\n",
    "\n",
    "plt.figure(figsize = (20, 2.5))\n",
    "for i in range(cleaned.shape[0]//2):\n",
    "    plt.plot(cleaned[i].flatten(), c='red', alpha = 0.05)\n",
    "for i in range(cleaned.shape[0]//2):\n",
    "    plt.plot(cleaned[cleaned.shape[0]//2+i].flatten(), c='blue', alpha = 0.05)\n",
    "for j in range(9):\n",
    "    plt.axvline(121+j*121, c='black')\n",
    "plt.show()\n",
    "    \n",
    "wfs_diptest = cleaned.reshape((N, C*T))\n",
    "labels_diptest = np.zeros(wfs_a.shape[0]+wfs_b.shape[0])\n",
    "labels_diptest[:wfs_a.shape[0]] = 1\n",
    "\n",
    "\n",
    "lda_model = LDA(n_components = 1)\n",
    "lda_comps = lda_model.fit_transform(wfs_diptest, labels_diptest)\n",
    "value_dpt, cut_calue = isocut(lda_comps[:, 0])\n",
    "\n",
    "print(value_dpt)\n",
    "plt.hist(lda_comps, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcdbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpca = PCA(5)\n",
    "cleaned = np.concatenate((wfs_a, wfs_b))\n",
    "N, T, C = cleaned.shape\n",
    "cleaned = cleaned.transpose(0, 2, 1).reshape(N*C, T)\n",
    "cleaned = tpca.inverse_transform(tpca.fit_transform(cleaned))\n",
    "cleaned = cleaned.reshape(N, C, T) #.transpose(0, 2, 1)\n",
    "\n",
    "plt.figure(figsize = (20, 2.5))\n",
    "for i in range(cleaned.shape[0]//2):\n",
    "    plt.plot(cleaned[i].flatten(), c='red', alpha = 0.05)\n",
    "for i in range(cleaned.shape[0]//2):\n",
    "    plt.plot(cleaned[cleaned.shape[0]//2+i].flatten(), c='blue', alpha = 0.05)\n",
    "for j in range(9):\n",
    "    plt.axvline(121+j*121, c='black')\n",
    "plt.show()\n",
    "    \n",
    "wfs_diptest = cleaned.reshape((N, C*T))\n",
    "labels_diptest = np.zeros(wfs_a.shape[0]+wfs_b.shape[0])\n",
    "labels_diptest[:wfs_a.shape[0]] = 1\n",
    "\n",
    "\n",
    "lda_model = LDA(n_components = 1)\n",
    "lda_comps = lda_model.fit_transform(wfs_diptest, labels_diptest)\n",
    "value_dpt, cut_calue = isocut(lda_comps[:, 0])\n",
    "\n",
    "print(value_dpt)\n",
    "plt.hist(lda_comps, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpca = PCA(8)\n",
    "cleaned = np.concatenate((wfs_a, wfs_b))\n",
    "N, T, C = cleaned.shape\n",
    "cleaned = cleaned.transpose(0, 2, 1).reshape(N*C, T)\n",
    "cleaned = tpca.inverse_transform(tpca.fit_transform(cleaned))\n",
    "cleaned = cleaned.reshape(N, C, T) #.transpose(0, 2, 1)\n",
    "\n",
    "plt.figure(figsize = (20, 2.5))\n",
    "for i in range(cleaned.shape[0]//2):\n",
    "    plt.plot(cleaned[i].flatten(), c='red', alpha = 0.05)\n",
    "# for i in range(cleaned.shape[0]//2):\n",
    "#     plt.plot(cleaned[cleaned.shape[0]//2+i].flatten(), c='blue', alpha = 0.05)\n",
    "for j in range(9):\n",
    "    plt.axvline(121+j*121, c='black')\n",
    "plt.show()\n",
    "    \n",
    "wfs_diptest = cleaned.reshape((N, C*T))\n",
    "labels_diptest = np.zeros(wfs_a.shape[0]+wfs_b.shape[0])\n",
    "labels_diptest[:wfs_a.shape[0]] = 1\n",
    "\n",
    "\n",
    "lda_model = LDA(n_components = 1)\n",
    "lda_comps = lda_model.fit_transform(wfs_diptest, labels_diptest)\n",
    "value_dpt, cut_calue = isocut(lda_comps[:, 0])\n",
    "\n",
    "print(value_dpt)\n",
    "plt.hist(lda_comps, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33cda1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (20, 2.5))\n",
    "for i in range(wfs_a_denoised.shape[0]//2):\n",
    "    plt.plot(wfs_a_denoised[i].T.flatten(), c='red', alpha = 0.05)\n",
    "# for i in range(wfs_a_denoised.shape[0]//2):\n",
    "#     plt.plot(wfs_b_denoised[i].T.flatten(), c='blue', alpha = 0.05)\n",
    "for j in range(9):\n",
    "    plt.axvline(121+j*121, c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0a829",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (20, 2.5))\n",
    "for i in range(wfs_a_denoised.shape[0]//2):\n",
    "    plt.plot(wfs_a[i].T.flatten(), c='red', alpha = 0.05)\n",
    "for i in range(wfs_a_denoised.shape[0]//2):\n",
    "    plt.plot(wfs_b[i].T.flatten(), c='blue', alpha = 0.05)\n",
    "for j in range(9):\n",
    "    plt.axvline(121+j*121, c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee4e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 2.5))\n",
    "for i in range(wfs_a.shape[0]//2):\n",
    "    plt.plot(wfs_a[i, :, 3:6].T.flatten(), c='red', alpha = 0.5)\n",
    "for i in range(wfs_a.shape[0]//2):\n",
    "    plt.plot(wfs_b[i, :,  3:6].T.flatten(), c='blue', alpha = 0.5)\n",
    "# for j in range(9):\n",
    "#     plt.axvline(121+j*121, c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197758b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 2.5))\n",
    "for i in range(cleaned.shape[0]//2):\n",
    "    plt.plot(cleaned[i].flatten(), c='red', alpha = 0.05)\n",
    "for i in range(cleaned.shape[0]//2):\n",
    "    plt.plot(cleaned[cleaned.shape[0]//2+i].flatten(), c='blue', alpha = 0.05)\n",
    "for j in range(9):\n",
    "    plt.axvline(121+j*121, c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b99d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "proj_vector = (lda_model.means_[0] - lda_model.means_[1]).reshape((10, 121))\n",
    "plt.figure(figsize = (10, 2.5))\n",
    "for i in range(proj_vector.shape[0]):\n",
    "    plt.plot(proj_vector[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119be41",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# from scipy.interpolate import griddata, interp2d\n",
    "\n",
    "# wfs_a_registered = np.zeros(wfs_a_denoised.shape)\n",
    "# for i in range(wfs_a_denoised.shape[0]):\n",
    "#     wfs_a_registered[i] = griddata(geom_array[mc-20:mc+20], wfs_a_denoised[i].T, geom_array[mc-20:mc+20] + [0, displacement_estimate[int(geom_array[mc, 1]), spike_index_unit_a[i]//30000]], fill_value = 0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34d667",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# wfs_b_registered = np.zeros(wfs_b_denoised.shape)\n",
    "# for i in range(wfs_a_denoised.shape[0]):\n",
    "#     wfs_b_registered[i] = griddata(geom_array[mc-20:mc+20], wfs_b_denoised[i].T, geom_array[mc-20:mc+20] + [0, displacement_estimate[int(geom_array[mc, 1]), spike_index_unit_b[i]//30000]], fill_value = 0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127021fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166844c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_rescaled = displacement_estimate[int(geom_array[mc, 1])] - displacement_estimate[int(geom_array[mc, 1])].min()\n",
    "disp_rescaled = disp_rescaled/disp_rescaled.max()\n",
    "plt.figure(figsize = (20, 2.5))\n",
    "for i in range(wfs_a_denoised.shape[0]):\n",
    "    plt.plot(wfs_a[i, :80].T.flatten(), c=vir(disp_rescaled[spike_index_unit_a[i]//30000]), alpha = 0.1)\n",
    "for i in range(wfs_b_denoised.shape[0]):\n",
    "    plt.plot(wfs_b[i, :80].T.flatten(), c=vir(disp_rescaled[spike_index_unit_b[i]//30000]), alpha = 0.1)\n",
    "for j in range(9):\n",
    "    plt.axvline(80+j*80, c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_rescaled = displacement_estimate[int(geom_array[mc, 1])] - displacement_estimate[int(geom_array[mc, 1])].min()\n",
    "disp_rescaled = disp_rescaled/disp_rescaled.max()\n",
    "plt.figure(figsize = (20, 2.5))\n",
    "for i in range(wfs_a_denoised.shape[0]):\n",
    "    plt.plot(wfs_a_denoised[i, :].T.flatten(), c=vir(disp_rescaled[spike_index_unit_a[i]//30000]), alpha = 0.1)\n",
    "for i in range(wfs_b_denoised.shape[0]):\n",
    "    plt.plot(wfs_b_denoised[i, :].T.flatten(), c=vir(disp_rescaled[spike_index_unit_b[i]//30000]), alpha = 0.1)\n",
    "for j in range(9):\n",
    "    plt.axvline(121+j*121, c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef874cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a238f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e0f1ee8",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c43960",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "idx_sorted = triaged_spike_index[:, 0].argsort()\n",
    "spike_index_triaged = triaged_spike_index[idx_sorted]\n",
    "clusterer.labels_ = clusterer.labels_[idx_sorted]\n",
    "triaged_x = triaged_x[idx_sorted]\n",
    "triaged_z = triaged_z[idx_sorted]\n",
    "triaged_maxptps = triaged_maxptps[idx_sorted]\n",
    "triaged_sub_wfs = triaged_sub_wfs[idx_sorted]  \n",
    "triaged_first_chan = triaged_first_chan[idx_sorted]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc04a2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#create hdbscan/localization SpikeInterface sorting (with triage)\n",
    "sorting_hdbl_t = make_sorting_from_labels_frames(clusterer.labels_, spike_index_triaged[:,0])\n",
    "\n",
    "cmp_5 = compare_two_sorters(sorting_hdbl_t, sorting_kilo, sorting1_name='ours', sorting2_name='kilosort', match_score=.5)\n",
    "matched_units_5 = cmp_5.get_matching()[0].index.to_numpy()[np.where(cmp_5.get_matching()[0] != -1.)]\n",
    "matches_kilos_5 = cmp_5.get_best_unit_match1(matched_units_5).values.astype('int')\n",
    "\n",
    "cmp_1 = compare_two_sorters(sorting_hdbl_t, sorting_kilo, sorting1_name='ours', sorting2_name='kilosort', match_score=.1)\n",
    "matched_units_1 = cmp_1.get_matching()[0].index.to_numpy()[np.where(cmp_1.get_matching()[0] != -1.)]\n",
    "unmatched_units_1 = cmp_1.get_matching()[0].index.to_numpy()[np.where(cmp_1.get_matching()[0] == -1.)]\n",
    "matches_kilos_1 = cmp_1.get_best_unit_match1(matched_units_1).values.astype('int')\n",
    "\n",
    "cmp_kilo_5 = compare_two_sorters(sorting_kilo, sorting_hdbl_t, sorting1_name='kilosort', sorting2_name='ours', match_score=.5)\n",
    "matched_units_kilo_5 = cmp_kilo_5.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_5.get_matching()[0] != -1.)]\n",
    "unmatched_units_kilo_5 = cmp_kilo_5.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_5.get_matching()[0] == -1.)]\n",
    "\n",
    "cmp_kilo_1 = compare_two_sorters(sorting_kilo, sorting_hdbl_t, sorting1_name='kilosort', sorting2_name='ours', match_score=.1)\n",
    "matched_units_kilo_1 = cmp_kilo_1.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_1.get_matching()[0].to_numpy() != -1.)]\n",
    "unmatched_units_kilo_1 = cmp_kilo_1.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_1.get_matching()[0].to_numpy() == -1.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff34fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e0f6a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#remove duplicate units by spike_times_agreement and ptp\n",
    "# clusterer, duplicate_ids = remove_duplicate_units(clusterer, triaged_spike_index[:,0], triaged_maxptps)\n",
    "\n",
    "#re-compute cluster centers\n",
    "cluster_centers = compute_cluster_centers(clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c1468",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##### plot array scatter #####\n",
    "fig = plot_array_scatter(clusterer.labels_, geom_array, triaged_x, triaged_z, triaged_maxptps, cluster_color_dict, color_arr, min_cluster_size=clusterer.min_cluster_size, min_samples=clusterer.min_samples, \n",
    "                         z_cutoff=(1600, 2400), figsize=(18, 12))\n",
    "# fig.suptitle(f'x,z,scaled_logptp features,\" {num_spikes} datapoints');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfa915",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##### plot array scatter #####\n",
    "fig = plot_array_scatter(clusterer.labels_, geom_array, triaged_x, triaged_z, triaged_maxptps, cluster_color_dict, color_arr, min_cluster_size=clusterer.min_cluster_size, min_samples=clusterer.min_samples, \n",
    "                         z_cutoff=(800, 1600), figsize=(18, 12))\n",
    "# fig.suptitle(f'x,z,scaled_logptp features,\" {num_spikes} datapoints');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a4334",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##### plot array scatter #####\n",
    "fig = plot_array_scatter(clusterer.labels_, geom_array, triaged_x, triaged_z, triaged_maxptps, cluster_color_dict, color_arr, min_cluster_size=clusterer.min_cluster_size, min_samples=clusterer.min_samples, \n",
    "                         z_cutoff=(0, 800), figsize=(18, 12))\n",
    "# fig.suptitle(f'x,z,scaled_logptp features,\" {num_spikes} datapoints');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2e899-02ea-49db-b9bf-ed42866701c1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##### plot individual cluster summaries #####\n",
    "#load waveforms as memmap\n",
    "wfs_localized = np.load(data_dir+'denoised_wfs.npy', mmap_mode='r') #np.memmap(data_dir+'denoised_waveforms.npy', dtype='float32', shape=(290025, 121, 40))\n",
    "wfs_subtracted = np.load(data_dir+'subtracted_wfs.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d19a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d9673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c8087-dc60-462b-b8d7-270f64641195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create kilosort SpikeInterface sorting\n",
    "times_list = []\n",
    "labels_list = []\n",
    "for cluster_id in np.unique(kilo_spike_clusters):\n",
    "    spike_train_kilo = kilo_spike_frames[np.where(kilo_spike_clusters==cluster_id)]\n",
    "    times_list.append(spike_train_kilo)\n",
    "    labels_list.append(np.zeros(spike_train_kilo.shape[0])+cluster_id)\n",
    "times_array = np.concatenate(times_list).astype('int')\n",
    "labels_array = np.concatenate(labels_list).astype('int')\n",
    "sorting_kilo = spikeinterface.numpyextractors.NumpySorting.from_times_labels(times_list=times_array, \n",
    "                                                                             labels_list=labels_array, \n",
    "                                                                                  sampling_frequency=30000)\n",
    "#create hdbscan/localization SpikeInterface sorting (with triage)\n",
    "times_list = []\n",
    "labels_list = []\n",
    "for cluster_id in np.unique(clusterer.labels_):\n",
    "    spike_train_hdbl_t = spike_index_triaged[:,0][np.where(clusterer.labels_==cluster_id)]\n",
    "    times_list.append(spike_train_hdbl_t)\n",
    "    labels_list.append(np.zeros(spike_train_hdbl_t.shape[0])+cluster_id)\n",
    "times_array = np.concatenate(times_list).astype('int')\n",
    "labels_array = np.concatenate(labels_list).astype('int')\n",
    "sorting_hdbl_t = spikeinterface.numpyextractors.NumpySorting.from_times_labels(times_list=times_array, \n",
    "                                                                                labels_list=labels_array, \n",
    "                                                                                sampling_frequency=30000)\n",
    "\n",
    "cmp_5 = compare_two_sorters(sorting_hdbl_t, sorting_kilo, sorting1_name='ours', sorting2_name='kilosort', match_score=.5)\n",
    "matched_units_5 = cmp_5.get_matching()[0].index.to_numpy()[np.where(cmp_5.get_matching()[0] != -1.)]\n",
    "matches_kilos_5 = cmp_5.get_best_unit_match1(matched_units_5).values.astype('int')\n",
    "\n",
    "cmp_1 = compare_two_sorters(sorting_hdbl_t, sorting_kilo, sorting1_name='ours', sorting2_name='kilosort', match_score=.1)\n",
    "matched_units_1 = cmp_1.get_matching()[0].index.to_numpy()[np.where(cmp_1.get_matching()[0] != -1.)]\n",
    "unmatched_units_1 = cmp_1.get_matching()[0].index.to_numpy()[np.where(cmp_1.get_matching()[0] == -1.)]\n",
    "matches_kilos_1 = cmp_1.get_best_unit_match1(matched_units_1).values.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c088fc1-509d-454f-90bb-7a750559a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### plot individual cluster summaries #####\n",
    "#load waveforms as memmap\n",
    "wfs_localized = np.load(data_dir+'denoised_wfs.npy', mmap_mode='r') #np.memmap(data_dir+'denoised_waveforms.npy', dtype='float32', shape=(290025, 121, 40))\n",
    "wfs_subtracted = np.load(data_dir+'subtracted_wfs.npy', mmap_mode='r')\n",
    "non_triaged_idxs = ptp_filter[0][idx_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd5ad2-53bd-4d72-a11e-8d0415041035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_id = 16\n",
    "\n",
    "if cluster_id in matched_units_5:\n",
    "    cmp = cmp_5\n",
    "    print(\">50% match\")\n",
    "elif cluster_id in matched_units_1:\n",
    "    cmp = cmp_1\n",
    "    print(\"50%> and >10% match\")\n",
    "else:\n",
    "    cmp = None\n",
    "    print(\"<10% match\")\n",
    "    \n",
    "\n",
    "#plot cluster summary\n",
    "fig = plot_single_unit_summary(cluster_id, clusterer.labels_, cluster_centers, geom_array, 50, num_rows_plot, triaged_x, triaged_z, triaged_maxptps, \n",
    "                               triaged_firstchans, triaged_mcs_abs, triaged_spike_index, non_triaged_idxs, wfs_localized, wfs_subtracted, cluster_color_dict, \n",
    "                               color_arr, raw_data_bin, residual_data_bin)\n",
    "plt.show()\n",
    "\n",
    "# plot agreement with kilosort\n",
    "if cmp is not None:\n",
    "    num_channels = wfs_localized.shape[2]\n",
    "    cluster_id_match = cmp.get_best_unit_match1(cluster_id)\n",
    "    sorting1 = sorting_hdbl_t\n",
    "    sorting2 = sorting_kilo\n",
    "    sorting1_name = \"hdb\"\n",
    "    sorting2_name = \"kilo\"\n",
    "    firstchans_cluster_sorting1 = triaged_firstchans[clusterer.labels_ == cluster_id]\n",
    "    mcs_abs_cluster_sorting1 = triaged_mcs_abs[clusterer.labels_ == cluster_id]\n",
    "    spike_depths = kilo_spike_depths[np.where(kilo_spike_clusters==cluster_id_match)]\n",
    "    mcs_abs_cluster_sorting2 = np.asarray([np.argmin(np.abs(spike_depth - geom_array[:,1])) for spike_depth in spike_depths])\n",
    "    firstchans_cluster_sorting2 = (mcs_abs_cluster_sorting2 - 20).clip(min=0)\n",
    "    \n",
    "    plot_agreement_venn(cluster_id, cluster_id_match, cmp, sorting1, sorting2, sorting1_name, sorting2_name, geom_array, num_channels, num_spikes_plot, firstchans_cluster_sorting1, mcs_abs_cluster_sorting1, \n",
    "                        firstchans_cluster_sorting2, mcs_abs_cluster_sorting2, raw_data_bin, delta_frames = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2111bfa-77d1-41a3-8d5a-5939c855945d",
   "metadata": {},
   "source": [
    "# Oversplit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fa6af-a4e1-4087-8f0a-e8ab0261aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_kilo.get_unit_ids()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d76d1c-85c0-48db-8e31-2170fd62896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 10\n",
    "num_spikes_plot = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19064f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b0c78-391f-4f38-93c6-6adf86b8f66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###Kilosort\n",
    "%matplotlib inline\n",
    "save_dir_path = \"oversplit_cluster_summaries_kilosort\"\n",
    "if not os.path.exists(save_dir_path):\n",
    "    os.makedirs(save_dir_path)\n",
    "    \n",
    "num_close_clusters = 50\n",
    "num_close_clusters_plot=10\n",
    "num_channels_similarity = 20\n",
    "num_under_threshold = 0\n",
    "shifts_align=np.arange(-8,9)\n",
    "for cluster_id in sorting_kilo.get_unit_ids()[:1]:\n",
    "    st_1 = sorting_kilo.get_unit_spike_train(cluster_id)\n",
    "    \n",
    "    #compute K closest clsuters\n",
    "    curr_cluster_depth = kilo_cluster_depth_means[cluster_id]\n",
    "    dist_to_other_cluster_dict = {cluster_id:abs(mean_depth-curr_cluster_depth) for (cluster_id,mean_depth) in kilo_cluster_depth_means.items()}\n",
    "    closest_clusters = [y[0] for y in sorted(dist_to_other_cluster_dict.items(), key = lambda x: x[1])[1:1+num_close_clusters]]\n",
    "    \n",
    "    #compute unit similarties\n",
    "    original_template, closest_clusters, similarities, agreements, templates, shifts = get_unit_similarities(cluster_id, st_1, closest_clusters, sorting_kilo, geom_array, raw_data_bin, \n",
    "                                                                                                             num_channels_similarity=num_channels_similarity, \n",
    "                                                                                                             num_close_clusters=num_close_clusters, shifts_align=shifts_align,\n",
    "                                                                                                             order_by ='similarity')\n",
    "    if similarities[0] < 2.0: #arbitrary..\n",
    "        fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_kilo, sorting_kilo, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "                                     num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"both\")\n",
    "#         plt.close(fig)\n",
    "#         fig.savefig(save_dir_path + f\"/cluster_{cluster_id}_summary.png\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907e4f4-b296-47aa-8482-8dcea7144502",
   "metadata": {},
   "outputs": [],
   "source": [
    "###hdbscan\n",
    "%matplotlib inline\n",
    "save_dir_path = \"oversplit_cluster_summaries_hdbscan\"\n",
    "if not os.path.exists(save_dir_path):\n",
    "    os.makedirs(save_dir_path)\n",
    "    \n",
    "for cluster_id in sorting_hdbl_t.get_unit_ids()[:3]:\n",
    "    if cluster_id != -1:\n",
    "        #compute firing rate\n",
    "        st_1 = sorting_hdbl_t.get_unit_spike_train(cluster_id)\n",
    "        #compute K closest clsuters\n",
    "        curr_cluster_center = cluster_centers[cluster_id]\n",
    "        dist_other_clusters = np.linalg.norm(curr_cluster_center[:2] - cluster_centers[:,:2], axis=1)\n",
    "        closest_clusters = np.argsort(dist_other_clusters)[1:num_close_clusters + 1]\n",
    "        #compute unit similarties\n",
    "        original_template, closest_clusters, similarities, agreements, templates, shifts = get_unit_similarities(cluster_id, st_1, closest_clusters, sorting_hdbl_t, geom_array, raw_data_bin, \n",
    "                                                                                                                 num_channels_similarity=num_channels_similarity, \n",
    "                                                                                                                 num_close_clusters=num_close_clusters, shifts_align=shifts_align,\n",
    "                                                                                                                 order_by ='similarity')\n",
    "#         if similarities[0] < 2.0: #arbitrary..\n",
    "        fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_hdbl_t, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "                                     num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"both\",\n",
    "                                     denoised_waveforms=wfs_localized, cluster_labels=clusterer.labels_, non_triaged_idxs=non_triaged_idxs, triaged_mcs_abs=triaged_mcs_abs, \n",
    "                                     triaged_firstchans=triaged_firstchans)\n",
    "        fig.show()\n",
    "#         plt.close(fig)\n",
    "#         fig.savefig(save_dir_path + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93127c-72c9-4b05-af63-0fb5df6e3772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_dir_path_hdbscan_kilo = \"cluster_summaries_hdbscan_kilo\"\n",
    "# if not os.path.exists(save_dir_path_hdbscan_kilo):\n",
    "#     os.makedirs(save_dir_path_hdbscan_kilo)\n",
    "    \n",
    "# for cluster_id in tqdm(sorting_hdbl_t.get_unit_ids()):\n",
    "#     if cluster_id != -1:\n",
    "#         st_1 = sorting_hdbl_t.get_unit_spike_train(cluster_id)\n",
    "\n",
    "#         #compute K closest kilosort clsuters\n",
    "#         cluster_center = cluster_centers[cluster_id]\n",
    "#         curr_cluster_depth = cluster_center[1]\n",
    "#         dist_to_other_cluster_dict = {cluster_id:abs(mean_depth-curr_cluster_depth) for (cluster_id,mean_depth) in kilo_cluster_depth_means.items()}\n",
    "#         closest_clusters = [y[0] for y in sorted(dist_to_other_cluster_dict.items(), key = lambda x: x[1])[:num_close_clusters]]\n",
    "        \n",
    "#         fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_hdbl_t, sorting_kilo, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                      num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"second\")\n",
    "#         plt.close(fig)\n",
    "#         fig.savefig(save_dir_path_hdbscan_kilo + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870f35a-4487-4593-b9fa-b2d5828fed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir_path_hdbscan_kilo = \"cluster_summaries_hdbscan_kilo_agreement\"\n",
    "# if not os.path.exists(save_dir_path_hdbscan_kilo):\n",
    "#     os.makedirs(save_dir_path_hdbscan_kilo)\n",
    "    \n",
    "# for cluster_id in tqdm(sorting_hdbl_t.get_unit_ids()):\n",
    "#     if cluster_id != -1:\n",
    "#         st_1 = sorting_hdbl_t.get_unit_spike_train(cluster_id)\n",
    "\n",
    "#         #compute K closest kilosort clsuters\n",
    "#         cluster_center = cluster_centers[cluster_id]\n",
    "#         curr_cluster_depth = cluster_center[1]\n",
    "#         dist_to_other_cluster_dict = {cluster_id:abs(mean_depth-curr_cluster_depth) for (cluster_id,mean_depth) in kilo_cluster_depth_means.items()}\n",
    "#         closest_clusters = [y[0] for y in sorted(dist_to_other_cluster_dict.items(), key = lambda x: x[1])[:num_close_clusters]]\n",
    "        \n",
    "#         fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_hdbl_t, sorting_kilo, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                      num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='agreement', normalize_agreement_by=\"second\")\n",
    "#         plt.close(fig)\n",
    "#         fig.savefig(save_dir_path_hdbscan_kilo + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a80139-e7c5-4c7b-814a-261395749b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_dir_path_kilo_hdbscan = \"cluster_summaries_kilo_hdbscan\"\n",
    "# if not os.path.exists(save_dir_path_kilo_hdbscan):\n",
    "#     os.makedirs(save_dir_path_kilo_hdbscan)\n",
    "    \n",
    "# for cluster_id in tqdm(sorting_kilo.get_unit_ids()):\n",
    "\n",
    "#     st_1 = sorting_kilo.get_unit_spike_train(cluster_id)\n",
    "\n",
    "#     #compute K closest hdbscan clsuters\n",
    "#     curr_cluster_depth = kilo_cluster_depth_means[cluster_id]\n",
    "#     closest_clusters = np.argsort(np.abs(cluster_centers[:,1] - kilo_cluster_depth_means[cluster_id]))[:num_close_clusters]\n",
    "\n",
    "#     fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_kilo, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                  num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"second\")\n",
    "#     plt.close(fig)\n",
    "#     fig.savefig(save_dir_path_kilo_hdbscan + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11ed14-bab6-4e6d-a25a-e585161153ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir_path_kilo_hdbscan = \"cluster_summaries_kilo_hdbscan_agreement\"\n",
    "# if not os.path.exists(save_dir_path_kilo_hdbscan):\n",
    "#     os.makedirs(save_dir_path_kilo_hdbscan)\n",
    "    \n",
    "# for cluster_id in tqdm(sorting_kilo.get_unit_ids()):\n",
    "\n",
    "#     st_1 = sorting_kilo.get_unit_spike_train(cluster_id)\n",
    "\n",
    "#     #compute K closest hdbscan clsuters\n",
    "#     curr_cluster_depth = kilo_cluster_depth_means[cluster_id]\n",
    "#     closest_clusters = np.argsort(np.abs(cluster_centers[:,1] - kilo_cluster_depth_means[cluster_id]))[:num_close_clusters]\n",
    "\n",
    "#     fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_kilo, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                  num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='agreement', normalize_agreement_by=\"second\")\n",
    "#     plt.close(fig)\n",
    "#     fig.savefig(save_dir_path_kilo_hdbscan + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcac8a-d46b-4ede-a1dd-5c71f7d5b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot specific kilosort example\n",
    "# cluster_id = 166\n",
    "# num_close_clusters = 50\n",
    "# num_close_clusters_plot=10\n",
    "# num_channels_similarity = 20\n",
    "# shifts_align=np.arange(-8,9)\n",
    "# cluster_id = 116\n",
    "\n",
    "# st_1 = sorting_kilo.get_unit_spike_train(cluster_id)\n",
    "\n",
    "# #compute K closest hdbscan clsuters\n",
    "# curr_cluster_depth = kilo_cluster_depth_means[cluster_id]\n",
    "# closest_clusters = np.argsort(np.abs(cluster_centers[:,1] - kilo_cluster_depth_means[cluster_id]))[:num_close_clusters]\n",
    "\n",
    "# fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_kilo, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                              num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb8ccb-7b78-48bf-bc59-b7c96134072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot specific hdbscan example\n",
    "# cluster_id = 207\n",
    "# num_close_clusters = 50\n",
    "# num_close_clusters_plot=10\n",
    "# num_channels_similarity = 20\n",
    "# shifts_align=np.arange(-8,9)\n",
    "\n",
    "# st_1 = sorting_hdbl_t.get_unit_spike_train(cluster_id)\n",
    "\n",
    "# #compute K closest kilosort clsuters\n",
    "# cluster_center = cluster_centers[cluster_id]\n",
    "# curr_cluster_depth = cluster_center[1]\n",
    "# dist_to_other_cluster_dict = {cluster_id:abs(mean_depth-curr_cluster_depth) for (cluster_id,mean_depth) in kilo_cluster_depth_means.items()}\n",
    "# closest_clusters = [y[0] for y in sorted(dist_to_other_cluster_dict.items(), key = lambda x: x[1])[:num_close_clusters]]\n",
    "\n",
    "# fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_hdbl_t, sorting_kilo, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                              num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"second\")\n",
    "# # plt.close(fig)\n",
    "# # fig.savefig(save_dir_path_kilo_hdbscan + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

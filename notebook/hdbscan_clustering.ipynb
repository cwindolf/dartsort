{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a81481e-eb58-45be-a2d0-c86fbe81c3d3",
   "metadata": {},
   "source": [
    "# Clustering with localization-derived features with hdbscan\n",
    "\n",
    "HDBSCAN is a clustering algorithm developed by Campello, Moulavi, and Sander. It extends DBSCAN by converting it into a hierarchical clustering algorithm, and then using a technique to extract a flat clustering based in the stability of clusters. \n",
    "\n",
    "Steps\n",
    "> 1. Transform the space according to the density/sparsity. \n",
    "  2. Build the minimum spanning tree of the distance weighted graph. \n",
    "  3. Construct a cluster hierarchy of connected components. \n",
    "  4. Condense the cluster hierarchy based on minimum cluster size.\n",
    "  5. Extract the stable clusters from the condensed tree.\n",
    "\n",
    "Important parameters\n",
    "\n",
    "> min_cluster_size, int, optional (default=5): The minimum size of clusters; single linkage splits that contain fewer points than this will be considered points “falling out” of a cluster rather than a cluster splitting into two new clusters.\n",
    "\n",
    "> min_samples, int, optional (default=None): The number of samples in a neighbourhood for a point to be considered a core point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ae6d49-91ba-442a-9bef-2d5fa009b8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'run_weighted_triage' from 'spike_psvae.cluster_utils' (/media/cat/julien/spike_psvae/spike_psvae/cluster_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_viz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_self_agreement, plot_single_unit_summary, plot_agreement_venn, plot_isi_distribution, plot_unit_similarities\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_viz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_unit_similarity_heatmaps\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_sorting_from_labels_frames, compute_cluster_centers, relabel_by_depth, run_weighted_triage, remove_duplicate_units\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_agreement_indices, compute_spiketrain_agreement, get_unit_similarities, compute_shifted_similarity, read_waveforms\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspike_psvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_closest_clusters_hdbscan, get_closest_clusters_kilosort, get_closest_clusters_hdbscan_kilosort, get_closest_clusters_kilosort_hdbscan\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'run_weighted_triage' from 'spike_psvae.cluster_utils' (/media/cat/julien/spike_psvae/spike_psvae/cluster_utils.py)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import argparse\n",
    "import hdbscan\n",
    "from spike_psvae.cluster_viz import cluster_scatter, plot_waveforms_geom, plot_venn_agreement\n",
    "from spike_psvae.cluster_viz import plot_self_agreement, plot_single_unit_summary, plot_agreement_venn, plot_isi_distribution, plot_unit_similarities\n",
    "from spike_psvae.cluster_viz import plot_unit_similarity_heatmaps\n",
    "from spike_psvae.cluster_utils import make_sorting_from_labels_frames, compute_cluster_centers, relabel_by_depth, run_weighted_triage, remove_duplicate_units\n",
    "from spike_psvae.cluster_utils import get_agreement_indices, compute_spiketrain_agreement, get_unit_similarities, compute_shifted_similarity, read_waveforms\n",
    "from spike_psvae.cluster_utils import get_closest_clusters_hdbscan, get_closest_clusters_kilosort, get_closest_clusters_hdbscan_kilosort, get_closest_clusters_kilosort_hdbscan\n",
    "import spikeinterface \n",
    "from spikeinterface.toolkit import compute_correlograms\n",
    "from spikeinterface.comparison import compare_two_sorters\n",
    "from spikeinterface.widgets import plot_agreement_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib_venn import venn3, venn3_circles, venn2\n",
    "import matplotlib.gridspec as gridspec\n",
    "from spike_psvae.merge_split import split_clusters, get_templates, get_merged, align_templates\n",
    "from spike_psvae.denoise import SingleChanDenoiser\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "%matplotlib inline\n",
    "import pandas\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "import h5py\n",
    "import pickle\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "#random seed for provenance\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af75856-a2b6-4081-bbbb-a75e7e2fb1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = 'np1_channel_map.npy'\n",
    "do_infer_ptp = False\n",
    "num_spikes_cluster = None\n",
    "min_cluster_size = 25\n",
    "min_samples = 25\n",
    "num_spikes_plot = 250\n",
    "num_rows_plot = 3\n",
    "num_channels = 40\n",
    "no_verbose = True\n",
    "\n",
    "data_path = '/media/cat/cole/'\n",
    "data_name = 'CSH_ZAD_026_1800_1860'\n",
    "data_dir = data_path + data_name + '/'\n",
    "raw_data_bin = data_dir + '1min_standardized.bin'\n",
    "residual_data_bin = data_dir + 'residual_1min_standardized_t_0_None.bin'\n",
    "\n",
    "#load features\n",
    "spike_index = np.load(data_dir+'spike_index.npy')\n",
    "num_spikes = spike_index.shape[0]\n",
    "spike_index[:,0] = spike_index[:,0] #only for Hyun's data\n",
    "results_localization = np.load(data_dir+'localization_results.npy')\n",
    "ptps_localized = np.load(data_dir+'ptps.npy')\n",
    "geom_array = np.load(data_dir+geom)\n",
    "#AE features not used at this point.\n",
    "# ae_features = np.load(data_dir+'ae_features.npy') \n",
    "# register displacement (here starts at sec 50)\n",
    "# displacement = np.load(data_dir+'displacement_array.npy' )(if you have displacement)\n",
    "# z_abs = results_localization[:, 1] - displacement[spike_index[:, 0]//30000] (if you have displacement)\n",
    "z_abs =  np.load(data_dir+'z_reg.npy') #if you already have registered zs\n",
    "x = results_localization[:, 0]\n",
    "y = results_localization[:, 2]\n",
    "z = z_abs\n",
    "alpha = results_localization[:, 3]\n",
    "maxptps = results_localization[:, 4]\n",
    "ae_features = np.load(data_dir+'ae_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9d6f3-e14d-4197-a589-16534461c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load denoising NN\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "denoiser = SingleChanDenoiser()\n",
    "denoiser.load()\n",
    "denoiser.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc9bcba-cd5e-4a90-806a-7b62eb63daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform triaging \n",
    "# triaged_x, triaged_y, triaged_z, triaged_alpha, triaged_maxptps, triaged_ae_features, ptp_filter, idx_keep = run_weighted_triage(x, y, z, alpha, maxptps, pcs=ae_features, threshold=75, ptp_threshold=3, ptp_weighting=True) #pcs is None here\n",
    "triaged_x, triaged_y, triaged_z, triaged_alpha, triaged_maxptps, _, ptp_filter, idx_keep = run_weighted_triage(x, y, z, alpha, maxptps, threshold=80, ptp_threshold=3, ptp_weighting=True) #pcs is None here\n",
    "# triaged_x, triaged_y, triaged_z, triaged_alpha, triaged_maxptps, _, ptp_filter, idx_keep = run_weighted_triage(x, y, z, alpha, maxptps, threshold=100, ptp_threshold=0, ptp_weighting=False) #pcs is None here\n",
    "triaged_spike_index = spike_index[ptp_filter][idx_keep]\n",
    "triaged_mcs_abs = spike_index[:,1][ptp_filter][idx_keep]\n",
    "non_triaged_idxs = ptp_filter[0][idx_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3d5a9-727a-4c56-8a5e-c8fcbffb3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(spike_index[:,1].size, dtype=bool)\n",
    "mask[ptp_filter[0][idx_keep]] = False\n",
    "triaged_indices = np.where(mask)[0]\n",
    "# np.save('triaged_indices', triaged_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776db0a-cfba-4140-8325-b67f9bf1e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load firstchans\n",
    "#triaged_firstchans = results_localization[:,5][ptp_filter][idx_keep] #if you saved firstchans\n",
    "filename = data_dir + \"subtraction_1min_standardized_t_0_None.h5\"\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[2]\n",
    "    firstchans = np.asarray(list(f[\"first_channels\"]))\n",
    "    print(f[\"end_sample\"])\n",
    "    end_sample = f[\"end_sample\"][()]\n",
    "    start_sample = f[\"start_sample\"][()]\n",
    "triaged_firstchans = firstchans[ptp_filter][idx_keep]\n",
    "end_time = end_sample / 30000\n",
    "start_time = start_sample / 30000\n",
    "recording_duration = end_time - start_time\n",
    "print(f\"duration of recording: {recording_duration} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b3bd2-5071-481b-a26a-25c266988595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kilosort results\n",
    "kilo_spike_samples = np.load(data_dir + 'kilosort_spk_samples.npy')\n",
    "kilo_spike_frames = (kilo_spike_samples - 30*recording_duration*30000) #to match our detection alignment\n",
    "kilo_spike_clusters = np.load(data_dir + 'kilsort_spk_clusters.npy')\n",
    "kilo_spike_depths = np.load(data_dir + 'kilsort_spk_depths.npy')\n",
    "kilo_cluster_depth_means = {}\n",
    "for cluster_id in np.unique(kilo_spike_clusters):\n",
    "    kilo_cluster_depth_means[cluster_id] = np.mean(kilo_spike_depths[kilo_spike_clusters==cluster_id])\n",
    "    \n",
    "#create kilosort SpikeInterface sorting\n",
    "sorting_kilo = make_sorting_from_labels_frames(kilo_spike_clusters, kilo_spike_frames)\n",
    "    \n",
    "good_kilo_sort_clusters_all = np.array([  0,  17,  19,  25,  30,  33,  36,  38,  41,  47,  48,  53,  64,\n",
    "        70,  78,  82,  83,  85,  88,  90,  97, 103, 109, 112, 114, 115,\n",
    "       117, 119, 120, 131, 132, 133, 141, 142, 153, 158, 169, 172, 185,\n",
    "       187, 189, 193, 197, 199, 205, 208, 211, 215, 217, 224, 237, 244,\n",
    "       247, 269, 272, 274, 280, 283, 289, 291, 292, 296, 300, 303, 304,\n",
    "       308, 309, 320, 328, 331, 336, 341, 349, 350, 380, 382, 386, 400,\n",
    "       409, 411, 414, 435, 438, 439, 464, 474, 476, 478, 485, 487, 488,\n",
    "       496, 503, 509, 512, 521, 522, 523, 529, 533, 534, 535, 536, 537,\n",
    "       539, 544, 545, 547, 548, 551, 552, 555, 557, 570, 583, 596, 598,\n",
    "       621, 629, 633, 637, 648, 655, 660, 670, 671, 677, 678, 681, 682,\n",
    "       683, 699, 700, 702, 708, 709])\n",
    "\n",
    "#remove empty clusters\n",
    "good_kilo_sort_clusters = []\n",
    "for good_cluster in good_kilo_sort_clusters_all:\n",
    "    if good_cluster in sorting_kilo.get_unit_ids():\n",
    "        good_kilo_sort_clusters.append(good_cluster)\n",
    "good_kilo_sort_clusters = np.asarray(good_kilo_sort_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e36cc-bd93-4f8a-b92c-09133dfe65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create feature set for clustering\n",
    "if num_spikes_cluster is None:\n",
    "    num_spikes = triaged_x.shape[0]\n",
    "else:\n",
    "    num_spikes = num_spikes_cluster\n",
    "triaged_firstchans = triaged_firstchans[:num_spikes]\n",
    "triaged_alpha = triaged_alpha[:num_spikes]\n",
    "triaged_spike_index = triaged_spike_index[:num_spikes]\n",
    "triaged_x = triaged_x[:num_spikes]\n",
    "triaged_y = triaged_y[:num_spikes]\n",
    "triaged_z = triaged_z[:num_spikes]\n",
    "triaged_maxptps = triaged_maxptps[:num_spikes]\n",
    "triaged_mcs_abs = triaged_mcs_abs[:num_spikes]\n",
    "# triaged_ae_features = triaged_ae_features[:num_spikes]\n",
    "\n",
    "scales = (1,10,1,15,30,10) #predefined scales for each feature\n",
    "features = np.concatenate((np.expand_dims(triaged_x,1), np.expand_dims(triaged_z,1), np.expand_dims(np.log(triaged_maxptps)*scales[4],1)), axis=1)\n",
    "# features = np.concatenate((np.expand_dims(triaged_x,1), np.expand_dims(triaged_z,1), triaged_ae_features*scales[5]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa888c6-175f-41f3-a458-449514149b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform hdbscan clustering\n",
    "min_cluster_size =  min_cluster_size\n",
    "min_samples = min_samples\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "clusterer.fit(features)\n",
    "if no_verbose:\n",
    "    print(clusterer)\n",
    "\n",
    "#compute cluster centers\n",
    "cluster_centers = compute_cluster_centers(clusterer)\n",
    "    \n",
    "#re-label each cluster by z-depth\n",
    "clusterer = relabel_by_depth(clusterer, cluster_centers)\n",
    "\n",
    "#remove duplicate units by spike_times_agreement and ptp\n",
    "clusterer, duplicate_ids = remove_duplicate_units(clusterer, triaged_spike_index[:,0], triaged_maxptps)\n",
    "\n",
    "#re-compute cluster centers\n",
    "cluster_centers = compute_cluster_centers(clusterer)\n",
    "\n",
    "#re-label each cluster by z-depth\n",
    "clusterer = relabel_by_depth(clusterer, cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c88d36-288b-497b-973e-07017cbc48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create hdbscan/localization SpikeInterface sorting (with triage)\n",
    "sorting_hdbl_t_original = make_sorting_from_labels_frames(clusterer.labels_, triaged_spike_index[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b9519-0140-4504-bf91-f380b771c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir = cm.get_cmap('viridis')\n",
    "triaged_log_ptp = triaged_maxptps.copy()\n",
    "triaged_log_ptp[triaged_log_ptp >= 27.5] = 27.5\n",
    "triaged_log_ptp = np.log(triaged_log_ptp+1)\n",
    "triaged_log_ptp[triaged_log_ptp<=1.25] = 1.25\n",
    "triaged_ptp_rescaled = (triaged_log_ptp - triaged_log_ptp.min())/(triaged_log_ptp.max() - triaged_log_ptp.min())\n",
    "color_arr = vir(triaged_ptp_rescaled)\n",
    "color_arr[:, 3] = triaged_ptp_rescaled\n",
    "\n",
    "# ## Define colors\n",
    "unique_colors = ['#e6194b', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#008080', '#e6beff', '#9a6324', '#800000', '#aaffc3', '#808000', '#000075', '#000000']\n",
    "\n",
    "cluster_color_dict = {}\n",
    "for cluster_id in np.unique(clusterer.labels_):\n",
    "    cluster_color_dict[cluster_id] = unique_colors[cluster_id % len(unique_colors)]\n",
    "cluster_color_dict[-1] = '#808080' #set outlier color to grey\n",
    "\n",
    "##### plot array scatter #####\n",
    "fig = plot_array_scatter(clusterer.labels_, geom_array, triaged_x, triaged_z, triaged_maxptps, cluster_color_dict, color_arr, min_cluster_size=clusterer.min_cluster_size, min_samples=clusterer.min_samples, \n",
    "                         z_cutoff=(0, 500), figsize=(18, 12))\n",
    "# fig.suptitle(f'x,z,scaled_logptp features,\" {num_spikes} datapoints');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be8494-56e2-4d7a-bc84-50f26adaeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####split units####\n",
    "templates = get_templates(raw_data_bin, geom_array, clusterer.labels_.max()+1, triaged_spike_index, clusterer.labels_)\n",
    "\n",
    "plt.plot(templates[12]);\n",
    "\n",
    "#align all templates to 42\n",
    "triaged_spike_index, idx_sorted = align_templates(clusterer.labels_, templates, triaged_spike_index)\n",
    "\n",
    "clusterer.labels_ = clusterer.labels_[idx_sorted]\n",
    "triaged_x = triaged_x[idx_sorted]\n",
    "triaged_z = triaged_z[idx_sorted]\n",
    "triaged_maxptps = triaged_maxptps[idx_sorted]\n",
    "triaged_alpha = triaged_alpha[idx_sorted]\n",
    "triaged_firstchans = triaged_firstchans[idx_sorted]\n",
    "triaged_mcs_abs = triaged_mcs_abs[idx_sorted]\n",
    "non_triaged_idxs = non_triaged_idxs[idx_sorted]\n",
    "\n",
    "\n",
    "#split clusters\n",
    "labels_split = split_clusters(raw_data_bin, triaged_spike_index, clusterer.labels_, triaged_x, triaged_z, triaged_maxptps, geom_array, denoiser, device, n_channels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c542b5-e38d-4c9b-a408-417e6887df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(labels_split).shape)\n",
    "\n",
    "#change clusterer raw data\n",
    "raw_data = np.concatenate((np.expand_dims(triaged_x,1), np.expand_dims(triaged_z,1), np.expand_dims(np.log(triaged_maxptps)*scales[4],1)), axis=1)\n",
    "clusterer._raw_data = raw_data\n",
    "\n",
    "clusterer.labels_ = labels_split\n",
    "\n",
    "#compute cluster centers\n",
    "cluster_centers = compute_cluster_centers(clusterer)\n",
    "    \n",
    "#re-label each cluster by z-depth\n",
    "clusterer = relabel_by_depth(clusterer, cluster_centers)\n",
    "\n",
    "#compute cluster centers\n",
    "cluster_centers = compute_cluster_centers(clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615c03b-77e2-4555-b794-0f13723a0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir = cm.get_cmap('viridis')\n",
    "triaged_log_ptp = triaged_maxptps.copy()\n",
    "triaged_log_ptp[triaged_log_ptp >= 27.5] = 27.5\n",
    "triaged_log_ptp = np.log(triaged_log_ptp+1)\n",
    "triaged_log_ptp[triaged_log_ptp<=1.25] = 1.25\n",
    "triaged_ptp_rescaled = (triaged_log_ptp - triaged_log_ptp.min())/(triaged_log_ptp.max() - triaged_log_ptp.min())\n",
    "color_arr = vir(triaged_ptp_rescaled)\n",
    "color_arr[:, 3] = triaged_ptp_rescaled\n",
    "\n",
    "# ## Define colors\n",
    "unique_colors = ['#e6194b', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#008080', '#e6beff', '#9a6324', '#800000', '#aaffc3', '#808000', '#000075', '#000000']\n",
    "\n",
    "cluster_color_dict = {}\n",
    "for cluster_id in np.unique(clusterer.labels_):\n",
    "    cluster_color_dict[cluster_id] = unique_colors[cluster_id % len(unique_colors)]\n",
    "cluster_color_dict[-1] = '#808080' #set outlier color to grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a91e40-3639-42f4-948b-240617eeddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### plot array scatter #####\n",
    "fig = plot_array_scatter(clusterer.labels_, geom_array, triaged_x, triaged_z, triaged_maxptps, cluster_color_dict, color_arr, min_cluster_size=clusterer.min_cluster_size, min_samples=clusterer.min_samples, \n",
    "                         z_cutoff=(0, 500), figsize=(18, 12))\n",
    "# fig.suptitle(f'x,z,scaled_logptp features,\" {num_spikes} datapoints');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597c3c3-b7b3-4fcf-855b-dc5e613f4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####merge units####\n",
    "templates = get_templates(raw_data_bin, geom_array, clusterer.labels_.max()+1, triaged_spike_index, clusterer.labels_)\n",
    "\n",
    "#align all templates to 42\n",
    "triaged_spike_index, idx_sorted = align_templates(clusterer.labels_, templates, triaged_spike_index)\n",
    "\n",
    "clusterer.labels_ = clusterer.labels_[idx_sorted]\n",
    "triaged_x = triaged_x[idx_sorted]\n",
    "triaged_z = triaged_z[idx_sorted]\n",
    "triaged_maxptps = triaged_maxptps[idx_sorted]\n",
    "triaged_alpha = triaged_alpha[idx_sorted]\n",
    "triaged_firstchans = triaged_firstchans[idx_sorted]\n",
    "triaged_mcs_abs = triaged_mcs_abs[idx_sorted]\n",
    "non_triaged_idxs = non_triaged_idxs[idx_sorted]\n",
    "\n",
    "#change clusterer raw data\n",
    "raw_data = np.concatenate((np.expand_dims(triaged_x,1), np.expand_dims(triaged_z,1), np.expand_dims(np.log(triaged_maxptps)*scales[4],1)), axis=1)\n",
    "clusterer._raw_data = raw_data\n",
    "\n",
    "labels_merged = get_merged(raw_data_bin, geom_array, clusterer.labels_.max()+1, triaged_spike_index, clusterer.labels_, triaged_x, triaged_z, denoiser, device, n_channels=10, n_temp = 5, distance_threshold = 4.0, threshold_diptest = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17c0b1-9b39-4649-91fd-a165190b0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(labels_merged).shape)\n",
    "clusterer.labels_ = labels_merged\n",
    "\n",
    "#compute cluster centers\n",
    "cluster_centers = compute_cluster_centers(clusterer)\n",
    "    \n",
    "#re-label each cluster by z-depth\n",
    "clusterer = relabel_by_depth(clusterer, cluster_centers)\n",
    "\n",
    "#compute cluster centers\n",
    "cluster_centers = compute_cluster_centers(clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6556553-6b27-4a35-8792-100919289220",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir = cm.get_cmap('viridis')\n",
    "triaged_log_ptp = triaged_maxptps.copy()\n",
    "triaged_log_ptp[triaged_log_ptp >= 27.5] = 27.5\n",
    "triaged_log_ptp = np.log(triaged_log_ptp+1)\n",
    "triaged_log_ptp[triaged_log_ptp<=1.25] = 1.25\n",
    "triaged_ptp_rescaled = (triaged_log_ptp - triaged_log_ptp.min())/(triaged_log_ptp.max() - triaged_log_ptp.min())\n",
    "color_arr = vir(triaged_ptp_rescaled)\n",
    "color_arr[:, 3] = triaged_ptp_rescaled\n",
    "\n",
    "# ## Define colors\n",
    "unique_colors = ['#e6194b', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#008080', '#e6beff', '#9a6324', '#800000', '#aaffc3', '#808000', '#000075', '#000000']\n",
    "\n",
    "cluster_color_dict = {}\n",
    "for cluster_id in np.unique(clusterer.labels_):\n",
    "    cluster_color_dict[cluster_id] = unique_colors[cluster_id % len(unique_colors)]\n",
    "cluster_color_dict[-1] = '#808080' #set outlier color to grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b659440-7e1a-454f-92f1-1ae7378ceb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### plot array scatter #####\n",
    "fig = plot_array_scatter(clusterer.labels_, geom_array, triaged_x, triaged_z, triaged_maxptps, cluster_color_dict, color_arr, min_cluster_size=clusterer.min_cluster_size, min_samples=clusterer.min_samples, \n",
    "                         z_cutoff=(0, 500), figsize=(18, 12))\n",
    "# fig.suptitle(f'x,z,scaled_logptp features,\" {num_spikes} datapoints');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42fb6a-afc2-4a4e-9d35-9af59db8e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2e899-02ea-49db-b9bf-ed42866701c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### plot individual cluster summaries #####\n",
    "#load waveforms as memmap\n",
    "wfs_localized = np.load(data_dir+'denoised_wfs.npy', mmap_mode='r') #np.memmap(data_dir+'denoised_waveforms.npy', dtype='float32', shape=(290025, 121, 40))\n",
    "wfs_subtracted = np.load(data_dir+'subtracted_wfs.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c8087-dc60-462b-b8d7-270f64641195",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 327\n",
    "num_spikes_plot=50\n",
    "#plot cluster summary\n",
    "fig = plot_single_unit_summary(cluster_id, clusterer.labels_, cluster_centers, geom_array, num_spikes_plot, num_rows_plot, triaged_x, triaged_z, triaged_maxptps, \n",
    "                               triaged_firstchans, triaged_mcs_abs, triaged_spike_index[:,0], non_triaged_idxs, wfs_localized, wfs_subtracted, cluster_color_dict, \n",
    "                               color_arr, raw_data_bin, residual_data_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c088fc1-509d-454f-90bb-7a750559a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create hdbscan/localization SpikeInterface sorting (with triage)\n",
    "sorting_hdbl_t = make_sorting_from_labels_frames(clusterer.labels_, triaged_spike_index[:,0])\n",
    "\n",
    "cmp_5 = compare_two_sorters(sorting_hdbl_t, sorting_kilo, sorting1_name='ours', sorting2_name='kilosort', match_score=.5)\n",
    "matched_units_5 = cmp_5.get_matching()[0].index.to_numpy()[np.where(cmp_5.get_matching()[0] != -1.)]\n",
    "matches_kilos_5 = cmp_5.get_best_unit_match1(matched_units_5).values.astype('int')\n",
    "\n",
    "cmp_1 = compare_two_sorters(sorting_hdbl_t, sorting_kilo, sorting1_name='ours', sorting2_name='kilosort', match_score=.1)\n",
    "matched_units_1 = cmp_1.get_matching()[0].index.to_numpy()[np.where(cmp_1.get_matching()[0] != -1.)]\n",
    "unmatched_units_1 = cmp_1.get_matching()[0].index.to_numpy()[np.where(cmp_1.get_matching()[0] == -1.)]\n",
    "matches_kilos_1 = cmp_1.get_best_unit_match1(matched_units_1).values.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd5ad2-53bd-4d72-a11e-8d0415041035",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_kilo_5 = compare_two_sorters(sorting_kilo, sorting_hdbl_t, sorting1_name='kilosort', sorting2_name='ours', match_score=.5)\n",
    "matched_units_kilo_5 = cmp_kilo_5.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_5.get_matching()[0] != -1.)]\n",
    "unmatched_units_kilo_5 = cmp_kilo_5.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_5.get_matching()[0] == -1.)]\n",
    "\n",
    "cmp_kilo_1 = compare_two_sorters(sorting_kilo, sorting_hdbl_t, sorting1_name='kilosort', sorting2_name='ours', match_score=.1)\n",
    "matched_units_kilo_1 = cmp_kilo_1.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_1.get_matching()[0].to_numpy() != -1.)]\n",
    "unmatched_units_kilo_1 = cmp_kilo_1.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_1.get_matching()[0].to_numpy() == -1.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce029e4-bf7f-49be-b378-54368222e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# from isosplit import isocut\n",
    "\n",
    "# relmaxa = max_ptp_channel - max_ptp_similar\n",
    "# wfs_a = wfs_localized[non_triaged_idxs[clusterer.labels_==336]]\n",
    "# wfs_b = wfs_localized[non_triaged_idxs[clusterer.labels_==338]]\n",
    "# n_channels = 40\n",
    "# wfs_diptest = np.concatenate((wfs_a, wfs_b)).reshape((-1, n_channels*121))\n",
    "# labels_diptest = np.zeros(wfs_a.shape[0]+wfs_b.shape[0])\n",
    "# labels_diptest[:wfs_a.shape[0]] = 1\n",
    "\n",
    "# lda_model = LDA(n_components = 1)\n",
    "# lda_comps = lda_model.fit_transform(wfs_diptest, labels_diptest)\n",
    "# value_dpt, cut_calue = isocut(lda_comps[:, 0])\n",
    "\n",
    "# plt.hist(lda_comps[:, 0], bins = 20)\n",
    "# plt.title(\"Diptest value : \" + str(value_dpt))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724c93b-ffb2-47d5-a323-5c39c0ff7185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###hdbscan\n",
    "save_dir_path = \"good_unit_kilo_comparison2\"\n",
    "if not os.path.exists(save_dir_path):\n",
    "    os.makedirs(save_dir_path)\n",
    "\n",
    "for good_kilo_sort_cluster in good_kilo_sort_clusters:\n",
    "    cluster_id_match = good_kilo_sort_cluster\n",
    "    cluster_id = int(cmp_kilo_1.get_best_unit_match1(cluster_id_match))\n",
    "    depth = int(kilo_cluster_depth_means[cluster_id_match])\n",
    "    save_str = str(depth).zfill(4)\n",
    "    if cluster_id != -1:\n",
    "        sorting1 = sorting_hdbl_t\n",
    "        sorting2 = sorting_kilo\n",
    "        sorting1_name = \"hdb\"\n",
    "        sorting2_name = \"kilo\"\n",
    "        firstchans_cluster_sorting1 = triaged_firstchans[clusterer.labels_ == cluster_id]\n",
    "        mcs_abs_cluster_sorting1 = triaged_mcs_abs[clusterer.labels_ == cluster_id]\n",
    "        spike_depths = kilo_spike_depths[np.where(kilo_spike_clusters==cluster_id_match)]\n",
    "        mcs_abs_cluster_sorting2 = np.asarray([np.argmin(np.abs(spike_depth - geom_array[:,1])) for spike_depth in spike_depths])\n",
    "        firstchans_cluster_sorting2 = (mcs_abs_cluster_sorting2 - 20).clip(min=0)\n",
    "\n",
    "        fig = plot_agreement_venn(cluster_id, cluster_id_match, cmp_1, sorting1, sorting2, sorting1_name, sorting2_name, geom_array, num_channels, num_spikes_plot, firstchans_cluster_sorting1, mcs_abs_cluster_sorting1, \n",
    "                                  firstchans_cluster_sorting2, mcs_abs_cluster_sorting2, raw_data_bin, delta_frames = 12, alpha=.2)\n",
    "        plt.close(fig)\n",
    "        fig.savefig(save_dir_path + f\"/Z{save_str}_{cluster_id_match}_{cluster_id}_comparison.png\")\n",
    "    # else:\n",
    "    num_spikes = len(sorting_kilo.get_unit_spike_train(cluster_id_match))\n",
    "    print(f\"skipped {cluster_id_match} with {num_spikes} spikes\")\n",
    "    if num_spikes > 0:\n",
    "        #plot specific kilosort example\n",
    "        num_close_clusters = 50\n",
    "        num_close_clusters_plot=10\n",
    "        num_channels_similarity = 20\n",
    "        shifts_align=np.arange(-8,9)\n",
    "\n",
    "        st_1 = sorting_kilo.get_unit_spike_train(cluster_id_match)\n",
    "\n",
    "        #compute K closest hdbscan clsuters\n",
    "        closest_clusters = get_closest_clusters_kilosort_hdbscan(cluster_id_match, kilo_cluster_depth_means, cluster_centers, num_close_clusters)\n",
    "\n",
    "        fig = plot_unit_similarities(cluster_id_match, closest_clusters, sorting_kilo, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "                                     num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"both\")\n",
    "        plt.close(fig)\n",
    "        fig.savefig(save_dir_path + f\"/Z{save_str}_{cluster_id_match}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c096fe-1517-46bd-8f43-d65c3c98026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_psvae import denoise, subtract, localization, ibme, deconvolve, residual\n",
    "\n",
    "###hdbscan\n",
    "save_dir_path = \"/media/cat/cole/deconv_results\"\n",
    "if not os.path.exists(save_dir_path):\n",
    "    os.makedirs(save_dir_path)\n",
    "output_directory = save_dir_path\n",
    "geom_path = data_dir+geom\n",
    "\n",
    "result_file_names = deconvolve.deconvolution(triaged_spike_index[np.not_equal(clusterer.labels_, -1)], clusterer.labels_[np.not_equal(clusterer.labels_, -1)], \n",
    "                                             output_directory, raw_data_bin, \n",
    "                                             geom_path, multi_processing=True, n_processors=6, threshold=40)\n",
    "\n",
    "deconv_spike_train_up = np.load(result_file_names[1])\n",
    "deconv_templates_up = np.load(result_file_names[0])\n",
    "\n",
    "deconv_spike_train = np.load(result_file_names[3])\n",
    "deconv_templates = np.load(result_file_names[2])\n",
    "\n",
    "\n",
    "n_spikes = deconv_spike_train.shape[0]\n",
    "print(f'number of deconv spikes: {n_spikes}')\n",
    "print(f'deconv templates shape: {deconv_templates.shape}')\n",
    "\n",
    "residual_path = residual.run_residual(result_file_names[0], result_file_names[1],\n",
    "                                      output_directory, raw_data_bin, geom_path)\n",
    "\n",
    "# channels to extract for each mc\n",
    "extract_channel_index = []\n",
    "for c in range(384):\n",
    "    low = max(0, c - 40 // 2)\n",
    "    low = min(384 - 40, low)\n",
    "    extract_channel_index.append(\n",
    "        np.arange(low, low + 40)\n",
    "    )\n",
    "extract_channel_index = np.array(extract_channel_index)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# load tPCA\n",
    "h5_subtract = data_dir + \"subtraction_1min_standardized_t_0_None.h5\"\n",
    "with h5py.File(h5_subtract, \"r\") as f:\n",
    "    tpca_components = f['tpca_components'][:]\n",
    "    tpca_mean = f['tpca_mean'][:]\n",
    "\n",
    "tpca = PCA(8)\n",
    "tpca.components_ = tpca_components\n",
    "tpca.mean_ = tpca_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d41b74-f79f-4d6e-90f2-fd72423f3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_align(waveforms, maxchans, offset=42):\n",
    "    N, T, C = waveforms.shape\n",
    "    offsets = waveforms[np.arange(N), :, maxchans].argmin(1)\n",
    "    rolls = offset - offsets\n",
    "    out = np.empty_like(waveforms)\n",
    "    pads = [(0, 0), (0, 0)]\n",
    "    for i, roll in enumerate(rolls):\n",
    "        if roll > 0:\n",
    "            pads[0] = (roll, 0)\n",
    "            start, end = 0, T\n",
    "        elif roll < 0:\n",
    "            pads[0] = (0, -roll)\n",
    "            start, end = -roll, T - roll\n",
    "        else:\n",
    "            out[i] = waveforms[i]\n",
    "            continue\n",
    "\n",
    "        pwf = np.pad(waveforms[i], pads, mode=\"linear_ramp\")\n",
    "        out[i] = pwf[start:end, :]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb8218-e95e-446e-99ab-a8e395aab9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtracted_waveforms_dir = os.path.join(output_directory, 'subtracted_waveforms')\n",
    "if not os.path.exists(subtracted_waveforms_dir):\n",
    "    os.makedirs(subtracted_waveforms_dir)\n",
    "    \n",
    "collision_subtracted_waveforms_dir = os.path.join(output_directory, 'collision_subtracted_waveforms')\n",
    "if not os.path.exists(collision_subtracted_waveforms_dir):\n",
    "    os.makedirs(collision_subtracted_waveforms_dir)\n",
    "    \n",
    "denoised_waveforms_dir = os.path.join(output_directory, 'denoised_waveforms')\n",
    "if not os.path.exists(denoised_waveforms_dir):\n",
    "    os.makedirs(denoised_waveforms_dir)\n",
    "    \n",
    "geom_array = np.load(geom_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bafb4-b00c-4492-a00a-eb0ccf3468e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = 0\n",
    "batch_size=1024\n",
    "\n",
    "skipped_count = 0\n",
    "\n",
    "deconv_spike_index = np.zeros((n_spikes,2)).astype(int)\n",
    "deconv_labels = np.zeros(n_spikes).astype(int)\n",
    "for start in tqdm(range(0, n_spikes, batch_size)):\n",
    "    end = start + batch_size\n",
    "    \n",
    "    batch_deconv_spike_train_up = deconv_spike_train_up[start:end]\n",
    "    batch_t, batch_template_idx = batch_deconv_spike_train_up[:,0], batch_deconv_spike_train_up[:,1]\n",
    "    batch_subtracted_wfs = deconv_templates_up[batch_template_idx]\n",
    "    batch_mcs = batch_subtracted_wfs.ptp(1).argmax(1)\n",
    "    batch_extract_channel_index = extract_channel_index[batch_mcs]\n",
    "\n",
    "    batch_subtracted_wfs = np.array(list(map(lambda x, idx: x[:,idx], \n",
    "                                             batch_subtracted_wfs, \n",
    "                                             batch_extract_channel_index)))\n",
    "    \n",
    "    deconv_spike_index[start:end,0] = batch_t\n",
    "    deconv_spike_index[start:end,1] = batch_mcs\n",
    "    deconv_labels[start:end] = batch_template_idx\n",
    "    \n",
    "    # load residual batch\n",
    "    residual_batch, skipped_idx = deconvolve.read_waveforms(batch_t, residual_path, geom_array)\n",
    "    residual_batch = np.array(list(map(lambda x, idx: x[:,idx], \n",
    "                                         residual_batch, \n",
    "                                         batch_extract_channel_index)))\n",
    "    kept_idx = np.ones(batch_subtracted_wfs.shape[0]).astype(bool)\n",
    "    kept_idx[skipped_idx] = False\n",
    "    skipped_count += len(skipped_idx)\n",
    "    \n",
    "    batch_collision_subtracted_wfs = batch_subtracted_wfs[kept_idx] + residual_batch\n",
    "    \n",
    "    relative_batch_mcs = np.where(batch_extract_channel_index-batch_mcs[:,None]==0)[1]\n",
    "    aligned_wfs = temporal_align(batch_collision_subtracted_wfs, relative_batch_mcs)\n",
    "\n",
    "    batch_denoised_wfs = subtract.full_denoising(aligned_wfs, \n",
    "                                       batch_mcs,\n",
    "                                       extract_channel_index,\n",
    "                                       None,\n",
    "                                       probe='np1',\n",
    "                                       tpca=tpca,\n",
    "                                       device=device,\n",
    "                                       denoiser=denoiser,\n",
    "                                       )\n",
    "    \n",
    "    np.save(os.path.join(subtracted_waveforms_dir,f'subtracted_{str(batch_id).zfill(6)}.npy'), \n",
    "            batch_subtracted_wfs.astype(np.float32))\n",
    "    np.save(os.path.join(collision_subtracted_waveforms_dir,\n",
    "                         f'collision_subtracted_{str(batch_id).zfill(6)}.npy'), \n",
    "            batch_collision_subtracted_wfs.astype(np.float32))\n",
    "    np.save(os.path.join(denoised_waveforms_dir,f'denoised_{str(batch_id).zfill(6)}.npy'), \n",
    "            batch_denoised_wfs.astype(np.float32))\n",
    "    batch_id += 1\n",
    "print(f'number of spikes skipped: {skipped_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588fcb6-32ae-4292-b4e8-44d3fa530524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def merge_files_h5(filtered_location, output_h5, dataset_name, shape, delete=False):\n",
    "    with h5py.File(output_h5, \"w\") as out:\n",
    "        wfs = out.create_dataset(dataset_name, shape=shape, dtype=np.float32)\n",
    "        filenames = os.listdir(filtered_location)\n",
    "        filenames_sorted = sorted(filenames)\n",
    "        i = 0\n",
    "        for fname in tqdm(filenames_sorted):\n",
    "            if '.ipynb' in fname or '.bin' in fname:\n",
    "                continue\n",
    "            res = np.load(os.path.join(filtered_location, fname)).astype('float32')\n",
    "            n_new = res.shape[0]\n",
    "            wfs[i:i+n_new] = res\n",
    "            i += n_new\n",
    "            \n",
    "            if delete:\n",
    "                Path(os.path.join(filtered_location, fname)).unlink()\n",
    "                \n",
    "# save deconv spike index and labels\n",
    "np.save(os.path.join(output_directory, 'spike_index.npy'), deconv_spike_index)\n",
    "np.save(os.path.join(output_directory, 'spike_labels.npy'), deconv_labels)\n",
    "\n",
    "shape = (n_spikes-skipped_count, 121, 40)\n",
    "merge_files_h5(subtracted_waveforms_dir, \n",
    "               os.path.join(output_directory,'subtracted_wfs.h5'), 'wfs', shape, delete=True)\n",
    "merge_files_h5(collision_subtracted_waveforms_dir, \n",
    "               os.path.join(output_directory,'collision_subtracted_wfs.h5'), 'wfs', shape, delete=True)\n",
    "merge_files_h5(denoised_waveforms_dir, \n",
    "               os.path.join(output_directory,'denoised_wfs.h5'), 'wfs', shape, delete=True)\n",
    "\n",
    "h5 = h5py.File(os.path.join(output_directory,'denoised_wfs.h5'))\n",
    "denoised_wfs = h5[\"wfs\"]\n",
    "n_spikes = denoised_wfs.shape[0]\n",
    "\n",
    "deconv_spike_index = np.load(os.path.join(output_directory, 'spike_index.npy'))\n",
    "assert deconv_spike_index.shape[0] == n_spikes\n",
    "print(f'number of deconv spikes: {n_spikes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc644d7-3253-4c81-a202-e43669dc82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_subtract = h5py.File(os.path.join(output_directory,'collision_subtracted_wfs.h5'))\n",
    "subtracted_wfs = h5_subtract[\"wfs\"]\n",
    "n_spikes = subtracted_wfs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade0f58-9bd3-48ab-9452-73d65ad39677",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers=8\n",
    "batch_size=16384\n",
    "times = deconv_spike_train_up[:,0].copy()/30000\n",
    "xss = []\n",
    "yss = []\n",
    "z_relss = []\n",
    "z_absss = []\n",
    "alphass = []\n",
    "maxptpss = []\n",
    "mcss = []\n",
    "fcss = []\n",
    "for start in tqdm(range(0, n_spikes, batch_size)):\n",
    "    end = start+batch_size\n",
    "    ptps = denoised_wfs[start:end].copy().ptp(1)\n",
    "    batch_mcs = deconv_spike_index[start:end,1].copy()\n",
    "    batch_fcs = extract_channel_index[batch_mcs][:,0]\n",
    "    xs, ys, z_rels, z_abss, alphas = localization.localize_ptps(ptps, geom_array, batch_fcs, \n",
    "                                                                batch_mcs, n_workers=n_workers)\n",
    "    xss.append(xs)\n",
    "    yss.append(ys)\n",
    "    z_relss.append(z_rels)\n",
    "    z_absss.append(z_abss)\n",
    "    alphass.append(alphas)\n",
    "    maxptpss.append(ptps.max(1))\n",
    "    mcss.append(batch_mcs)\n",
    "    fcss.append(batch_fcs)\n",
    "    \n",
    "xss = np.concatenate(xss)\n",
    "yss = np.concatenate(yss)\n",
    "z_relss = np.concatenate(z_relss)\n",
    "z_absss = np.concatenate(z_absss)\n",
    "alphass = np.concatenate(alphass)\n",
    "maxptpss = np.concatenate(maxptpss)\n",
    "mcss = np.concatenate(mcss)\n",
    "fcss = np.concatenate(fcss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19562f-8664-4159-9d3f-52c468d42e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv_labels_original = deconv_spike_train[:,1]\n",
    "deconv_frames = deconv_spike_train[:,0]\n",
    "\n",
    "#create hdbscan/localization SpikeInterface sorting (with triage)\n",
    "sorting_hdbl_t_deconv = make_sorting_from_labels_frames(deconv_labels_original, deconv_frames+18)\n",
    "\n",
    "cmp_5_deconv = compare_two_sorters(sorting_hdbl_t_deconv, sorting_kilo, sorting1_name='ours', sorting2_name='kilosort', match_score=.5)\n",
    "matched_units_5_deconv = cmp_5_deconv.get_matching()[0].index.to_numpy()[np.where(cmp_5_deconv.get_matching()[0] != -1.)]\n",
    "matches_kilos_5_deconv = cmp_5_deconv.get_best_unit_match1(matched_units_5_deconv).values.astype('int')\n",
    "\n",
    "cmp_1_deconv = compare_two_sorters(sorting_hdbl_t_deconv, sorting_kilo, sorting1_name='ours', sorting2_name='kilosort', match_score=.1)\n",
    "matched_units_1_deconv = cmp_1_deconv.get_matching()[0].index.to_numpy()[np.where(cmp_1_deconv.get_matching()[0] != -1.)]\n",
    "unmatched_units_1_deconv = cmp_1_deconv.get_matching()[0].index.to_numpy()[np.where(cmp_1_deconv.get_matching()[0] == -1.)]\n",
    "matches_kilos_1_deconv = cmp_1_deconv.get_best_unit_match1(matched_units_1_deconv).values.astype('int')\n",
    "\n",
    "cmp_kilo_5_deconv = compare_two_sorters(sorting_kilo, sorting_hdbl_t_deconv, sorting1_name='kilosort', sorting2_name='ours', match_score=.5)\n",
    "matched_units_kilo_5_deconv = cmp_kilo_5_deconv.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_5_deconv.get_matching()[0] != -1.)]\n",
    "unmatched_units_kilo_5_deconv = cmp_kilo_5_deconv.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_5_deconv.get_matching()[0] == -1.)]\n",
    "\n",
    "cmp_kilo_1_deconv = compare_two_sorters(sorting_kilo, sorting_hdbl_t_deconv, sorting1_name='kilosort', sorting2_name='ours', match_score=.1)\n",
    "matched_units_kilo_1_deconv = cmp_kilo_1_deconv.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_1_deconv.get_matching()[0].to_numpy() != -1.)]\n",
    "unmatched_units_kilo_1_deconv = cmp_kilo_1_deconv.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_1_deconv.get_matching()[0].to_numpy() == -1.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec717af9-799a-4bf7-a7c0-747d2890ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_kilo_1_deconv.get_agreement_fraction(17, 276.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387dd1e4-8b85-4877-b06a-5d5f6d432cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir = cm.get_cmap('viridis')\n",
    "triaged_log_ptp = maxptpss.copy()\n",
    "triaged_log_ptp[triaged_log_ptp >= 27.5] = 27.5\n",
    "triaged_log_ptp = np.log(triaged_log_ptp+1)\n",
    "triaged_log_ptp[triaged_log_ptp<=1.25] = 1.25\n",
    "triaged_ptp_rescaled = (triaged_log_ptp - triaged_log_ptp.min())/(triaged_log_ptp.max() - triaged_log_ptp.min())\n",
    "color_arr = vir(triaged_ptp_rescaled)\n",
    "color_arr[:, 3] = triaged_ptp_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c5a7b-7c55-4b6e-8130-3ebb7bd06a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###hdbscan\n",
    "save_dir_path = \"good_unit_kilo_comparison_deconv\"\n",
    "if not os.path.exists(save_dir_path):\n",
    "    os.makedirs(save_dir_path)\n",
    "\n",
    "for good_kilo_sort_cluster in good_kilo_sort_clusters:\n",
    "    cluster_id_match = good_kilo_sort_cluster\n",
    "    cluster_id = int(cmp_kilo_1_deconv.get_best_unit_match1(cluster_id_match))\n",
    "    depth = int(kilo_cluster_depth_means[cluster_id_match])\n",
    "    save_str = str(depth).zfill(4)\n",
    "    if cluster_id != -1:\n",
    "        sorting1 = sorting_hdbl_t_deconv\n",
    "        sorting2 = sorting_kilo\n",
    "        sorting1_name = \"hdb\"\n",
    "        sorting2_name = \"kilo\"\n",
    "        firstchans_cluster_sorting1 = fcss[deconv_labels_original == cluster_id]\n",
    "        mcs_abs_cluster_sorting1 = mcss[deconv_labels_original == cluster_id]\n",
    "        spike_depths = kilo_spike_depths[np.where(kilo_spike_clusters==cluster_id_match)]\n",
    "        mcs_abs_cluster_sorting2 = np.asarray([np.argmin(np.abs(spike_depth - geom_array[:,1])) for spike_depth in spike_depths])\n",
    "        firstchans_cluster_sorting2 = (mcs_abs_cluster_sorting2 - 20).clip(min=0)\n",
    "\n",
    "        fig = plot_agreement_venn(cluster_id, cluster_id_match, cmp_1_deconv, sorting1, sorting2, sorting1_name, sorting2_name, geom_array, num_channels, num_spikes_plot, firstchans_cluster_sorting1, mcs_abs_cluster_sorting1, \n",
    "                                  firstchans_cluster_sorting2, mcs_abs_cluster_sorting2, raw_data_bin, delta_frames = 12, alpha=.2)\n",
    "        plt.close(fig)\n",
    "        fig.savefig(save_dir_path + f\"/Z{save_str}_{cluster_id_match}_{cluster_id}_comparison.png\")\n",
    "        \n",
    "#         fig = plot_single_unit_summary(cluster_id, deconv_labels_original, cluster_centers, geom_array, num_spikes_plot, num_rows_plot, xss, z_absss, maxptpss, \n",
    "#                                fcss, mcss, deconv_frames+18, np.arange(deconv_frames.shape[0]), denoised_wfs, subtracted_wfs, cluster_color_dict, \n",
    "#                                color_arr, raw_data_bin, residual_data_bin)\n",
    "#         plt.close(fig)\n",
    "#         fig.savefig(save_dir_path + f\"/Z{save_str}_hdb{cluster_id}_full_summary.png\")\n",
    "        \n",
    "    num_spikes = len(sorting_kilo.get_unit_spike_train(cluster_id_match))\n",
    "    if num_spikes > 0:\n",
    "        #plot specific kilosort example\n",
    "        num_close_clusters = 50\n",
    "        num_close_clusters_plot=10\n",
    "        num_channels_similarity = 20\n",
    "        shifts_align=np.arange(-8,9)\n",
    "\n",
    "        st_1 = sorting_kilo.get_unit_spike_train(cluster_id_match)\n",
    "\n",
    "        #compute K closest hdbscan clsuters\n",
    "        closest_clusters = get_closest_clusters_kilosort_hdbscan(cluster_id_match, kilo_cluster_depth_means, cluster_centers, num_close_clusters)\n",
    "\n",
    "        fig = plot_unit_similarities(cluster_id_match, closest_clusters, sorting_kilo, sorting_hdbl_t_deconv, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "                                     num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"first\")\n",
    "        plt.close(fig)\n",
    "        fig.savefig(save_dir_path + f\"/Z{save_str}_{cluster_id_match}_summary_similarity.png\")\n",
    "        \n",
    "        fig = plot_unit_similarities(cluster_id_match, closest_clusters, sorting_kilo, sorting_hdbl_t_deconv, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "                                     num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='agreement', normalize_agreement_by=\"first\")\n",
    "        plt.close(fig)\n",
    "        fig.savefig(save_dir_path + f\"/Z{save_str}_{cluster_id_match}_summary_agreement.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf7ada-6e15-43cf-b27a-8f9b599ee09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ece3b-38c1-43f1-8de7-a826e0953d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46518ac9-6da5-4ec7-90f0-bf621c292806",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(templates[275].T[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef2e3f-092f-4a14-95b2-7bb734a9b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(templates[275,:,30].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b334bae-f6a7-49ef-83df-2a1ee62f6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z_absss[deconv_labels_original==275], maxptpss[deconv_labels_original==275])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b6c27-2b90-4cf6-a5ce-19a9af9a5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_single_unit_summary(cluster_id, deconv_labels_original, cluster_centers, geom_array, num_spikes_plot, num_rows_plot, xss, z_absss, maxptpss, \n",
    "                               fcss, mcss, deconv_frames+18, np.arange(deconv_frames.shape[0]), denoised_wfs, subtracted_wfs, cluster_color_dict, \n",
    "                               color_arr, raw_data_bin, residual_data_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee491a3-19c9-4f37-88aa-132be1721387",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f3ba7-01e5-4c1c-b7a2-175e51cc0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_hdbl_t.get_unit_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d04d8-b392-4c93-b5e3-118ff48a1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dba043-4fc9-4cd2-9a2b-559c77669b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_cluster_depth = kilo_cluster_depth_means[cluster_id]\n",
    "closest_cluster_indices = np.argsort(np.abs(cluster_centers.iloc[:,1].to_numpy() - kilo_cluster_depth_means[cluster_id]))[:num_close_clusters]\n",
    "closest_clusters = cluster_centers.index[closest_cluster_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f0f202-23dc-47fc-9ad7-1c02f0a90d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot specific kilosort example\n",
    "cluster_id_match = 17\n",
    "num_close_clusters = 100\n",
    "num_close_clusters_plot=10\n",
    "num_channels_similarity = 20\n",
    "shifts_align=np.arange(-8,9)\n",
    "\n",
    "st_1 = sorting_kilo.get_unit_spike_train(cluster_id_match)\n",
    "\n",
    "#compute K closest hdbscan clsuters\n",
    "closest_clusters = get_closest_clusters_kilosort_hdbscan(cluster_id_match, kilo_cluster_depth_means, cluster_centers, num_close_clusters)\n",
    "\n",
    "fig = plot_unit_similarities(cluster_id_match, closest_clusters, sorting_kilo, sorting_hdbl_t_deconv, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "                             num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='agreement', normalize_agreement_by=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05595f91-38a3-4fdf-9f3d-ccbc99909409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2120720-a446-4f05-87cd-819c518d079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_id = 349\n",
    "# cluster_id_match = 25\n",
    "# sorting1 = sorting_hdbl_t_deconv\n",
    "# sorting2 = sorting_kilo\n",
    "# sorting1_name = \"hdb_deconv\"\n",
    "# sorting2_name = \"kilo\"\n",
    "# firstchans_cluster_sorting1 = fcss[deconv_labels_original == cluster_id]\n",
    "# mcs_abs_cluster_sorting1 = mcss[deconv_labels_original == cluster_id]\n",
    "# spike_depths = kilo_spike_depths[np.where(kilo_spike_clusters==cluster_id_match)]\n",
    "# mcs_abs_cluster_sorting2 = np.asarray([np.argmin(np.abs(spike_depth - geom_array[:,1])) for spike_depth in spike_depths])\n",
    "# firstchans_cluster_sorting2 = (mcs_abs_cluster_sorting2 - 20).clip(min=0)\n",
    "\n",
    "# fig = plot_agreement_venn(cluster_id, cluster_id_match, cmp_5_deconv, sorting1, sorting2, sorting1_name, sorting2_name, geom_array, num_channels, num_spikes_plot, firstchans_cluster_sorting1, mcs_abs_cluster_sorting1, \n",
    "#                           firstchans_cluster_sorting2, mcs_abs_cluster_sorting2, raw_data_bin, delta_frames = 12, alpha=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b8708-d7bb-4303-8335-aff80047bacf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster_id = 187\n",
    "\n",
    "# if cluster_id in matched_units_5:\n",
    "#     cmp = cmp_5\n",
    "#     print(\">50% match\")\n",
    "# elif cluster_id in matched_units_1:\n",
    "#     cmp = cmp_1\n",
    "#     print(\"50%> and >10% match\")\n",
    "# else:\n",
    "#     cmp = None\n",
    "#     print(\"<10% match\")\n",
    "    \n",
    "# num_spikes_plot=50\n",
    "# #plot cluster summary\n",
    "# fig = plot_single_unit_summary(cluster_id, clusterer.labels_, cluster_centers, geom_array, num_spikes_plot, num_rows_plot, triaged_x, triaged_z, triaged_maxptps, \n",
    "#                                triaged_firstchans, triaged_mcs_abs, triaged_spike_index+18, non_triaged_idxs, wfs_localized, wfs_subtracted, cluster_color_dict, \n",
    "#                                color_arr, raw_data_bin, residual_data_bin)\n",
    "# plt.show()\n",
    "\n",
    "# # plot agreement with kilosort\n",
    "# if cmp is not None:\n",
    "#     num_channels = wfs_localized.shape[2]\n",
    "#     cluster_id_match = cmp.get_best_unit_match1(cluster_id)\n",
    "#     sorting1 = sorting_hdbl_t\n",
    "#     sorting2 = sorting_kilo\n",
    "#     sorting1_name = \"hdb\"\n",
    "#     sorting2_name = \"kilo\"\n",
    "#     firstchans_cluster_sorting1 = triaged_firstchans[clusterer.labels_ == cluster_id]\n",
    "#     mcs_abs_cluster_sorting1 = triaged_mcs_abs[clusterer.labels_ == cluster_id]\n",
    "#     spike_depths = kilo_spike_depths[np.where(kilo_spike_clusters==cluster_id_match)]\n",
    "#     mcs_abs_cluster_sorting2 = np.asarray([np.argmin(np.abs(spike_depth - geom_array[:,1])) for spike_depth in spike_depths])\n",
    "#     firstchans_cluster_sorting2 = (mcs_abs_cluster_sorting2 - 20).clip(min=0)\n",
    "    \n",
    "#     plot_agreement_venn(cluster_id, cluster_id_match, cmp, sorting1, sorting2, sorting1_name, sorting2_name, geom_array, num_channels, num_spikes_plot, firstchans_cluster_sorting1, mcs_abs_cluster_sorting1, \n",
    "#                         firstchans_cluster_sorting2, mcs_abs_cluster_sorting2, raw_data_bin, delta_frames = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0b359-e7c1-498f-9e63-53d1452bb398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# save_dir_parallel = \"parallel_cluster_summary_plots\"\n",
    "# if not os.path.exists(save_dir_parallel):\n",
    "#     os.makedirs(save_dir_parallel)\n",
    "    \n",
    "# num_spikes_plot = 50\n",
    "    \n",
    "# def job(cluster_id):\n",
    "#     fig = plot_single_unit_summary(\n",
    "#         cluster_id,\n",
    "#         clusterer.labels_,\n",
    "#         cluster_centers,\n",
    "#         geom_array,\n",
    "#         num_spikes_plot,\n",
    "#         num_rows_plot,\n",
    "#         triaged_x,\n",
    "#         triaged_z,\n",
    "#         triaged_maxptps,\n",
    "#         triaged_firstchans,\n",
    "#         triaged_mcs_abs,\n",
    "#         triaged_spike_index,\n",
    "#         non_triaged_idxs,\n",
    "#         wfs_localized,\n",
    "#         wfs_subtracted,\n",
    "#         cluster_color_dict,\n",
    "#         color_arr,\n",
    "#         raw_data_bin,\n",
    "#         residual_data_bin,\n",
    "#     )\n",
    "#     save_z_int = int(cluster_centers.loc[cluster_id][1])\n",
    "#     save_str = str(save_z_int).zfill(4)\n",
    "#     fig.savefig(save_dir_parallel + f\"/Z{save_str}_cluster{cluster_id}.png\", transparent=False, pad_inches=0)\n",
    "#     plt.close(fig)\n",
    "# with Parallel(\n",
    "#     12,\n",
    "# ) as p:\n",
    "#     unit_ids = list(range(0,24))#np.setdiff1d(np.unique(clusterer.labels_), [-1])\n",
    "#     for res in p(delayed(job)(u) for u in  tqdm(unit_ids)):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2111bfa-77d1-41a3-8d5a-5939c855945d",
   "metadata": {},
   "source": [
    "# Oversplit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fa6af-a4e1-4087-8f0a-e8ab0261aa73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###Kilosort\n",
    "# save_dir_path = \"oversplit_cluster_summaries_kilosort\"\n",
    "# if not os.path.exists(save_dir_path):\n",
    "#     os.makedirs(save_dir_path)\n",
    "    \n",
    "# num_close_clusters = 50\n",
    "# num_close_clusters_plot=10\n",
    "# num_channels_similarity = 20\n",
    "# num_under_threshold = 0\n",
    "# num_spikes_plot = 50\n",
    "# shifts_align=np.arange(-8,9)\n",
    "# for cluster_id in sorting_kilo.get_unit_ids():\n",
    "#     st_1 = sorting_kilo.get_unit_spike_train(cluster_id)\n",
    "    \n",
    "#     #compute K closest clsuters\n",
    "#     closest_clusters = get_closest_clusters_kilosort(cluster_id, kilo_cluster_depth_means, num_close_clusters=num_close_clusters)\n",
    "    \n",
    "#     #compute unit similarties\n",
    "#     original_template, closest_clusters, similarities, agreements, templates, shifts = get_unit_similarities(cluster_id, st_1, closest_clusters, sorting_kilo, geom_array, raw_data_bin, \n",
    "#                                                                                                              num_channels_similarity=num_channels_similarity, \n",
    "#                                                                                                              num_close_clusters=num_close_clusters, shifts_align=shifts_align,\n",
    "#                                                                                                              order_by ='similarity')\n",
    "#     if similarities[0] < 2.0: #arbitrary..\n",
    "#         print(similarities[0], closest_clusters[0])\n",
    "#         fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_kilo, sorting_kilo, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                      num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"both\")\n",
    "#         plt.close(fig)\n",
    "#         fig.savefig(save_dir_path + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d76d1c-85c0-48db-8e31-2170fd62896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###hdbscan\n",
    "# save_dir_path = \"oversplit_cluster_summaries_hdbscan\"\n",
    "# if not os.path.exists(save_dir_path):\n",
    "#     os.makedirs(save_dir_path)\n",
    "\n",
    "# num_close_clusters = 50\n",
    "# num_close_clusters_plot=10\n",
    "# num_channels_similarity = 20\n",
    "# num_under_threshold = 0\n",
    "# num_spikes_plot = 50\n",
    "# shifts_align=np.arange(-8,9)\n",
    "# for cluster_id in sorting_hdbl_t.get_unit_ids():\n",
    "#     if cluster_id != -1:\n",
    "#         #compute firing rate\n",
    "#         st_1 = sorting_hdbl_t.get_unit_spike_train(cluster_id)\n",
    "#         #compute K closest clsuters\n",
    "#         closest_clusters = get_closest_clusters_hdbscan(cluster_id, cluster_centers, num_close_clusters=num_close_clusters)\n",
    "#         #compute unit similarties\n",
    "#         original_template, closest_clusters, similarities, agreements, templates, shifts = get_unit_similarities(cluster_id, st_1, closest_clusters, sorting_hdbl_t, geom_array, raw_data_bin, \n",
    "#                                                                                                                  num_channels_similarity=num_channels_similarity, \n",
    "#                                                                                                                  num_close_clusters=num_close_clusters, shifts_align=shifts_align,\n",
    "#                                                                                                                  order_by ='similarity')\n",
    "#         if similarities[0] < 2.0: #arbitrary..\n",
    "#             print(similarities[0], closest_clusters[0])\n",
    "#             fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_hdbl_t, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                          num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"both\",\n",
    "#                                          denoised_waveforms=wfs_localized, cluster_labels=clusterer.labels_, non_triaged_idxs=non_triaged_idxs, triaged_mcs_abs=triaged_mcs_abs, \n",
    "#                                          triaged_firstchans=triaged_firstchans)\n",
    "#             plt.close(fig)\n",
    "#             fig.savefig(save_dir_path + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b0c78-391f-4f38-93c6-6adf86b8f66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_dir_path_hdbscan_kilo = \"cluster_summaries_hdbscan_kilo\"\n",
    "# if not os.path.exists(save_dir_path_hdbscan_kilo):\n",
    "#     os.makedirs(save_dir_path_hdbscan_kilo)\n",
    "\n",
    "# num_close_clusters = 50\n",
    "# num_close_clusters_plot=10\n",
    "# num_channels_similarity = 20\n",
    "# num_under_threshold = 0\n",
    "# num_spikes_plot = 50\n",
    "# shifts_align=np.arange(-8,9)\n",
    "# for cluster_id in [18]:#tqdm(sorting_hdbl_t.get_unit_ids()):\n",
    "#     if cluster_id != -1:\n",
    "#         st_1 = sorting_hdbl_t.get_unit_spike_train(cluster_id)\n",
    "\n",
    "#         #compute K closest kilosort clsuters\n",
    "#         closest_clusters = get_closest_clusters_hdbscan_kilosort(cluster_id, cluster_centers, kilo_cluster_depth_means, num_close_clusters)\n",
    "        \n",
    "#         fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_hdbl_t, sorting_kilo, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                      num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"second\")\n",
    "#         plt.close(fig)\n",
    "#         fig.savefig(save_dir_path_hdbscan_kilo + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907e4f4-b296-47aa-8482-8dcea7144502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir_path_hdbscan_kilo = \"cluster_summaries_hdbscan_kilo_agreement\"\n",
    "# if not os.path.exists(save_dir_path_hdbscan_kilo):\n",
    "#     os.makedirs(save_dir_path_hdbscan_kilo)\n",
    "    \n",
    "# for cluster_id in tqdm(sorting_hdbl_t.get_unit_ids()):\n",
    "#     if cluster_id != -1:\n",
    "#         st_1 = sorting_hdbl_t.get_unit_spike_train(cluster_id)\n",
    "\n",
    "#         #compute K closest kilosort clsuters\n",
    "#         closest_clusters = get_closest_clusters_hdbscan_kilosort(cluster_id, cluster_centers, kilo_cluster_depth_means, num_close_clusters)\n",
    "        \n",
    "#         fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_hdbl_t, sorting_kilo, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                      num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='agreement', normalize_agreement_by=\"second\")\n",
    "#         plt.close(fig)\n",
    "#         fig.savefig(save_dir_path_hdbscan_kilo + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93127c-72c9-4b05-af63-0fb5df6e3772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_dir_path_kilo_hdbscan = \"cluster_summaries_kilo_hdbscan\"\n",
    "# if not os.path.exists(save_dir_path_kilo_hdbscan):\n",
    "#     os.makedirs(save_dir_path_kilo_hdbscan)\n",
    "    \n",
    "# for cluster_id in tqdm(sorting_kilo.get_unit_ids()):\n",
    "\n",
    "#     st_1 = sorting_kilo.get_unit_spike_train(cluster_id)\n",
    "\n",
    "#     #compute K closest hdbscan clsuters\n",
    "#     closest_clusters = get_closest_clusters_kilosort_hdbscan(cluster_id, kilo_cluster_depth_means, cluster_centers, num_close_clusters)\n",
    "\n",
    "#     fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_kilo, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                  num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"second\")\n",
    "#     plt.close(fig)\n",
    "#     fig.savefig(save_dir_path_kilo_hdbscan + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870f35a-4487-4593-b9fa-b2d5828fed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir_path_kilo_hdbscan = \"cluster_summaries_kilo_hdbscan_agreement\"\n",
    "# if not os.path.exists(save_dir_path_kilo_hdbscan):\n",
    "#     os.makedirs(save_dir_path_kilo_hdbscan)\n",
    "    \n",
    "# for cluster_id in tqdm(sorting_kilo.get_unit_ids()):\n",
    "\n",
    "#     st_1 = sorting_kilo.get_unit_spike_train(cluster_id)\n",
    "\n",
    "#     #compute K closest hdbscan clsuters\n",
    "#     closest_clusters = get_closest_clusters_kilosort_hdbscan(cluster_id, kilo_cluster_depth_means, cluster_centers, num_close_clusters)\n",
    "\n",
    "#     fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_kilo, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                                  num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='agreement', normalize_agreement_by=\"second\")\n",
    "#     plt.close(fig)\n",
    "#     fig.savefig(save_dir_path_kilo_hdbscan + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a80139-e7c5-4c7b-814a-261395749b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmp_kilo_5 = compare_two_sorters(sorting_kilo, sorting_hdbl_t, sorting1_name='kilosort', sorting2_name='ours', match_score=.5)\n",
    "# matched_units_kilo_5 = cmp_kilo_5.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_5.get_matching()[0] != -1.)]\n",
    "# unmatched_units_kilo_5 = cmp_kilo_5.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_5.get_matching()[0] == -1.)]\n",
    "\n",
    "# cmp_kilo_1 = compare_two_sorters(sorting_kilo, sorting_hdbl_t, sorting1_name='kilosort', sorting2_name='ours', match_score=.1)\n",
    "# matched_units_kilo_1 = cmp_kilo_1.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_1.get_matching()[0].to_numpy() != -1.)]\n",
    "# unmatched_units_kilo_1 = cmp_kilo_1.get_matching()[0].index.to_numpy()[np.where(cmp_kilo_1.get_matching()[0].to_numpy() == -1.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11ed14-bab6-4e6d-a25a-e585161153ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too_small = 0\n",
    "# not_in_snippet = 0\n",
    "# good_match = 0\n",
    "# bad_match = 0\n",
    "# y_we_no_catch = []\n",
    "# for good_cluster in good_kilo_sort_clusters:\n",
    "#     if good_cluster not in sorting_kilo.get_unit_ids():\n",
    "#         not_in_snippet += 1\n",
    "#     elif len(sorting_kilo.get_unit_spike_train(good_cluster)) < 25:\n",
    "#         too_small += 1 \n",
    "#     elif good_cluster in matched_units_kilo_5:\n",
    "#         good_match += 1\n",
    "#     elif good_cluster in matched_units_kilo_1:\n",
    "#         bad_match += 1\n",
    "#     else:\n",
    "#         y_we_no_catch.append(good_cluster)\n",
    "# print(f\"total understood: {not_in_snippet + too_small + bad_match + good_match}, good_match (>.5): {good_match}, bad_match (<.5): {bad_match}, too_few_spikes_in_snippet: {too_small}, not_in_snippet {not_in_snippet}\")\n",
    "# print(f\"total not accounted for: {len(y_we_no_catch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcac8a-d46b-4ede-a1dd-5c71f7d5b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot specific kilosort example\n",
    "# cluster_id = 53\n",
    "# num_close_clusters = 50\n",
    "# num_close_clusters_plot=10\n",
    "# num_channels_similarity = 20\n",
    "# shifts_align=np.arange(-8,9)\n",
    "\n",
    "# st_1 = sorting_kilo.get_unit_spike_train(cluster_id)\n",
    "\n",
    "# #compute K closest hdbscan clsuters\n",
    "# closest_clusters = get_closest_clusters_kilosort_hdbscan(cluster_id, kilo_cluster_depth_means, cluster_centers, num_close_clusters)\n",
    "\n",
    "# fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_kilo, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "#                              num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb8ccb-7b78-48bf-bc59-b7c96134072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot specific hdbscan example\n",
    "cluster_id = 336\n",
    "num_close_clusters = 50\n",
    "num_close_clusters_plot=10\n",
    "num_channels_similarity = 20\n",
    "shifts_align=np.arange(-8,9)\n",
    "\n",
    "st_1 = sorting_hdbl_t.get_unit_spike_train(cluster_id)\n",
    "\n",
    "#compute K closest kilosort clsuters\n",
    "closest_clusters = get_closest_clusters_hdbscan(cluster_id, cluster_centers, num_close_clusters)\n",
    "\n",
    "fig = plot_unit_similarities(cluster_id, closest_clusters, sorting_hdbl_t, sorting_hdbl_t, geom_array, raw_data_bin, recording_duration, num_channels, num_spikes_plot, num_channels_similarity=num_channels_similarity, \n",
    "                             num_close_clusters_plot=num_close_clusters_plot, num_close_clusters=num_close_clusters, shifts_align = shifts_align, order_by ='similarity', normalize_agreement_by=\"second\")\n",
    "# plt.close(fig)\n",
    "# fig.savefig(save_dir_path_kilo_hdbscan + f\"/cluster_{cluster_id}_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce53d7c-02e6-4bf6-86ef-732dc25ae978",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfs_a = wfs_localized[non_triaged_idxs[clusterer.labels_==336]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22863ee8-7ffd-41d6-a435-c6065110deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfs_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33325c9f-987d-4522-b336-5dda8a97e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31beebd-f513-4470-a925-7d2043f6f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.mode(triaged_mcs_abs[clusterer.labels_==336])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4c42d-56a0-490a-9026-6e9ae27eeedb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "relmaxa = scipy.stats.mode(triaged_mcs_abs[clusterer.labels_==336])[0][0] - scipy.stats.mode(triaged_firstchans[clusterer.labels_==336])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a141b4-76de-45ec-88ea-87c8dc99acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d766c-2133-4565-8780-084f6b2f9ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_templates = clusterer.labels_.max()+1\n",
    "labels=clusterer.labels_\n",
    "templates = get_templates(raw_data_bin, geom_array, n_templates, triaged_spike_index, labels)\n",
    "\n",
    "from spike_psvae.merge_split import get_n_spikes_templates, get_x_z_templates,get_proposed_pairs\n",
    "n_spikes_templates = get_n_spikes_templates(n_templates, labels)\n",
    "x_z_templates = get_x_z_templates(n_templates, labels, triaged_x, triaged_z)\n",
    "print(\"GET PROPOSED PAIRS\")\n",
    "dist_argsort, dist_template = get_proposed_pairs(n_templates, templates, x_z_templates, n_temp = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7e5ab-937e-4086-9a00-123a4605f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates[336].ptp(0).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424ab93-c673-4555-ac63-9638ba2781c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    from spike_psvae.merge_split import get_diptest_value\n",
    "    unit_a = 336\n",
    "    unit_b = 338\n",
    "    mc = templates[unit_a].ptp(0).argmax()\n",
    "    two_units_shift = templates[unit_b, :, mc].argmin() - templates[unit_a, :, mc].argmin()\n",
    "    unit_shifted = unit_b\n",
    "\n",
    "\n",
    "    print(get_diptest_value(raw_data_bin, geom_array, triaged_spike_index, labels, unit_a, unit_b, n_spikes_templates, mc, two_units_shift, unit_shifted, denoiser, device, n_channels=40, n_times=121))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0974453-192c-4bc1-8d32-6932e45b5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_id = 37\n",
    "# if cluster_id != -1:\n",
    "#     curr_clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "#     initial_indices = np.nonzero(clusterer.labels_ == cluster_id)[0]\n",
    "#     cluster_features = np.concatenate((np.expand_dims(triaged_x,1), np.expand_dims(triaged_z,1), np.expand_dims(np.log(triaged_maxptps)*scales[4],1)), axis=1)[np.nonzero(clusterer.labels_ == cluster_id)[0]]\n",
    "#     curr_clusterer.fit(cluster_features)\n",
    "#     final_indices_list = []\n",
    "#     final_labels_list = []\n",
    "#     indices_to_be_processed = []\n",
    "#     final_labels_concat = None\n",
    "#     unique_labels = np.unique(curr_clusterer.labels_)\n",
    "#     if len(unique_labels) > 1:\n",
    "#         for label in unique_labels:\n",
    "#             curr_indices = np.nonzero(curr_clusterer.labels_ == label)[0]\n",
    "#             if label != -1:\n",
    "#                 indices_to_be_processed.append(curr_indices)\n",
    "#             else:\n",
    "#                 final_indices_list.append(curr_indices)\n",
    "#                 final_labels = (np.zeros(len(curr_indices)) - 1).astype('int')\n",
    "#                 final_labels_list.append(final_labels)\n",
    "#         cluster_id_curr = 0\n",
    "#         while len(indices_to_be_processed) > 0:\n",
    "#             indices = indices_to_be_processed.pop()\n",
    "#             curr_clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "#             curr_clusterer.fit(cluster_features[indices])\n",
    "#             # print(np.unique(curr_clusterer.labels_))\n",
    "#             if len(np.unique(curr_clusterer.labels_)) > 1:\n",
    "#                 for label in np.unique(curr_clusterer.labels_):\n",
    "#                     curr_indices = indices[np.nonzero(curr_clusterer.labels_ == label)[0]]\n",
    "#                     if label != -1:\n",
    "#                         indices_to_be_processed.append(curr_indices)\n",
    "#                     else:\n",
    "#                         final_indices_list.append(curr_indices)\n",
    "#                         final_labels = (np.zeros(len(curr_indices)) - 1).astype('int')\n",
    "#                         final_labels_list.append(final_labels)\n",
    "#             else:\n",
    "#                 final_indices_list.append(indices)\n",
    "#                 final_labels = (np.zeros(len(indices)) + cluster_id_curr).astype('int')\n",
    "#                 final_labels_list.append(final_labels)\n",
    "#                 cluster_id_curr += 1\n",
    "#         final_indices_concat = np.concatenate(final_indices_list)\n",
    "#         final_labels_concat = np.concatenate(final_labels_list)\n",
    "#         sort_idxs = np.argsort(final_indices_concat)\n",
    "#         final_indices_concat = final_indices_concat[sort_idxs]\n",
    "#         final_labels_concat = final_labels_concat[sort_idxs]\n",
    "#     else:\n",
    "#         print(\"no split\")\n",
    "#     print(np.unique(final_labels_concat))\n",
    "\n",
    "# fig = plot_array_scatter(final_labels_concat, geom_array, triaged_x[clusterer.labels_ == cluster_id], \n",
    "#                          triaged_z[clusterer.labels_ == cluster_id], \n",
    "#                          triaged_maxptps[clusterer.labels_ == cluster_id], \n",
    "#                          cluster_color_dict, color_arr[clusterer.labels_ == cluster_id], \n",
    "#                          min_samples=25, min_cluster_size=25, z_cutoff=(cluster_centers.iloc[cluster_id][1]-100,cluster_centers.iloc[cluster_id][1]+100), figsize=(18, 12))\n",
    "\n",
    "# fig = plt.figure(figsize=(6,12))\n",
    "# ax_all = fig.gca()\n",
    "# hshifts=[0, .2,.4,.6,.8, 1]\n",
    "# for unit_id, hshift in zip(np.unique(final_labels_concat)[1:],hshifts):\n",
    "#     first_chans_cluster = triaged_firstchans[clusterer.labels_ == cluster_id][final_labels_concat==unit_id]\n",
    "#     mcs_abs_cluster = triaged_mcs_abs[clusterer.labels_ == cluster_id][final_labels_concat==unit_id]\n",
    "#     spike_times = triaged_spike_index[:,0][clusterer.labels_ == cluster_id][final_labels_concat==unit_id]\n",
    "#     num_channels = 40\n",
    "#     bin_file = raw_data_bin\n",
    "#     fig = plot_raw_waveforms_unit_geom(geom_array, num_channels, first_chans_cluster, mcs_abs_cluster, spike_times, bin_file, x_geom_scale = 1/15, \n",
    "#                                        y_geom_scale = 1/10, waveform_scale = .15, spikes_plot = 100, waveform_shape=(30,90), num_rows=3, \n",
    "#                                        alpha=.05, h_shift=hshift, do_mean=False, ax=ax_all, color=cluster_color_dict[unit_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56bdf0-2481-4bf1-92cd-69aafce485fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir_path_duplicates_hdbscan = \"duplicates_hdbscan\"\n",
    "# if not os.path.exists(save_dir_path_duplicates_hdbscan):\n",
    "#     os.makedirs(save_dir_path_duplicates_hdbscan)\n",
    "    \n",
    "# for i in range(0,len(duplicates),2):\n",
    "#     cmp = cmp_self\n",
    "#     cluster_id = duplicates[i]\n",
    "#     cluster_id_match = duplicates[i+1]\n",
    "#     num_channels = wfs_localized.shape[2]\n",
    "#     # cluster_id_match = cmp.get_best_unit_match1(cluster_id)\n",
    "#     sorting1 = sorting_hdbl_t\n",
    "#     sorting2 = sorting_hdbl_t\n",
    "#     sorting1_name = \"hdb\"\n",
    "#     sorting2_name = \"hdb\"\n",
    "#     firstchans_cluster_sorting1 = triaged_firstchans[clusterer.labels_ == cluster_id]\n",
    "#     mcs_abs_cluster_sorting1 = triaged_mcs_abs[clusterer.labels_ == cluster_id]\n",
    "#     firstchans_cluster_sorting2 = triaged_firstchans[clusterer.labels_ == cluster_id_match]\n",
    "#     mcs_abs_cluster_sorting2 = triaged_mcs_abs[clusterer.labels_ == cluster_id_match]\n",
    "\n",
    "#     fig = plot_agreement_venn(cluster_id, cluster_id_match, cmp, sorting1, sorting2, sorting1_name, sorting2_name, geom_array, num_channels, num_spikes_plot, firstchans_cluster_sorting1, mcs_abs_cluster_sorting1, \n",
    "#                               firstchans_cluster_sorting2, mcs_abs_cluster_sorting2, raw_data_bin, delta_frames = 12)\n",
    "#     plt.close(fig)\n",
    "#     fig.savefig(save_dir_path_duplicates_hdbscan + f\"/cluster_{cluster_id}_cluster_{cluster_id_match}_agreement.png\")\n",
    "\n",
    "#     #plot cluster summary\n",
    "#     fig = plot_single_unit_summary(cluster_id, clusterer.labels_, cluster_centers, geom_array, 50, num_rows_plot, triaged_x, triaged_z, triaged_maxptps, \n",
    "#                                    triaged_firstchans, triaged_mcs_abs, triaged_spike_index, non_triaged_idxs, wfs_localized, wfs_subtracted, cluster_color_dict, \n",
    "#                                    color_arr, raw_data_bin, residual_data_bin)\n",
    "#     plt.close(fig)\n",
    "#     fig.savefig(save_dir_path_duplicates_hdbscan + f\"/cluster_{cluster_id}_summary.png\")\n",
    "#     #plot cluster summary\n",
    "#     fig = plot_single_unit_summary(cluster_id_match, clusterer.labels_, cluster_centers, geom_array, 50, num_rows_plot, triaged_x, triaged_z, triaged_maxptps, \n",
    "#                                    triaged_firstchans, triaged_mcs_abs, triaged_spike_index, non_triaged_idxs, wfs_localized, wfs_subtracted, cluster_color_dict, \n",
    "#                                    color_arr, raw_data_bin, residual_data_bin)\n",
    "#     plt.close(fig)\n",
    "#     fig.savefig(save_dir_path_duplicates_hdbscan + f\"/cluster_{cluster_id_match}_summary.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv",
   "language": "python",
   "name": "iblenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
